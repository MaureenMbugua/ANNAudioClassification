{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "AudioClassification.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaureenMbugua/ANNAudioClassification/blob/main/AudioClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8kJe3jd56Ax"
      },
      "source": [
        "#Exploratory Data Analysis\n",
        "#Install libraries #librosa #librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItECbKzb56A4",
        "outputId": "dd08c1c9-1647-4848-8646-45e9e4e17f2a"
      },
      "source": [
        "!pip install librosa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.19.5)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.0.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyynxNRe56A6"
      },
      "source": [
        "#for displaying signal\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuuDhy7C56A7"
      },
      "source": [
        "filename='//NonverbalVocalization/NonverbalVocalization/2CVI_15_11_0_15_0_0_0.wav'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clm1skMl56A7"
      },
      "source": [
        "#for displaying graphs in a certain manner\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om7OyFiy69HB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8cbe33d-9007-405c-91e1-6052f44c6b29"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huXZtVhvEQLT"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/drive/MyDrive/NonverbalVocalization.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('/NonverbalVocalization') #Extracts the files into the /tmp folder\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "oujOdFE4GCJy",
        "outputId": "04d08872-cd11-499d-db07-8cd293d78ab9"
      },
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "data,sample_rate=librosa.load(filename)#gives two sets of info\n",
        "librosa.display.waveplot(data,sr=sample_rate)\n",
        "ipd.Audio(filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "                <audio controls=\"controls\" >\n",
              "                    <source src=\"data:audio/x-wav;base64,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\" type=\"audio/x-wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t617peXL56A9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785732d1-3f06-49f3-e624-11a565a6974a"
      },
      "source": [
        "sample_rate\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22050"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95Uz106j56A-"
      },
      "source": [
        "from scipy.io import wavfile as wav\n",
        "wave_sample_rate, wave_audio=wav.read(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blc7k8Wm56A_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269087b5-148e-4e8f-e1b7-3c70c0e2ba6f"
      },
      "source": [
        "wave_sample_rate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16000"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OorT3usT56BA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9e27981-745c-4e6d-f82a-99d8ef5736f4"
      },
      "source": [
        "wave_audio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0, ..., 15, 15, 17], dtype=int16)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjfQXtL756BB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "203d4626-2f1e-421b-eb06-3ac633e09cfc"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , ..., 0.00044033, 0.00056158,\n",
              "       0.00039001], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL5nwYJi56BB"
      },
      "source": [
        "import json\n",
        "with open(\"NonverbalVocalization/Nonverbal_Vocalization.json\", \"r\") as read_file:\n",
        "    mdata = json.load(read_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9x5yWeH56BC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0dc03dd3-e330-40a3-9115-bb3c7ec1d848"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "metadata=pd.read_csv(\"/NonverbalVocalization/NonverbalVocalization/Nonverbal_Vocalization.csv\")\n",
        "metadata.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>audfile_name</th>\n",
              "      <th>age</th>\n",
              "      <th>noise</th>\n",
              "      <th>sex</th>\n",
              "      <th>speakerID</th>\n",
              "      <th>class_ID</th>\n",
              "      <th>class_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>87LX_0_6_0_19_0_0_0.wav</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>87LX</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4MYS_0_5_0_27_0_0_0.wav</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4MYS</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F8BC_0_6_0_14_0_0_0.wav</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>F8BC</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TZCX_0_3_0_20_0_0_0.wav</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>TZCX</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CTTJ_0_10_0_14_0_0_0.wav</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>CTTJ</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               audfile_name  age  noise  ...  speakerID class_ID        class_name\n",
              "0   87LX_0_6_0_19_0_0_0.wav   19      0  ...       87LX        0  teeth-chattering\n",
              "1   4MYS_0_5_0_27_0_0_0.wav   27      0  ...       4MYS        0  teeth-chattering\n",
              "2   F8BC_0_6_0_14_0_0_0.wav   14      0  ...       F8BC        0  teeth-chattering\n",
              "3   TZCX_0_3_0_20_0_0_0.wav   20      0  ...       TZCX        0  teeth-chattering\n",
              "4  CTTJ_0_10_0_14_0_0_0.wav   14      0  ...       CTTJ        0  teeth-chattering\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPrQpsqT56BD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6cf117c-b50e-4839-cb0e-07d74af637d9"
      },
      "source": [
        "#Extract Features for Classification\n",
        "mfccs = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40)\n",
        "mfccs\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-5.3737079e+02, -5.3737079e+02, -5.3737079e+02, ...,\n",
              "        -5.2416022e+02, -5.2323865e+02, -5.2497742e+02],\n",
              "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
              "         1.8584249e+01,  1.9869776e+01,  1.7438576e+01],\n",
              "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
              "         1.8291756e+01,  1.9524261e+01,  1.7175488e+01],\n",
              "       ...,\n",
              "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
              "        -2.2395653e-01,  1.5370879e+00,  6.9571972e-01],\n",
              "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
              "        -3.7662068e-01,  1.5612111e+00,  8.7277460e-01],\n",
              "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
              "        -5.2437663e-01,  1.5261453e+00,  9.9874914e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7DQe6RH56BD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "e008c840-3d2b-450b-f5e6-b4895a47fe5c"
      },
      "source": [
        "#Extracting MFCC's for every audio file\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "\n",
        "audio_dataset_path='/NonverbalVocalization/NonverbalVocalization'\n",
        "metadata.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>audfile_name</th>\n",
              "      <th>age</th>\n",
              "      <th>noise</th>\n",
              "      <th>sex</th>\n",
              "      <th>speakerID</th>\n",
              "      <th>class_ID</th>\n",
              "      <th>class_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>87LX_0_6_0_19_0_0_0.wav</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>87LX</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4MYS_0_5_0_27_0_0_0.wav</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4MYS</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F8BC_0_6_0_14_0_0_0.wav</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>F8BC</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TZCX_0_3_0_20_0_0_0.wav</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>TZCX</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CTTJ_0_10_0_14_0_0_0.wav</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>CTTJ</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>VVBA_0_4_0_23_0_0_0.wav</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>VVBA</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>CNWF_0_3_0_20_0_0_0.wav</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>CNWF</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>O5E1_0_6_1_29_0_0_0.wav</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>O5E1</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>5AG3_0_3_1_18_0_0_0.wav</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5AG3</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>OI67_0_2_1_22_0_0_0.wav</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>OI67</td>\n",
              "      <td>0</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               audfile_name  age  noise  ...  speakerID class_ID        class_name\n",
              "0   87LX_0_6_0_19_0_0_0.wav   19      0  ...       87LX        0  teeth-chattering\n",
              "1   4MYS_0_5_0_27_0_0_0.wav   27      0  ...       4MYS        0  teeth-chattering\n",
              "2   F8BC_0_6_0_14_0_0_0.wav   14      0  ...       F8BC        0  teeth-chattering\n",
              "3   TZCX_0_3_0_20_0_0_0.wav   20      0  ...       TZCX        0  teeth-chattering\n",
              "4  CTTJ_0_10_0_14_0_0_0.wav   14      0  ...       CTTJ        0  teeth-chattering\n",
              "5   VVBA_0_4_0_23_0_0_0.wav   23      0  ...       VVBA        0  teeth-chattering\n",
              "6   CNWF_0_3_0_20_0_0_0.wav   20      0  ...       CNWF        0  teeth-chattering\n",
              "7   O5E1_0_6_1_29_0_0_0.wav   29      0  ...       O5E1        0  teeth-chattering\n",
              "8   5AG3_0_3_1_18_0_0_0.wav   18      0  ...       5AG3        0  teeth-chattering\n",
              "9   OI67_0_2_1_22_0_0_0.wav   22      0  ...       OI67        0  teeth-chattering\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIvbh9dR56BE"
      },
      "source": [
        "#creating a feature extractor function\n",
        "#loads the audio file, extracts features and finds scaled features by finding mean of transpose of value of mfcc features\n",
        " \n",
        "def features_extractor(file):\n",
        "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
        "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
        "    \n",
        "    return mfccs_scaled_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGuygtMB56BF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ba9b44-af3d-44f2-9b7c-3312c8679801"
      },
      "source": [
        "#iterating through every audio file and extracting features\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "extracted_features=[] #list to store the features and class label\n",
        "for index_num,row in tqdm(metadata.iterrows()):\n",
        "    file_name = os.path.join(os.path.abspath(audio_dataset_path), str(row[\"class_name\"])+'/', str(row[\"audfile_name\"]))\n",
        "    final_class_labels = row[\"class_name\"]\n",
        "    data = features_extractor(file_name)\n",
        "    extracted_features.append([data, final_class_labels]) #our dependent and independent features, later converted into a dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "727it [00:30, 24.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3GJVq0P56BF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "3dd739d1-d2c2-4caa-a342-3b999fcc9902"
      },
      "source": [
        "#converting the features into a Pandas dataframe\n",
        "\n",
        "extracted_features_df = pd.DataFrame(extracted_features,columns=['feature','class'])\n",
        "extracted_features_df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-596.0521, 46.280857, 21.179968, 21.595993, 1...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-430.47748, 165.47548, -19.052588, 2.3916497,...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-490.7734, 108.566605, -22.723782, 39.0963, -...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-501.74335, 143.3307, 2.2156296, 18.220789, -...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-514.83105, 145.79393, -28.424004, 60.687637,...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[-627.4276, 126.80181, -38.632298, 49.74178, -...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[-680.6681, 82.789894, -11.279946, 47.058838, ...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[-534.5318, 183.14403, 51.81739, 41.85067, -2....</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[-646.11005, 110.1571, -18.720848, 54.75976, -...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[-543.9393, 139.39474, -15.06812, 37.28891, -1...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[-514.8898, 149.27742, -0.107826084, 52.434475...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[-642.7364, 159.02129, -5.9242506, 63.123566, ...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[-668.6489, 112.715515, 1.1403782, 40.328175, ...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[-696.14667, 97.25866, -26.763723, 56.122772, ...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[-601.80695, 87.50566, -15.090966, 65.451065, ...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[-723.0953, 111.19238, -32.666832, 51.784718, ...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[-559.222, 161.1549, -11.419534, 30.979658, -2...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[-654.4111, 91.70491, 10.047834, 21.975424, 4....</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[-552.4976, 147.30885, 0.57267, 21.985813, -23...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[-454.0023, 137.15785, -8.696181, 19.607586, -...</td>\n",
              "      <td>teeth-chattering</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              feature             class\n",
              "0   [-596.0521, 46.280857, 21.179968, 21.595993, 1...  teeth-chattering\n",
              "1   [-430.47748, 165.47548, -19.052588, 2.3916497,...  teeth-chattering\n",
              "2   [-490.7734, 108.566605, -22.723782, 39.0963, -...  teeth-chattering\n",
              "3   [-501.74335, 143.3307, 2.2156296, 18.220789, -...  teeth-chattering\n",
              "4   [-514.83105, 145.79393, -28.424004, 60.687637,...  teeth-chattering\n",
              "5   [-627.4276, 126.80181, -38.632298, 49.74178, -...  teeth-chattering\n",
              "6   [-680.6681, 82.789894, -11.279946, 47.058838, ...  teeth-chattering\n",
              "7   [-534.5318, 183.14403, 51.81739, 41.85067, -2....  teeth-chattering\n",
              "8   [-646.11005, 110.1571, -18.720848, 54.75976, -...  teeth-chattering\n",
              "9   [-543.9393, 139.39474, -15.06812, 37.28891, -1...  teeth-chattering\n",
              "10  [-514.8898, 149.27742, -0.107826084, 52.434475...  teeth-chattering\n",
              "11  [-642.7364, 159.02129, -5.9242506, 63.123566, ...  teeth-chattering\n",
              "12  [-668.6489, 112.715515, 1.1403782, 40.328175, ...  teeth-chattering\n",
              "13  [-696.14667, 97.25866, -26.763723, 56.122772, ...  teeth-chattering\n",
              "14  [-601.80695, 87.50566, -15.090966, 65.451065, ...  teeth-chattering\n",
              "15  [-723.0953, 111.19238, -32.666832, 51.784718, ...  teeth-chattering\n",
              "16  [-559.222, 161.1549, -11.419534, 30.979658, -2...  teeth-chattering\n",
              "17  [-654.4111, 91.70491, 10.047834, 21.975424, 4....  teeth-chattering\n",
              "18  [-552.4976, 147.30885, 0.57267, 21.985813, -23...  teeth-chattering\n",
              "19  [-454.0023, 137.15785, -8.696181, 19.607586, -...  teeth-chattering"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTg-vLM156BG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43a21dcb-b02c-4925-a3a8-8395ba15e221"
      },
      "source": [
        "#split into independent and dependent dataset\n",
        "X = np.array(extracted_features_df[\"feature\"].tolist()) #convert series to list then array\n",
        "Y = np.array(extracted_features_df[\"class\"].tolist())\n",
        "Y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(727,)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uRuf4rw56BG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a5426f-a1fc-4c8d-849f-1ce4c336591b"
      },
      "source": [
        "#Label encoding\n",
        "#split\n",
        "y = np.array(pd.get_dummies(Y))\n",
        "y.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(727, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyUr-vPp56BH"
      },
      "source": [
        "#train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhAaxXWx56BH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85c2c34-8c35-4d7c-b252-7203efbfad45"
      },
      "source": [
        " X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(581, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14c0uShA56BI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfd4d35-4602-4c20-c85c-bac96dd90b71"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(581, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1omICB856BI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a09e819e-2847-446c-d872-a9dbdc9451c6"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(146, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqGxh_CL56BJ"
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pi_y7VH56BL"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9QhbGzq56BM"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW5SPFDL56BN"
      },
      "source": [
        "num_labels=y.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGWkI8ka56BN"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(100,input_shape=(40,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(200))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfBpkNPS56BO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84330f46-ee5d-42cb-8c8e-201cf612a3c5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 100)               4100      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 100)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 200)               20200     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 200)               0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               20100     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 100)               0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                1616      \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 46,016\n",
            "Trainable params: 46,016\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1Rp8goR56BP"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IL3Y6rW56BP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9252474-6a5b-43e0-e0dd-84afcaeb40d9"
      },
      "source": [
        "## Trianing my model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from datetime import datetime \n",
        "\n",
        "num_epochs = 5000\n",
        "num_batch_size = 32\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "start = datetime.now()\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1436 - accuracy: 0.6328\n",
            "Epoch 03751: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1431 - accuracy: 0.6145 - val_loss: 3.0852 - val_accuracy: 0.4521\n",
            "Epoch 3752/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0298 - accuracy: 0.6406\n",
            "Epoch 03752: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0260 - accuracy: 0.6368 - val_loss: 3.1907 - val_accuracy: 0.4589\n",
            "Epoch 3753/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0716 - accuracy: 0.6146\n",
            "Epoch 03753: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0526 - accuracy: 0.6059 - val_loss: 3.1002 - val_accuracy: 0.4589\n",
            "Epoch 3754/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9273 - accuracy: 0.6641\n",
            "Epoch 03754: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9363 - accuracy: 0.6678 - val_loss: 3.2616 - val_accuracy: 0.4589\n",
            "Epoch 3755/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0166 - accuracy: 0.6477\n",
            "Epoch 03755: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9871 - accuracy: 0.6454 - val_loss: 3.2072 - val_accuracy: 0.4863\n",
            "Epoch 3756/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9903 - accuracy: 0.6346\n",
            "Epoch 03756: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0001 - accuracy: 0.6386 - val_loss: 3.1341 - val_accuracy: 0.4726\n",
            "Epoch 3757/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1068 - accuracy: 0.6380\n",
            "Epoch 03757: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0812 - accuracy: 0.6386 - val_loss: 3.0635 - val_accuracy: 0.4452\n",
            "Epoch 3758/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9793 - accuracy: 0.6676\n",
            "Epoch 03758: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9841 - accuracy: 0.6678 - val_loss: 3.0626 - val_accuracy: 0.4452\n",
            "Epoch 3759/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1283 - accuracy: 0.6222\n",
            "Epoch 03759: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0539 - accuracy: 0.6368 - val_loss: 3.0625 - val_accuracy: 0.4315\n",
            "Epoch 3760/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9909 - accuracy: 0.6172\n",
            "Epoch 03760: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0706 - accuracy: 0.6162 - val_loss: 3.1735 - val_accuracy: 0.4178\n",
            "Epoch 3761/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1034 - accuracy: 0.6250\n",
            "Epoch 03761: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0675 - accuracy: 0.6403 - val_loss: 3.1918 - val_accuracy: 0.4521\n",
            "Epoch 3762/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1148 - accuracy: 0.6120\n",
            "Epoch 03762: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0619 - accuracy: 0.6145 - val_loss: 3.1796 - val_accuracy: 0.4726\n",
            "Epoch 3763/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0994 - accuracy: 0.6328\n",
            "Epoch 03763: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0955 - accuracy: 0.6248 - val_loss: 3.2736 - val_accuracy: 0.4521\n",
            "Epoch 3764/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0418 - accuracy: 0.6490\n",
            "Epoch 03764: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0709 - accuracy: 0.6472 - val_loss: 3.1762 - val_accuracy: 0.4315\n",
            "Epoch 3765/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1131 - accuracy: 0.6058\n",
            "Epoch 03765: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1192 - accuracy: 0.6024 - val_loss: 3.1514 - val_accuracy: 0.4521\n",
            "Epoch 3766/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1633 - accuracy: 0.6000\n",
            "Epoch 03766: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1574 - accuracy: 0.6059 - val_loss: 3.1020 - val_accuracy: 0.4589\n",
            "Epoch 3767/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0643 - accuracy: 0.6473\n",
            "Epoch 03767: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0427 - accuracy: 0.6506 - val_loss: 2.9852 - val_accuracy: 0.4863\n",
            "Epoch 3768/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1250 - accuracy: 0.5962\n",
            "Epoch 03768: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0847 - accuracy: 0.6231 - val_loss: 2.8934 - val_accuracy: 0.4863\n",
            "Epoch 3769/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1259 - accuracy: 0.6082\n",
            "Epoch 03769: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1145 - accuracy: 0.6179 - val_loss: 2.9425 - val_accuracy: 0.4795\n",
            "Epoch 3770/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0950 - accuracy: 0.6058\n",
            "Epoch 03770: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1249 - accuracy: 0.6007 - val_loss: 2.9478 - val_accuracy: 0.4384\n",
            "Epoch 3771/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1961 - accuracy: 0.6106\n",
            "Epoch 03771: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1506 - accuracy: 0.6265 - val_loss: 3.0946 - val_accuracy: 0.4795\n",
            "Epoch 3772/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9174 - accuracy: 0.6667\n",
            "Epoch 03772: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0235 - accuracy: 0.6299 - val_loss: 3.0495 - val_accuracy: 0.4452\n",
            "Epoch 3773/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1033 - accuracy: 0.6094\n",
            "Epoch 03773: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0639 - accuracy: 0.6231 - val_loss: 3.0275 - val_accuracy: 0.4384\n",
            "Epoch 3774/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1961 - accuracy: 0.5889\n",
            "Epoch 03774: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1671 - accuracy: 0.6076 - val_loss: 3.0407 - val_accuracy: 0.3973\n",
            "Epoch 3775/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1610 - accuracy: 0.5677\n",
            "Epoch 03775: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1265 - accuracy: 0.5904 - val_loss: 2.9068 - val_accuracy: 0.4110\n",
            "Epoch 3776/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1313 - accuracy: 0.6016\n",
            "Epoch 03776: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0618 - accuracy: 0.6145 - val_loss: 2.9210 - val_accuracy: 0.4384\n",
            "Epoch 3777/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1203 - accuracy: 0.6506\n",
            "Epoch 03777: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1203 - accuracy: 0.6506 - val_loss: 2.8812 - val_accuracy: 0.4384\n",
            "Epoch 3778/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0321 - accuracy: 0.6506\n",
            "Epoch 03778: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0710 - accuracy: 0.6420 - val_loss: 3.0235 - val_accuracy: 0.4384\n",
            "Epoch 3779/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1046 - accuracy: 0.6058\n",
            "Epoch 03779: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1479 - accuracy: 0.5990 - val_loss: 3.1097 - val_accuracy: 0.4110\n",
            "Epoch 3780/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.2582 - accuracy: 0.5911\n",
            "Epoch 03780: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1516 - accuracy: 0.6231 - val_loss: 2.9485 - val_accuracy: 0.4178\n",
            "Epoch 3781/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0762 - accuracy: 0.5885\n",
            "Epoch 03781: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0950 - accuracy: 0.6007 - val_loss: 2.9805 - val_accuracy: 0.4247\n",
            "Epoch 3782/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0440 - accuracy: 0.6406\n",
            "Epoch 03782: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0484 - accuracy: 0.6334 - val_loss: 3.2419 - val_accuracy: 0.4178\n",
            "Epoch 3783/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0451 - accuracy: 0.6250\n",
            "Epoch 03783: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0237 - accuracy: 0.6420 - val_loss: 3.2944 - val_accuracy: 0.4384\n",
            "Epoch 3784/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1944 - accuracy: 0.6222\n",
            "Epoch 03784: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1024 - accuracy: 0.6386 - val_loss: 3.2205 - val_accuracy: 0.4452\n",
            "Epoch 3785/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0892 - accuracy: 0.6322\n",
            "Epoch 03785: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0331 - accuracy: 0.6506 - val_loss: 3.2942 - val_accuracy: 0.4315\n",
            "Epoch 3786/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0696 - accuracy: 0.6094\n",
            "Epoch 03786: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0628 - accuracy: 0.6282 - val_loss: 3.1096 - val_accuracy: 0.4658\n",
            "Epoch 3787/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1927 - accuracy: 0.6224\n",
            "Epoch 03787: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1921 - accuracy: 0.6213 - val_loss: 3.2456 - val_accuracy: 0.4521\n",
            "Epoch 3788/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.0914 - accuracy: 0.6438\n",
            "Epoch 03788: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.0686 - accuracy: 0.6420 - val_loss: 3.0476 - val_accuracy: 0.4521\n",
            "Epoch 3789/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0965 - accuracy: 0.6418\n",
            "Epoch 03789: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1334 - accuracy: 0.6299 - val_loss: 3.0662 - val_accuracy: 0.4658\n",
            "Epoch 3790/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0661 - accuracy: 0.6346\n",
            "Epoch 03790: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0667 - accuracy: 0.6265 - val_loss: 3.3452 - val_accuracy: 0.4247\n",
            "Epoch 3791/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 0.8936 - accuracy: 0.7118\n",
            "Epoch 03791: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9625 - accuracy: 0.6678 - val_loss: 3.1832 - val_accuracy: 0.4452\n",
            "Epoch 3792/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0418 - accuracy: 0.6458\n",
            "Epoch 03792: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0105 - accuracy: 0.6575 - val_loss: 3.1365 - val_accuracy: 0.4452\n",
            "Epoch 3793/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9709 - accuracy: 0.6510\n",
            "Epoch 03793: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9928 - accuracy: 0.6403 - val_loss: 3.1774 - val_accuracy: 0.4178\n",
            "Epoch 3794/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0382 - accuracy: 0.6538\n",
            "Epoch 03794: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0123 - accuracy: 0.6454 - val_loss: 3.1189 - val_accuracy: 0.4521\n",
            "Epoch 3795/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0490 - accuracy: 0.6224\n",
            "Epoch 03795: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0546 - accuracy: 0.6231 - val_loss: 3.0292 - val_accuracy: 0.4384\n",
            "Epoch 3796/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1788 - accuracy: 0.6226\n",
            "Epoch 03796: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.1561 - accuracy: 0.6213 - val_loss: 3.0489 - val_accuracy: 0.4452\n",
            "Epoch 3797/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0462 - accuracy: 0.6276\n",
            "Epoch 03797: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0653 - accuracy: 0.6162 - val_loss: 3.0436 - val_accuracy: 0.4384\n",
            "Epoch 3798/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0361 - accuracy: 0.6587\n",
            "Epoch 03798: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0562 - accuracy: 0.6386 - val_loss: 3.1010 - val_accuracy: 0.4315\n",
            "Epoch 3799/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0209 - accuracy: 0.6518\n",
            "Epoch 03799: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0502 - accuracy: 0.6489 - val_loss: 3.0599 - val_accuracy: 0.4726\n",
            "Epoch 3800/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1164 - accuracy: 0.6146\n",
            "Epoch 03800: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0527 - accuracy: 0.6334 - val_loss: 3.0231 - val_accuracy: 0.4247\n",
            "Epoch 3801/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1340 - accuracy: 0.6328\n",
            "Epoch 03801: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0851 - accuracy: 0.6489 - val_loss: 3.0782 - val_accuracy: 0.4315\n",
            "Epoch 3802/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0718 - accuracy: 0.6562\n",
            "Epoch 03802: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0457 - accuracy: 0.6437 - val_loss: 3.1316 - val_accuracy: 0.4315\n",
            "Epoch 3803/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0213 - accuracy: 0.6635\n",
            "Epoch 03803: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0001 - accuracy: 0.6592 - val_loss: 3.0145 - val_accuracy: 0.4110\n",
            "Epoch 3804/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0715 - accuracy: 0.6068\n",
            "Epoch 03804: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0130 - accuracy: 0.6334 - val_loss: 3.0062 - val_accuracy: 0.4658\n",
            "Epoch 3805/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0407 - accuracy: 0.6215\n",
            "Epoch 03805: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0357 - accuracy: 0.6231 - val_loss: 3.1895 - val_accuracy: 0.4384\n",
            "Epoch 3806/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1815 - accuracy: 0.6432\n",
            "Epoch 03806: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2191 - accuracy: 0.6248 - val_loss: 3.1128 - val_accuracy: 0.4521\n",
            "Epoch 3807/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1110 - accuracy: 0.6094\n",
            "Epoch 03807: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0464 - accuracy: 0.6179 - val_loss: 3.0166 - val_accuracy: 0.4658\n",
            "Epoch 3808/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0988 - accuracy: 0.6016\n",
            "Epoch 03808: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0744 - accuracy: 0.6196 - val_loss: 3.0860 - val_accuracy: 0.4795\n",
            "Epoch 3809/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1150 - accuracy: 0.6298\n",
            "Epoch 03809: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1006 - accuracy: 0.6351 - val_loss: 3.1509 - val_accuracy: 0.4521\n",
            "Epoch 3810/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9882 - accuracy: 0.6589\n",
            "Epoch 03810: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0338 - accuracy: 0.6506 - val_loss: 3.1495 - val_accuracy: 0.4521\n",
            "Epoch 3811/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1282 - accuracy: 0.6202\n",
            "Epoch 03811: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0741 - accuracy: 0.6317 - val_loss: 3.1304 - val_accuracy: 0.4589\n",
            "Epoch 3812/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0293 - accuracy: 0.6615\n",
            "Epoch 03812: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0025 - accuracy: 0.6661 - val_loss: 3.1874 - val_accuracy: 0.4384\n",
            "Epoch 3813/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0123 - accuracy: 0.6693\n",
            "Epoch 03813: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0677 - accuracy: 0.6506 - val_loss: 3.1411 - val_accuracy: 0.4384\n",
            "Epoch 3814/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0329 - accuracy: 0.6562\n",
            "Epoch 03814: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0087 - accuracy: 0.6661 - val_loss: 3.1876 - val_accuracy: 0.4589\n",
            "Epoch 3815/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0967 - accuracy: 0.6278\n",
            "Epoch 03815: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1102 - accuracy: 0.6248 - val_loss: 3.1602 - val_accuracy: 0.4589\n",
            "Epoch 3816/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0328 - accuracy: 0.6394\n",
            "Epoch 03816: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0782 - accuracy: 0.6299 - val_loss: 3.3073 - val_accuracy: 0.4178\n",
            "Epoch 3817/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9960 - accuracy: 0.6364\n",
            "Epoch 03817: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9801 - accuracy: 0.6386 - val_loss: 3.0873 - val_accuracy: 0.4247\n",
            "Epoch 3818/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9924 - accuracy: 0.6016\n",
            "Epoch 03818: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0436 - accuracy: 0.6145 - val_loss: 3.0384 - val_accuracy: 0.4178\n",
            "Epoch 3819/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.9207 - accuracy: 0.6649\n",
            "Epoch 03819: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9177 - accuracy: 0.6678 - val_loss: 2.9952 - val_accuracy: 0.4315\n",
            "Epoch 3820/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9686 - accuracy: 0.6510\n",
            "Epoch 03820: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9767 - accuracy: 0.6454 - val_loss: 3.0309 - val_accuracy: 0.4521\n",
            "Epoch 3821/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9636 - accuracy: 0.6432\n",
            "Epoch 03821: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9982 - accuracy: 0.6317 - val_loss: 3.1176 - val_accuracy: 0.4452\n",
            "Epoch 3822/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0206 - accuracy: 0.6442\n",
            "Epoch 03822: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0617 - accuracy: 0.6454 - val_loss: 3.0726 - val_accuracy: 0.4452\n",
            "Epoch 3823/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1600 - accuracy: 0.6042\n",
            "Epoch 03823: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1248 - accuracy: 0.6127 - val_loss: 3.0070 - val_accuracy: 0.4384\n",
            "Epoch 3824/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1080 - accuracy: 0.6250\n",
            "Epoch 03824: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1153 - accuracy: 0.6145 - val_loss: 3.2038 - val_accuracy: 0.4658\n",
            "Epoch 3825/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0139 - accuracy: 0.6438\n",
            "Epoch 03825: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0142 - accuracy: 0.6575 - val_loss: 3.0979 - val_accuracy: 0.4315\n",
            "Epoch 3826/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0165 - accuracy: 0.6707\n",
            "Epoch 03826: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0023 - accuracy: 0.6627 - val_loss: 3.0760 - val_accuracy: 0.4110\n",
            "Epoch 3827/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0447 - accuracy: 0.6250\n",
            "Epoch 03827: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0627 - accuracy: 0.6196 - val_loss: 2.9660 - val_accuracy: 0.4315\n",
            "Epoch 3828/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0026 - accuracy: 0.6615\n",
            "Epoch 03828: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0183 - accuracy: 0.6489 - val_loss: 2.9043 - val_accuracy: 0.4247\n",
            "Epoch 3829/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9901 - accuracy: 0.6589\n",
            "Epoch 03829: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9488 - accuracy: 0.6713 - val_loss: 2.9490 - val_accuracy: 0.4247\n",
            "Epoch 3830/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0594 - accuracy: 0.6354\n",
            "Epoch 03830: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0685 - accuracy: 0.6231 - val_loss: 2.9408 - val_accuracy: 0.4315\n",
            "Epoch 3831/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9652 - accuracy: 0.6995\n",
            "Epoch 03831: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0071 - accuracy: 0.6781 - val_loss: 2.9339 - val_accuracy: 0.4247\n",
            "Epoch 3832/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1035 - accuracy: 0.6222\n",
            "Epoch 03832: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0405 - accuracy: 0.6403 - val_loss: 2.9954 - val_accuracy: 0.4315\n",
            "Epoch 3833/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0223 - accuracy: 0.6562\n",
            "Epoch 03833: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0660 - accuracy: 0.6368 - val_loss: 3.2357 - val_accuracy: 0.4521\n",
            "Epoch 3834/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.4515 - accuracy: 0.6068\n",
            "Epoch 03834: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.3386 - accuracy: 0.6162 - val_loss: 2.8607 - val_accuracy: 0.4521\n",
            "Epoch 3835/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0873 - accuracy: 0.6375\n",
            "Epoch 03835: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1254 - accuracy: 0.6299 - val_loss: 2.8452 - val_accuracy: 0.4178\n",
            "Epoch 3836/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1811 - accuracy: 0.6042\n",
            "Epoch 03836: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1021 - accuracy: 0.6282 - val_loss: 2.7442 - val_accuracy: 0.4315\n",
            "Epoch 3837/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0575 - accuracy: 0.6632\n",
            "Epoch 03837: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0635 - accuracy: 0.6609 - val_loss: 2.8270 - val_accuracy: 0.4452\n",
            "Epoch 3838/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1316 - accuracy: 0.6130\n",
            "Epoch 03838: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1329 - accuracy: 0.6059 - val_loss: 2.7917 - val_accuracy: 0.4521\n",
            "Epoch 3839/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0214 - accuracy: 0.6335\n",
            "Epoch 03839: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0305 - accuracy: 0.6437 - val_loss: 2.7623 - val_accuracy: 0.4452\n",
            "Epoch 3840/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9499 - accuracy: 0.6979\n",
            "Epoch 03840: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0384 - accuracy: 0.6472 - val_loss: 2.7113 - val_accuracy: 0.4452\n",
            "Epoch 3841/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0165 - accuracy: 0.6384\n",
            "Epoch 03841: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0002 - accuracy: 0.6472 - val_loss: 2.8531 - val_accuracy: 0.4315\n",
            "Epoch 3842/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1449 - accuracy: 0.6307\n",
            "Epoch 03842: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1654 - accuracy: 0.6231 - val_loss: 2.9761 - val_accuracy: 0.4315\n",
            "Epoch 3843/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 1.0214 - accuracy: 0.6287\n",
            "Epoch 03843: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0130 - accuracy: 0.6299 - val_loss: 2.9622 - val_accuracy: 0.4110\n",
            "Epoch 3844/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0315 - accuracy: 0.6335\n",
            "Epoch 03844: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0345 - accuracy: 0.6454 - val_loss: 3.0498 - val_accuracy: 0.4452\n",
            "Epoch 3845/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0748 - accuracy: 0.6163\n",
            "Epoch 03845: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0696 - accuracy: 0.6196 - val_loss: 3.0730 - val_accuracy: 0.4041\n",
            "Epoch 3846/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0146 - accuracy: 0.6392\n",
            "Epoch 03846: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0782 - accuracy: 0.6299 - val_loss: 3.0323 - val_accuracy: 0.4041\n",
            "Epoch 3847/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0836 - accuracy: 0.6094\n",
            "Epoch 03847: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0402 - accuracy: 0.6179 - val_loss: 3.0048 - val_accuracy: 0.4315\n",
            "Epoch 3848/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0883 - accuracy: 0.6392\n",
            "Epoch 03848: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1088 - accuracy: 0.6317 - val_loss: 3.0269 - val_accuracy: 0.4247\n",
            "Epoch 3849/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1506 - accuracy: 0.5964\n",
            "Epoch 03849: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1494 - accuracy: 0.5921 - val_loss: 2.9770 - val_accuracy: 0.4178\n",
            "Epoch 3850/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0330 - accuracy: 0.6406\n",
            "Epoch 03850: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0366 - accuracy: 0.6334 - val_loss: 2.9524 - val_accuracy: 0.4110\n",
            "Epoch 3851/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1052 - accuracy: 0.5964\n",
            "Epoch 03851: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0790 - accuracy: 0.6076 - val_loss: 2.9511 - val_accuracy: 0.4247\n",
            "Epoch 3852/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0715 - accuracy: 0.5966\n",
            "Epoch 03852: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0181 - accuracy: 0.6231 - val_loss: 3.0345 - val_accuracy: 0.4110\n",
            "Epoch 3853/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9920 - accuracy: 0.6442\n",
            "Epoch 03853: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0001 - accuracy: 0.6368 - val_loss: 2.9402 - val_accuracy: 0.4247\n",
            "Epoch 3854/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1213 - accuracy: 0.6062\n",
            "Epoch 03854: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0313 - accuracy: 0.6489 - val_loss: 2.9187 - val_accuracy: 0.4247\n",
            "Epoch 3855/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0476 - accuracy: 0.6538\n",
            "Epoch 03855: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0803 - accuracy: 0.6420 - val_loss: 2.8744 - val_accuracy: 0.4178\n",
            "Epoch 3856/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1562 - accuracy: 0.6146\n",
            "Epoch 03856: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0982 - accuracy: 0.6317 - val_loss: 2.7968 - val_accuracy: 0.4247\n",
            "Epoch 3857/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0465 - accuracy: 0.6745\n",
            "Epoch 03857: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0113 - accuracy: 0.6627 - val_loss: 2.9012 - val_accuracy: 0.4384\n",
            "Epoch 3858/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.1388 - accuracy: 0.6181\n",
            "Epoch 03858: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1358 - accuracy: 0.6162 - val_loss: 2.9222 - val_accuracy: 0.4315\n",
            "Epoch 3859/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0580 - accuracy: 0.6233\n",
            "Epoch 03859: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0543 - accuracy: 0.6248 - val_loss: 2.8195 - val_accuracy: 0.4178\n",
            "Epoch 3860/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9304 - accuracy: 0.6803\n",
            "Epoch 03860: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.9960 - accuracy: 0.6713 - val_loss: 2.9999 - val_accuracy: 0.4315\n",
            "Epoch 3861/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9426 - accuracy: 0.6611\n",
            "Epoch 03861: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9351 - accuracy: 0.6644 - val_loss: 3.1122 - val_accuracy: 0.4384\n",
            "Epoch 3862/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9945 - accuracy: 0.6589\n",
            "Epoch 03862: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0293 - accuracy: 0.6454 - val_loss: 3.0296 - val_accuracy: 0.4384\n",
            "Epoch 3863/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0309 - accuracy: 0.6250\n",
            "Epoch 03863: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.0452 - accuracy: 0.6386 - val_loss: 3.1810 - val_accuracy: 0.4452\n",
            "Epoch 3864/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0168 - accuracy: 0.6442\n",
            "Epoch 03864: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0529 - accuracy: 0.6351 - val_loss: 2.9249 - val_accuracy: 0.4658\n",
            "Epoch 3865/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1241 - accuracy: 0.6041\n",
            "Epoch 03865: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.1241 - accuracy: 0.6041 - val_loss: 3.0868 - val_accuracy: 0.4384\n",
            "Epoch 3866/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1427 - accuracy: 0.6335\n",
            "Epoch 03866: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0956 - accuracy: 0.6317 - val_loss: 3.0141 - val_accuracy: 0.4110\n",
            "Epoch 3867/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0057 - accuracy: 0.6178\n",
            "Epoch 03867: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9934 - accuracy: 0.6179 - val_loss: 3.0853 - val_accuracy: 0.4247\n",
            "Epoch 3868/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0502 - accuracy: 0.6181\n",
            "Epoch 03868: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0976 - accuracy: 0.6127 - val_loss: 3.1304 - val_accuracy: 0.4589\n",
            "Epoch 3869/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0015 - accuracy: 0.6484\n",
            "Epoch 03869: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9958 - accuracy: 0.6523 - val_loss: 3.1208 - val_accuracy: 0.4521\n",
            "Epoch 3870/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9950 - accuracy: 0.6484\n",
            "Epoch 03870: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0230 - accuracy: 0.6386 - val_loss: 3.2303 - val_accuracy: 0.4315\n",
            "Epoch 3871/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0185 - accuracy: 0.6531\n",
            "Epoch 03871: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0781 - accuracy: 0.6231 - val_loss: 3.0981 - val_accuracy: 0.4178\n",
            "Epoch 3872/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0464 - accuracy: 0.6222\n",
            "Epoch 03872: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0660 - accuracy: 0.6213 - val_loss: 3.2378 - val_accuracy: 0.3973\n",
            "Epoch 3873/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0117 - accuracy: 0.6354\n",
            "Epoch 03873: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0196 - accuracy: 0.6523 - val_loss: 3.1800 - val_accuracy: 0.4178\n",
            "Epoch 3874/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0754 - accuracy: 0.6193\n",
            "Epoch 03874: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0381 - accuracy: 0.6282 - val_loss: 3.0935 - val_accuracy: 0.4041\n",
            "Epoch 3875/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0894 - accuracy: 0.6406\n",
            "Epoch 03875: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0635 - accuracy: 0.6472 - val_loss: 3.1717 - val_accuracy: 0.4589\n",
            "Epoch 3876/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0154 - accuracy: 0.6536\n",
            "Epoch 03876: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0690 - accuracy: 0.6351 - val_loss: 2.9985 - val_accuracy: 0.4247\n",
            "Epoch 3877/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1585 - accuracy: 0.6024\n",
            "Epoch 03877: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1585 - accuracy: 0.6024 - val_loss: 2.9496 - val_accuracy: 0.4315\n",
            "Epoch 3878/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9836 - accuracy: 0.6797\n",
            "Epoch 03878: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0253 - accuracy: 0.6558 - val_loss: 2.9321 - val_accuracy: 0.4041\n",
            "Epoch 3879/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0400 - accuracy: 0.6771\n",
            "Epoch 03879: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0224 - accuracy: 0.6661 - val_loss: 2.8694 - val_accuracy: 0.4247\n",
            "Epoch 3880/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0094 - accuracy: 0.6562\n",
            "Epoch 03880: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9912 - accuracy: 0.6523 - val_loss: 2.9890 - val_accuracy: 0.4315\n",
            "Epoch 3881/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0444 - accuracy: 0.6591\n",
            "Epoch 03881: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0461 - accuracy: 0.6558 - val_loss: 2.9426 - val_accuracy: 0.4315\n",
            "Epoch 3882/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0222 - accuracy: 0.6334\n",
            "Epoch 03882: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0222 - accuracy: 0.6334 - val_loss: 2.8480 - val_accuracy: 0.4384\n",
            "Epoch 3883/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9558 - accuracy: 0.6540\n",
            "Epoch 03883: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9558 - accuracy: 0.6540 - val_loss: 2.8802 - val_accuracy: 0.4452\n",
            "Epoch 3884/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9855 - accuracy: 0.6644\n",
            "Epoch 03884: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9855 - accuracy: 0.6644 - val_loss: 2.9051 - val_accuracy: 0.4521\n",
            "Epoch 3885/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0462 - accuracy: 0.6392\n",
            "Epoch 03885: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0348 - accuracy: 0.6334 - val_loss: 2.9248 - val_accuracy: 0.4384\n",
            "Epoch 3886/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0760 - accuracy: 0.6302\n",
            "Epoch 03886: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0368 - accuracy: 0.6489 - val_loss: 2.8715 - val_accuracy: 0.4247\n",
            "Epoch 3887/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1847 - accuracy: 0.6023\n",
            "Epoch 03887: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1103 - accuracy: 0.6213 - val_loss: 2.9148 - val_accuracy: 0.4521\n",
            "Epoch 3888/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0722 - accuracy: 0.6295\n",
            "Epoch 03888: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0453 - accuracy: 0.6386 - val_loss: 2.9085 - val_accuracy: 0.4795\n",
            "Epoch 3889/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0667 - accuracy: 0.6278\n",
            "Epoch 03889: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0919 - accuracy: 0.6334 - val_loss: 3.0141 - val_accuracy: 0.4384\n",
            "Epoch 3890/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.1301 - accuracy: 0.6528\n",
            "Epoch 03890: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0587 - accuracy: 0.6489 - val_loss: 2.9323 - val_accuracy: 0.4589\n",
            "Epoch 3891/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9607 - accuracy: 0.6719\n",
            "Epoch 03891: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0293 - accuracy: 0.6540 - val_loss: 2.9164 - val_accuracy: 0.4521\n",
            "Epoch 3892/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0697 - accuracy: 0.6693\n",
            "Epoch 03892: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1165 - accuracy: 0.6575 - val_loss: 2.9148 - val_accuracy: 0.4589\n",
            "Epoch 3893/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1098 - accuracy: 0.6094\n",
            "Epoch 03893: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1798 - accuracy: 0.6127 - val_loss: 3.1005 - val_accuracy: 0.3904\n",
            "Epoch 3894/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1992 - accuracy: 0.6222\n",
            "Epoch 03894: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1651 - accuracy: 0.6317 - val_loss: 2.9023 - val_accuracy: 0.4384\n",
            "Epoch 3895/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.2171 - accuracy: 0.6108\n",
            "Epoch 03895: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2188 - accuracy: 0.6179 - val_loss: 2.9639 - val_accuracy: 0.4658\n",
            "Epoch 3896/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9254 - accuracy: 0.6615\n",
            "Epoch 03896: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0415 - accuracy: 0.6265 - val_loss: 3.0103 - val_accuracy: 0.4452\n",
            "Epoch 3897/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 0.9743 - accuracy: 0.6636\n",
            "Epoch 03897: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9746 - accuracy: 0.6609 - val_loss: 3.0224 - val_accuracy: 0.4452\n",
            "Epoch 3898/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9975 - accuracy: 0.6641\n",
            "Epoch 03898: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0230 - accuracy: 0.6558 - val_loss: 2.9928 - val_accuracy: 0.4384\n",
            "Epoch 3899/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9944 - accuracy: 0.6274\n",
            "Epoch 03899: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9912 - accuracy: 0.6437 - val_loss: 2.9252 - val_accuracy: 0.4315\n",
            "Epoch 3900/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9635 - accuracy: 0.6534\n",
            "Epoch 03900: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9761 - accuracy: 0.6592 - val_loss: 2.9487 - val_accuracy: 0.4384\n",
            "Epoch 3901/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9191 - accuracy: 0.6615\n",
            "Epoch 03901: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9737 - accuracy: 0.6506 - val_loss: 2.9247 - val_accuracy: 0.4452\n",
            "Epoch 3902/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1024 - accuracy: 0.6154\n",
            "Epoch 03902: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0896 - accuracy: 0.6179 - val_loss: 2.7895 - val_accuracy: 0.4315\n",
            "Epoch 3903/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0070 - accuracy: 0.6380\n",
            "Epoch 03903: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0271 - accuracy: 0.6317 - val_loss: 2.8125 - val_accuracy: 0.4589\n",
            "Epoch 3904/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0212 - accuracy: 0.6278\n",
            "Epoch 03904: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9959 - accuracy: 0.6558 - val_loss: 2.8736 - val_accuracy: 0.4589\n",
            "Epoch 3905/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0180 - accuracy: 0.6619\n",
            "Epoch 03905: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9956 - accuracy: 0.6558 - val_loss: 3.1176 - val_accuracy: 0.4658\n",
            "Epoch 3906/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0161 - accuracy: 0.6627\n",
            "Epoch 03906: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0161 - accuracy: 0.6627 - val_loss: 3.0217 - val_accuracy: 0.4658\n",
            "Epoch 3907/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9610 - accuracy: 0.6761\n",
            "Epoch 03907: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0808 - accuracy: 0.6179 - val_loss: 2.9639 - val_accuracy: 0.4589\n",
            "Epoch 3908/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9623 - accuracy: 0.6589\n",
            "Epoch 03908: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9909 - accuracy: 0.6472 - val_loss: 2.9358 - val_accuracy: 0.4452\n",
            "Epoch 3909/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0293 - accuracy: 0.6392\n",
            "Epoch 03909: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0663 - accuracy: 0.6368 - val_loss: 2.9637 - val_accuracy: 0.4589\n",
            "Epoch 3910/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0901 - accuracy: 0.6042\n",
            "Epoch 03910: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0582 - accuracy: 0.6127 - val_loss: 3.0356 - val_accuracy: 0.4247\n",
            "Epoch 3911/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1026 - accuracy: 0.6375\n",
            "Epoch 03911: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1093 - accuracy: 0.6282 - val_loss: 3.0772 - val_accuracy: 0.4315\n",
            "Epoch 3912/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0347 - accuracy: 0.6375\n",
            "Epoch 03912: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0120 - accuracy: 0.6472 - val_loss: 3.0783 - val_accuracy: 0.4452\n",
            "Epoch 3913/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0171 - accuracy: 0.6442\n",
            "Epoch 03913: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9955 - accuracy: 0.6575 - val_loss: 3.2195 - val_accuracy: 0.4247\n",
            "Epoch 3914/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0521 - accuracy: 0.6198\n",
            "Epoch 03914: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9965 - accuracy: 0.6454 - val_loss: 3.1791 - val_accuracy: 0.4384\n",
            "Epoch 3915/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0660 - accuracy: 0.6219\n",
            "Epoch 03915: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0717 - accuracy: 0.6196 - val_loss: 3.1098 - val_accuracy: 0.4315\n",
            "Epoch 3916/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9345 - accuracy: 0.6510\n",
            "Epoch 03916: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0097 - accuracy: 0.6334 - val_loss: 3.2460 - val_accuracy: 0.4315\n",
            "Epoch 3917/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 1.0811 - accuracy: 0.6268\n",
            "Epoch 03917: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0760 - accuracy: 0.6299 - val_loss: 3.1401 - val_accuracy: 0.3973\n",
            "Epoch 3918/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1483 - accuracy: 0.6307\n",
            "Epoch 03918: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1243 - accuracy: 0.6317 - val_loss: 3.2605 - val_accuracy: 0.3836\n",
            "Epoch 3919/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0737 - accuracy: 0.6354\n",
            "Epoch 03919: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0574 - accuracy: 0.6351 - val_loss: 3.0803 - val_accuracy: 0.3767\n",
            "Epoch 3920/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0150 - accuracy: 0.6406\n",
            "Epoch 03920: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0051 - accuracy: 0.6351 - val_loss: 3.1284 - val_accuracy: 0.3973\n",
            "Epoch 3921/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 0.9936 - accuracy: 0.6496\n",
            "Epoch 03921: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.0269 - accuracy: 0.6403 - val_loss: 3.1397 - val_accuracy: 0.4110\n",
            "Epoch 3922/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0301 - accuracy: 0.6193\n",
            "Epoch 03922: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0272 - accuracy: 0.6213 - val_loss: 3.1224 - val_accuracy: 0.4247\n",
            "Epoch 3923/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.1350 - accuracy: 0.6183\n",
            "Epoch 03923: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.1366 - accuracy: 0.6179 - val_loss: 3.0192 - val_accuracy: 0.4384\n",
            "Epoch 3924/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0625 - accuracy: 0.6558\n",
            "Epoch 03924: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0625 - accuracy: 0.6558 - val_loss: 3.0457 - val_accuracy: 0.4247\n",
            "Epoch 3925/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0157 - accuracy: 0.6346\n",
            "Epoch 03925: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9836 - accuracy: 0.6420 - val_loss: 3.0114 - val_accuracy: 0.4178\n",
            "Epoch 3926/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0679 - accuracy: 0.6250\n",
            "Epoch 03926: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0216 - accuracy: 0.6351 - val_loss: 3.0687 - val_accuracy: 0.4110\n",
            "Epoch 3927/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0609 - accuracy: 0.6202\n",
            "Epoch 03927: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0139 - accuracy: 0.6454 - val_loss: 3.1734 - val_accuracy: 0.4315\n",
            "Epoch 3928/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0852 - accuracy: 0.6298\n",
            "Epoch 03928: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0942 - accuracy: 0.6403 - val_loss: 3.0694 - val_accuracy: 0.4521\n",
            "Epoch 3929/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0499 - accuracy: 0.6406\n",
            "Epoch 03929: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0535 - accuracy: 0.6489 - val_loss: 2.9424 - val_accuracy: 0.4041\n",
            "Epoch 3930/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9379 - accuracy: 0.6849\n",
            "Epoch 03930: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9685 - accuracy: 0.6678 - val_loss: 2.9886 - val_accuracy: 0.4041\n",
            "Epoch 3931/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9950 - accuracy: 0.6562\n",
            "Epoch 03931: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0103 - accuracy: 0.6472 - val_loss: 3.0948 - val_accuracy: 0.4315\n",
            "Epoch 3932/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0596 - accuracy: 0.6319\n",
            "Epoch 03932: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0565 - accuracy: 0.6334 - val_loss: 3.1163 - val_accuracy: 0.4521\n",
            "Epoch 3933/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0607 - accuracy: 0.6172\n",
            "Epoch 03933: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0139 - accuracy: 0.6403 - val_loss: 2.8755 - val_accuracy: 0.4384\n",
            "Epoch 3934/5000\n",
            " 8/19 [===========>..................] - ETA: 0s - loss: 1.0022 - accuracy: 0.6523\n",
            "Epoch 03934: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0656 - accuracy: 0.6317 - val_loss: 3.0020 - val_accuracy: 0.4315\n",
            "Epoch 3935/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1020 - accuracy: 0.6198\n",
            "Epoch 03935: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1009 - accuracy: 0.6265 - val_loss: 2.9509 - val_accuracy: 0.4315\n",
            "Epoch 3936/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0343 - accuracy: 0.6313\n",
            "Epoch 03936: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0651 - accuracy: 0.6162 - val_loss: 3.1077 - val_accuracy: 0.4315\n",
            "Epoch 3937/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1311 - accuracy: 0.6364\n",
            "Epoch 03937: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0632 - accuracy: 0.6386 - val_loss: 3.1760 - val_accuracy: 0.4589\n",
            "Epoch 3938/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 0.9198 - accuracy: 0.6854\n",
            "Epoch 03938: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.9332 - accuracy: 0.6730 - val_loss: 3.1583 - val_accuracy: 0.4384\n",
            "Epoch 3939/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0065 - accuracy: 0.6370\n",
            "Epoch 03939: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0002 - accuracy: 0.6334 - val_loss: 3.1244 - val_accuracy: 0.4589\n",
            "Epoch 3940/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9601 - accuracy: 0.6587\n",
            "Epoch 03940: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.9680 - accuracy: 0.6575 - val_loss: 3.0832 - val_accuracy: 0.4315\n",
            "Epoch 3941/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0174 - accuracy: 0.6506\n",
            "Epoch 03941: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9887 - accuracy: 0.6540 - val_loss: 3.1022 - val_accuracy: 0.4452\n",
            "Epoch 3942/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0051 - accuracy: 0.6477\n",
            "Epoch 03942: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0108 - accuracy: 0.6454 - val_loss: 3.1732 - val_accuracy: 0.4315\n",
            "Epoch 3943/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0411 - accuracy: 0.6354\n",
            "Epoch 03943: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0413 - accuracy: 0.6437 - val_loss: 3.2380 - val_accuracy: 0.4315\n",
            "Epoch 3944/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 1.0313 - accuracy: 0.6287\n",
            "Epoch 03944: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0531 - accuracy: 0.6196 - val_loss: 3.2306 - val_accuracy: 0.4384\n",
            "Epoch 3945/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9883 - accuracy: 0.6625\n",
            "Epoch 03945: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0024 - accuracy: 0.6523 - val_loss: 3.1228 - val_accuracy: 0.4384\n",
            "Epoch 3946/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0205 - accuracy: 0.6493\n",
            "Epoch 03946: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0199 - accuracy: 0.6489 - val_loss: 3.1429 - val_accuracy: 0.4726\n",
            "Epoch 3947/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1346 - accuracy: 0.6307\n",
            "Epoch 03947: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0973 - accuracy: 0.6437 - val_loss: 2.9791 - val_accuracy: 0.4658\n",
            "Epoch 3948/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0586 - accuracy: 0.6558\n",
            "Epoch 03948: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0586 - accuracy: 0.6558 - val_loss: 3.0108 - val_accuracy: 0.4110\n",
            "Epoch 3949/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0926 - accuracy: 0.6248\n",
            "Epoch 03949: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0926 - accuracy: 0.6248 - val_loss: 2.7842 - val_accuracy: 0.4178\n",
            "Epoch 3950/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1368 - accuracy: 0.6328\n",
            "Epoch 03950: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1220 - accuracy: 0.6437 - val_loss: 2.7111 - val_accuracy: 0.4521\n",
            "Epoch 3951/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1127 - accuracy: 0.6406\n",
            "Epoch 03951: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0954 - accuracy: 0.6368 - val_loss: 2.7460 - val_accuracy: 0.4726\n",
            "Epoch 3952/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0332 - accuracy: 0.6420\n",
            "Epoch 03952: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0301 - accuracy: 0.6472 - val_loss: 2.8832 - val_accuracy: 0.4521\n",
            "Epoch 3953/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1413 - accuracy: 0.5769\n",
            "Epoch 03953: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1054 - accuracy: 0.5990 - val_loss: 2.8535 - val_accuracy: 0.4589\n",
            "Epoch 3954/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9231 - accuracy: 0.6587\n",
            "Epoch 03954: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9601 - accuracy: 0.6558 - val_loss: 2.7514 - val_accuracy: 0.4658\n",
            "Epoch 3955/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0138 - accuracy: 0.6282\n",
            "Epoch 03955: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0138 - accuracy: 0.6282 - val_loss: 2.8682 - val_accuracy: 0.4384\n",
            "Epoch 3956/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9812 - accuracy: 0.6562\n",
            "Epoch 03956: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0219 - accuracy: 0.6472 - val_loss: 2.8723 - val_accuracy: 0.4384\n",
            "Epoch 3957/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0766 - accuracy: 0.5966\n",
            "Epoch 03957: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0114 - accuracy: 0.6248 - val_loss: 2.9187 - val_accuracy: 0.4726\n",
            "Epoch 3958/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1078 - accuracy: 0.6224\n",
            "Epoch 03958: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0886 - accuracy: 0.6351 - val_loss: 2.9711 - val_accuracy: 0.4315\n",
            "Epoch 3959/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0340 - accuracy: 0.6154\n",
            "Epoch 03959: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0497 - accuracy: 0.6196 - val_loss: 3.0158 - val_accuracy: 0.4589\n",
            "Epoch 3960/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0159 - accuracy: 0.6648\n",
            "Epoch 03960: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0230 - accuracy: 0.6523 - val_loss: 2.9493 - val_accuracy: 0.4315\n",
            "Epoch 3961/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0186 - accuracy: 0.6484\n",
            "Epoch 03961: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0646 - accuracy: 0.6351 - val_loss: 2.9289 - val_accuracy: 0.4315\n",
            "Epoch 3962/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0993 - accuracy: 0.6418\n",
            "Epoch 03962: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0481 - accuracy: 0.6609 - val_loss: 2.8947 - val_accuracy: 0.4589\n",
            "Epoch 3963/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0614 - accuracy: 0.6418\n",
            "Epoch 03963: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0570 - accuracy: 0.6437 - val_loss: 2.9590 - val_accuracy: 0.4521\n",
            "Epoch 3964/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1193 - accuracy: 0.6322\n",
            "Epoch 03964: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0732 - accuracy: 0.6403 - val_loss: 2.9441 - val_accuracy: 0.4658\n",
            "Epoch 3965/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0334 - accuracy: 0.6458\n",
            "Epoch 03965: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0399 - accuracy: 0.6403 - val_loss: 2.9156 - val_accuracy: 0.4521\n",
            "Epoch 3966/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0614 - accuracy: 0.6344\n",
            "Epoch 03966: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0274 - accuracy: 0.6506 - val_loss: 2.9744 - val_accuracy: 0.4658\n",
            "Epoch 3967/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0174 - accuracy: 0.6641\n",
            "Epoch 03967: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9879 - accuracy: 0.6575 - val_loss: 2.9883 - val_accuracy: 0.4452\n",
            "Epoch 3968/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0416 - accuracy: 0.6016\n",
            "Epoch 03968: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0193 - accuracy: 0.6145 - val_loss: 2.8590 - val_accuracy: 0.4452\n",
            "Epoch 3969/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0000 - accuracy: 0.6432\n",
            "Epoch 03969: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0418 - accuracy: 0.6334 - val_loss: 2.8600 - val_accuracy: 0.4452\n",
            "Epoch 3970/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 0.9947 - accuracy: 0.6636\n",
            "Epoch 03970: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0008 - accuracy: 0.6575 - val_loss: 2.8412 - val_accuracy: 0.4452\n",
            "Epoch 3971/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9229 - accuracy: 0.6619\n",
            "Epoch 03971: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0243 - accuracy: 0.6540 - val_loss: 2.9915 - val_accuracy: 0.4247\n",
            "Epoch 3972/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0061 - accuracy: 0.6641\n",
            "Epoch 03972: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0077 - accuracy: 0.6523 - val_loss: 2.9998 - val_accuracy: 0.4178\n",
            "Epoch 3973/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0432 - accuracy: 0.6477\n",
            "Epoch 03973: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0172 - accuracy: 0.6523 - val_loss: 3.0195 - val_accuracy: 0.4452\n",
            "Epoch 3974/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0020 - accuracy: 0.6335\n",
            "Epoch 03974: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9977 - accuracy: 0.6334 - val_loss: 3.1588 - val_accuracy: 0.4658\n",
            "Epoch 3975/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.2463 - accuracy: 0.6250\n",
            "Epoch 03975: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2842 - accuracy: 0.6127 - val_loss: 3.1208 - val_accuracy: 0.4384\n",
            "Epoch 3976/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1531 - accuracy: 0.6219\n",
            "Epoch 03976: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.2183 - accuracy: 0.6213 - val_loss: 2.8636 - val_accuracy: 0.4452\n",
            "Epoch 3977/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1467 - accuracy: 0.6068\n",
            "Epoch 03977: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0813 - accuracy: 0.6162 - val_loss: 2.7340 - val_accuracy: 0.4384\n",
            "Epoch 3978/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0579 - accuracy: 0.6354\n",
            "Epoch 03978: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0536 - accuracy: 0.6368 - val_loss: 2.7830 - val_accuracy: 0.4315\n",
            "Epoch 3979/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.1336 - accuracy: 0.5955\n",
            "Epoch 03979: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1291 - accuracy: 0.5990 - val_loss: 2.8658 - val_accuracy: 0.4178\n",
            "Epoch 3980/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0326 - accuracy: 0.6493\n",
            "Epoch 03980: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0455 - accuracy: 0.6265 - val_loss: 2.8837 - val_accuracy: 0.4247\n",
            "Epoch 3981/5000\n",
            " 8/19 [===========>..................] - ETA: 0s - loss: 1.2151 - accuracy: 0.6406\n",
            "Epoch 03981: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1470 - accuracy: 0.6403 - val_loss: 2.7431 - val_accuracy: 0.4247\n",
            "Epoch 3982/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1452 - accuracy: 0.6222\n",
            "Epoch 03982: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1112 - accuracy: 0.6351 - val_loss: 2.8377 - val_accuracy: 0.4315\n",
            "Epoch 3983/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1383 - accuracy: 0.6165\n",
            "Epoch 03983: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0254 - accuracy: 0.6575 - val_loss: 2.8464 - val_accuracy: 0.4521\n",
            "Epoch 3984/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9842 - accuracy: 0.6745\n",
            "Epoch 03984: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0438 - accuracy: 0.6368 - val_loss: 2.9445 - val_accuracy: 0.4315\n",
            "Epoch 3985/5000\n",
            " 8/19 [===========>..................] - ETA: 0s - loss: 1.0227 - accuracy: 0.6328\n",
            "Epoch 03985: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0747 - accuracy: 0.6351 - val_loss: 2.9803 - val_accuracy: 0.4315\n",
            "Epoch 3986/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0814 - accuracy: 0.6510\n",
            "Epoch 03986: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0521 - accuracy: 0.6523 - val_loss: 2.9320 - val_accuracy: 0.4384\n",
            "Epoch 3987/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9890 - accuracy: 0.6490\n",
            "Epoch 03987: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9928 - accuracy: 0.6489 - val_loss: 2.9002 - val_accuracy: 0.4178\n",
            "Epoch 3988/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0245 - accuracy: 0.6364\n",
            "Epoch 03988: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9967 - accuracy: 0.6489 - val_loss: 2.9594 - val_accuracy: 0.4315\n",
            "Epoch 3989/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0112 - accuracy: 0.6299\n",
            "Epoch 03989: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0112 - accuracy: 0.6299 - val_loss: 2.9814 - val_accuracy: 0.4521\n",
            "Epoch 3990/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9862 - accuracy: 0.6844\n",
            "Epoch 03990: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0321 - accuracy: 0.6454 - val_loss: 2.9007 - val_accuracy: 0.4315\n",
            "Epoch 3991/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0663 - accuracy: 0.6380\n",
            "Epoch 03991: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1275 - accuracy: 0.6248 - val_loss: 3.0519 - val_accuracy: 0.4315\n",
            "Epoch 3992/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0707 - accuracy: 0.6276\n",
            "Epoch 03992: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0763 - accuracy: 0.6282 - val_loss: 2.9779 - val_accuracy: 0.4110\n",
            "Epoch 3993/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0351 - accuracy: 0.6406\n",
            "Epoch 03993: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0099 - accuracy: 0.6403 - val_loss: 3.0347 - val_accuracy: 0.4178\n",
            "Epoch 3994/5000\n",
            "16/19 [========================>.....] - ETA: 0s - loss: 1.0797 - accuracy: 0.6445\n",
            "Epoch 03994: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0812 - accuracy: 0.6386 - val_loss: 3.0801 - val_accuracy: 0.4247\n",
            "Epoch 3995/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0544 - accuracy: 0.6193\n",
            "Epoch 03995: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0487 - accuracy: 0.6145 - val_loss: 2.9502 - val_accuracy: 0.4178\n",
            "Epoch 3996/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0605 - accuracy: 0.6274\n",
            "Epoch 03996: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0816 - accuracy: 0.6196 - val_loss: 2.9695 - val_accuracy: 0.4178\n",
            "Epoch 3997/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0233 - accuracy: 0.6562\n",
            "Epoch 03997: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0025 - accuracy: 0.6644 - val_loss: 3.0663 - val_accuracy: 0.4247\n",
            "Epoch 3998/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9445 - accuracy: 0.6693\n",
            "Epoch 03998: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9948 - accuracy: 0.6437 - val_loss: 3.0290 - val_accuracy: 0.4247\n",
            "Epoch 3999/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1392 - accuracy: 0.6120\n",
            "Epoch 03999: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0446 - accuracy: 0.6420 - val_loss: 2.9835 - val_accuracy: 0.4452\n",
            "Epoch 4000/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1542 - accuracy: 0.6146\n",
            "Epoch 04000: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1391 - accuracy: 0.6162 - val_loss: 2.9776 - val_accuracy: 0.4110\n",
            "Epoch 4001/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0582 - accuracy: 0.6016\n",
            "Epoch 04001: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0683 - accuracy: 0.6093 - val_loss: 3.1040 - val_accuracy: 0.4315\n",
            "Epoch 4002/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0703 - accuracy: 0.6276\n",
            "Epoch 04002: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0453 - accuracy: 0.6437 - val_loss: 3.1654 - val_accuracy: 0.4247\n",
            "Epoch 4003/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.9932 - accuracy: 0.6632\n",
            "Epoch 04003: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9927 - accuracy: 0.6627 - val_loss: 3.2141 - val_accuracy: 0.4247\n",
            "Epoch 4004/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9565 - accuracy: 0.6442\n",
            "Epoch 04004: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0280 - accuracy: 0.6334 - val_loss: 3.1620 - val_accuracy: 0.4384\n",
            "Epoch 4005/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0233 - accuracy: 0.6484\n",
            "Epoch 04005: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0310 - accuracy: 0.6403 - val_loss: 3.2873 - val_accuracy: 0.4452\n",
            "Epoch 4006/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0197 - accuracy: 0.6790\n",
            "Epoch 04006: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0370 - accuracy: 0.6592 - val_loss: 3.1999 - val_accuracy: 0.4247\n",
            "Epoch 4007/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 0.9671 - accuracy: 0.6607\n",
            "Epoch 04007: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9810 - accuracy: 0.6730 - val_loss: 3.2276 - val_accuracy: 0.4315\n",
            "Epoch 4008/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9544 - accuracy: 0.6667\n",
            "Epoch 04008: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9734 - accuracy: 0.6627 - val_loss: 3.1322 - val_accuracy: 0.4247\n",
            "Epoch 4009/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9829 - accuracy: 0.6615\n",
            "Epoch 04009: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0409 - accuracy: 0.6454 - val_loss: 3.2095 - val_accuracy: 0.4521\n",
            "Epoch 4010/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 1.0232 - accuracy: 0.6562\n",
            "Epoch 04010: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0414 - accuracy: 0.6540 - val_loss: 3.0800 - val_accuracy: 0.4384\n",
            "Epoch 4011/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9865 - accuracy: 0.6458\n",
            "Epoch 04011: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0394 - accuracy: 0.6248 - val_loss: 3.1255 - val_accuracy: 0.4452\n",
            "Epoch 4012/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9544 - accuracy: 0.6693\n",
            "Epoch 04012: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9795 - accuracy: 0.6678 - val_loss: 3.1179 - val_accuracy: 0.4521\n",
            "Epoch 4013/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9989 - accuracy: 0.6538\n",
            "Epoch 04013: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9818 - accuracy: 0.6523 - val_loss: 3.4301 - val_accuracy: 0.4384\n",
            "Epoch 4014/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0048 - accuracy: 0.6562\n",
            "Epoch 04014: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9933 - accuracy: 0.6558 - val_loss: 3.0788 - val_accuracy: 0.4315\n",
            "Epoch 4015/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9774 - accuracy: 0.6531\n",
            "Epoch 04015: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9993 - accuracy: 0.6609 - val_loss: 3.0608 - val_accuracy: 0.4110\n",
            "Epoch 4016/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0008 - accuracy: 0.6420\n",
            "Epoch 04016: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9944 - accuracy: 0.6420 - val_loss: 3.2810 - val_accuracy: 0.4041\n",
            "Epoch 4017/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0753 - accuracy: 0.6298\n",
            "Epoch 04017: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0697 - accuracy: 0.6282 - val_loss: 3.2274 - val_accuracy: 0.4315\n",
            "Epoch 4018/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1046 - accuracy: 0.6224\n",
            "Epoch 04018: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.2403 - accuracy: 0.6248 - val_loss: 3.0845 - val_accuracy: 0.4521\n",
            "Epoch 4019/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0681 - accuracy: 0.6418\n",
            "Epoch 04019: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0814 - accuracy: 0.6334 - val_loss: 3.0659 - val_accuracy: 0.4384\n",
            "Epoch 4020/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1396 - accuracy: 0.6202\n",
            "Epoch 04020: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1442 - accuracy: 0.6213 - val_loss: 3.0133 - val_accuracy: 0.4452\n",
            "Epoch 4021/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1261 - accuracy: 0.6106\n",
            "Epoch 04021: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1189 - accuracy: 0.6041 - val_loss: 3.0444 - val_accuracy: 0.4452\n",
            "Epoch 4022/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0356 - accuracy: 0.6202\n",
            "Epoch 04022: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0482 - accuracy: 0.6213 - val_loss: 3.0881 - val_accuracy: 0.4384\n",
            "Epoch 4023/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0260 - accuracy: 0.5739\n",
            "Epoch 04023: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0019 - accuracy: 0.6041 - val_loss: 3.0735 - val_accuracy: 0.4521\n",
            "Epoch 4024/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.2545 - accuracy: 0.6023\n",
            "Epoch 04024: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1537 - accuracy: 0.6196 - val_loss: 3.0453 - val_accuracy: 0.4589\n",
            "Epoch 4025/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9812 - accuracy: 0.6250\n",
            "Epoch 04025: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0295 - accuracy: 0.6179 - val_loss: 3.2043 - val_accuracy: 0.4384\n",
            "Epoch 4026/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.1352 - accuracy: 0.5903\n",
            "Epoch 04026: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1033 - accuracy: 0.6110 - val_loss: 3.1419 - val_accuracy: 0.4315\n",
            "Epoch 4027/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0700 - accuracy: 0.6068\n",
            "Epoch 04027: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0919 - accuracy: 0.6059 - val_loss: 3.0791 - val_accuracy: 0.4178\n",
            "Epoch 4028/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1343 - accuracy: 0.6172\n",
            "Epoch 04028: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1299 - accuracy: 0.6145 - val_loss: 3.0486 - val_accuracy: 0.4247\n",
            "Epoch 4029/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.1064 - accuracy: 0.5893\n",
            "Epoch 04029: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1187 - accuracy: 0.5886 - val_loss: 3.1706 - val_accuracy: 0.4452\n",
            "Epoch 4030/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.2992 - accuracy: 0.6000\n",
            "Epoch 04030: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2290 - accuracy: 0.6024 - val_loss: 2.9089 - val_accuracy: 0.4384\n",
            "Epoch 4031/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0966 - accuracy: 0.6027\n",
            "Epoch 04031: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0881 - accuracy: 0.6093 - val_loss: 2.8817 - val_accuracy: 0.4521\n",
            "Epoch 4032/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0296 - accuracy: 0.6276\n",
            "Epoch 04032: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0375 - accuracy: 0.6403 - val_loss: 3.0550 - val_accuracy: 0.4384\n",
            "Epoch 4033/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9727 - accuracy: 0.6354\n",
            "Epoch 04033: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0384 - accuracy: 0.6127 - val_loss: 3.1872 - val_accuracy: 0.4247\n",
            "Epoch 4034/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0566 - accuracy: 0.6420\n",
            "Epoch 04034: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0365 - accuracy: 0.6558 - val_loss: 3.1590 - val_accuracy: 0.4315\n",
            "Epoch 4035/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0104 - accuracy: 0.6375\n",
            "Epoch 04035: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0510 - accuracy: 0.6472 - val_loss: 3.1113 - val_accuracy: 0.4315\n",
            "Epoch 4036/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9707 - accuracy: 0.6667\n",
            "Epoch 04036: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9942 - accuracy: 0.6609 - val_loss: 3.1592 - val_accuracy: 0.4315\n",
            "Epoch 4037/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9224 - accuracy: 0.6885\n",
            "Epoch 04037: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9224 - accuracy: 0.6885 - val_loss: 3.1848 - val_accuracy: 0.4589\n",
            "Epoch 4038/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0236 - accuracy: 0.6562\n",
            "Epoch 04038: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0265 - accuracy: 0.6506 - val_loss: 3.2336 - val_accuracy: 0.4315\n",
            "Epoch 4039/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0149 - accuracy: 0.6281\n",
            "Epoch 04039: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9703 - accuracy: 0.6592 - val_loss: 3.1155 - val_accuracy: 0.4315\n",
            "Epoch 4040/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.8894 - accuracy: 0.6818\n",
            "Epoch 04040: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9118 - accuracy: 0.6764 - val_loss: 3.2478 - val_accuracy: 0.4315\n",
            "Epoch 4041/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9426 - accuracy: 0.6676\n",
            "Epoch 04041: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9862 - accuracy: 0.6661 - val_loss: 3.1458 - val_accuracy: 0.4521\n",
            "Epoch 4042/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9793 - accuracy: 0.6676\n",
            "Epoch 04042: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9979 - accuracy: 0.6506 - val_loss: 3.0406 - val_accuracy: 0.4589\n",
            "Epoch 4043/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9233 - accuracy: 0.6847\n",
            "Epoch 04043: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9568 - accuracy: 0.6644 - val_loss: 3.0822 - val_accuracy: 0.4452\n",
            "Epoch 4044/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 1.0049 - accuracy: 0.6691\n",
            "Epoch 04044: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9826 - accuracy: 0.6713 - val_loss: 3.0695 - val_accuracy: 0.4521\n",
            "Epoch 4045/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0373 - accuracy: 0.6449\n",
            "Epoch 04045: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0402 - accuracy: 0.6386 - val_loss: 3.0017 - val_accuracy: 0.4452\n",
            "Epoch 4046/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.1509 - accuracy: 0.6004\n",
            "Epoch 04046: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1135 - accuracy: 0.6162 - val_loss: 3.0962 - val_accuracy: 0.4452\n",
            "Epoch 4047/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0057 - accuracy: 0.6313\n",
            "Epoch 04047: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9724 - accuracy: 0.6609 - val_loss: 3.1592 - val_accuracy: 0.4452\n",
            "Epoch 4048/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 0.9231 - accuracy: 0.6597\n",
            "Epoch 04048: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0679 - accuracy: 0.6351 - val_loss: 3.0933 - val_accuracy: 0.4315\n",
            "Epoch 4049/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9344 - accuracy: 0.6733\n",
            "Epoch 04049: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9483 - accuracy: 0.6730 - val_loss: 3.1099 - val_accuracy: 0.4589\n",
            "Epoch 4050/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9563 - accuracy: 0.6558\n",
            "Epoch 04050: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9563 - accuracy: 0.6558 - val_loss: 3.1587 - val_accuracy: 0.4384\n",
            "Epoch 4051/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9876 - accuracy: 0.6328\n",
            "Epoch 04051: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0254 - accuracy: 0.6265 - val_loss: 3.2222 - val_accuracy: 0.4452\n",
            "Epoch 4052/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.8419 - accuracy: 0.6875\n",
            "Epoch 04052: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.8995 - accuracy: 0.6781 - val_loss: 3.2389 - val_accuracy: 0.4589\n",
            "Epoch 4053/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9126 - accuracy: 0.6932\n",
            "Epoch 04053: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9661 - accuracy: 0.6730 - val_loss: 3.3155 - val_accuracy: 0.4452\n",
            "Epoch 4054/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1230 - accuracy: 0.6432\n",
            "Epoch 04054: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0762 - accuracy: 0.6317 - val_loss: 3.1118 - val_accuracy: 0.4658\n",
            "Epoch 4055/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1095 - accuracy: 0.6442\n",
            "Epoch 04055: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0791 - accuracy: 0.6609 - val_loss: 3.0265 - val_accuracy: 0.4726\n",
            "Epoch 4056/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0770 - accuracy: 0.6693\n",
            "Epoch 04056: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0745 - accuracy: 0.6730 - val_loss: 2.9971 - val_accuracy: 0.4589\n",
            "Epoch 4057/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0344 - accuracy: 0.6472\n",
            "Epoch 04057: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0344 - accuracy: 0.6472 - val_loss: 3.0048 - val_accuracy: 0.4315\n",
            "Epoch 4058/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9113 - accuracy: 0.6719\n",
            "Epoch 04058: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9193 - accuracy: 0.6644 - val_loss: 3.0381 - val_accuracy: 0.4384\n",
            "Epoch 4059/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9153 - accuracy: 0.7019\n",
            "Epoch 04059: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9729 - accuracy: 0.6730 - val_loss: 2.9684 - val_accuracy: 0.4452\n",
            "Epoch 4060/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0075 - accuracy: 0.6380\n",
            "Epoch 04060: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9901 - accuracy: 0.6506 - val_loss: 2.9879 - val_accuracy: 0.4315\n",
            "Epoch 4061/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9207 - accuracy: 0.6707\n",
            "Epoch 04061: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9344 - accuracy: 0.6661 - val_loss: 3.0103 - val_accuracy: 0.4452\n",
            "Epoch 4062/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 0.9518 - accuracy: 0.6585\n",
            "Epoch 04062: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9489 - accuracy: 0.6644 - val_loss: 3.0513 - val_accuracy: 0.4452\n",
            "Epoch 4063/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9754 - accuracy: 0.6635\n",
            "Epoch 04063: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9360 - accuracy: 0.6730 - val_loss: 2.9708 - val_accuracy: 0.4247\n",
            "Epoch 4064/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0074 - accuracy: 0.6562\n",
            "Epoch 04064: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9824 - accuracy: 0.6592 - val_loss: 2.9949 - val_accuracy: 0.4452\n",
            "Epoch 4065/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9634 - accuracy: 0.6375\n",
            "Epoch 04065: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0135 - accuracy: 0.6231 - val_loss: 3.0836 - val_accuracy: 0.4658\n",
            "Epoch 4066/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9323 - accuracy: 0.6771\n",
            "Epoch 04066: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9726 - accuracy: 0.6609 - val_loss: 3.0811 - val_accuracy: 0.4726\n",
            "Epoch 4067/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9278 - accuracy: 0.6683\n",
            "Epoch 04067: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9743 - accuracy: 0.6523 - val_loss: 3.0954 - val_accuracy: 0.4521\n",
            "Epoch 4068/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9738 - accuracy: 0.6224\n",
            "Epoch 04068: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9190 - accuracy: 0.6368 - val_loss: 3.1392 - val_accuracy: 0.4452\n",
            "Epoch 4069/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0492 - accuracy: 0.6406\n",
            "Epoch 04069: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0177 - accuracy: 0.6351 - val_loss: 3.1104 - val_accuracy: 0.4452\n",
            "Epoch 4070/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.0480 - accuracy: 0.6479\n",
            "Epoch 04070: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0221 - accuracy: 0.6523 - val_loss: 3.0889 - val_accuracy: 0.4452\n",
            "Epoch 4071/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9913 - accuracy: 0.6299\n",
            "Epoch 04071: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9913 - accuracy: 0.6299 - val_loss: 3.0166 - val_accuracy: 0.4384\n",
            "Epoch 4072/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0139 - accuracy: 0.6380\n",
            "Epoch 04072: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0138 - accuracy: 0.6403 - val_loss: 3.0237 - val_accuracy: 0.4315\n",
            "Epoch 4073/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.9738 - accuracy: 0.6493\n",
            "Epoch 04073: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9750 - accuracy: 0.6489 - val_loss: 3.1041 - val_accuracy: 0.4247\n",
            "Epoch 4074/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0386 - accuracy: 0.6380\n",
            "Epoch 04074: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9680 - accuracy: 0.6644 - val_loss: 3.0308 - val_accuracy: 0.4589\n",
            "Epoch 4075/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1145 - accuracy: 0.6490\n",
            "Epoch 04075: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1583 - accuracy: 0.6454 - val_loss: 3.1671 - val_accuracy: 0.4452\n",
            "Epoch 4076/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0279 - accuracy: 0.6562\n",
            "Epoch 04076: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9848 - accuracy: 0.6540 - val_loss: 3.1795 - val_accuracy: 0.4110\n",
            "Epoch 4077/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0146 - accuracy: 0.6328\n",
            "Epoch 04077: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0094 - accuracy: 0.6575 - val_loss: 3.1083 - val_accuracy: 0.4247\n",
            "Epoch 4078/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0084 - accuracy: 0.6609\n",
            "Epoch 04078: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0084 - accuracy: 0.6609 - val_loss: 3.2401 - val_accuracy: 0.4384\n",
            "Epoch 4079/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0008 - accuracy: 0.6812\n",
            "Epoch 04079: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0172 - accuracy: 0.6850 - val_loss: 3.0618 - val_accuracy: 0.4521\n",
            "Epoch 4080/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0250 - accuracy: 0.6506\n",
            "Epoch 04080: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0250 - accuracy: 0.6506 - val_loss: 3.0771 - val_accuracy: 0.4452\n",
            "Epoch 4081/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9771 - accuracy: 0.6615\n",
            "Epoch 04081: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9924 - accuracy: 0.6472 - val_loss: 3.0590 - val_accuracy: 0.4452\n",
            "Epoch 4082/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0593 - accuracy: 0.6432\n",
            "Epoch 04082: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9969 - accuracy: 0.6523 - val_loss: 3.0798 - val_accuracy: 0.4589\n",
            "Epoch 4083/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0059 - accuracy: 0.6484\n",
            "Epoch 04083: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0320 - accuracy: 0.6454 - val_loss: 3.1144 - val_accuracy: 0.4315\n",
            "Epoch 4084/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9895 - accuracy: 0.6592\n",
            "Epoch 04084: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9895 - accuracy: 0.6592 - val_loss: 3.1690 - val_accuracy: 0.4384\n",
            "Epoch 4085/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9735 - accuracy: 0.6719\n",
            "Epoch 04085: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0138 - accuracy: 0.6575 - val_loss: 3.1480 - val_accuracy: 0.4589\n",
            "Epoch 4086/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0545 - accuracy: 0.6875\n",
            "Epoch 04086: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0407 - accuracy: 0.6747 - val_loss: 3.1004 - val_accuracy: 0.4452\n",
            "Epoch 4087/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0875 - accuracy: 0.6500\n",
            "Epoch 04087: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0237 - accuracy: 0.6592 - val_loss: 3.1189 - val_accuracy: 0.4521\n",
            "Epoch 4088/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0672 - accuracy: 0.6389\n",
            "Epoch 04088: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9927 - accuracy: 0.6454 - val_loss: 3.0235 - val_accuracy: 0.4315\n",
            "Epoch 4089/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0863 - accuracy: 0.6611\n",
            "Epoch 04089: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0462 - accuracy: 0.6781 - val_loss: 3.1051 - val_accuracy: 0.4384\n",
            "Epoch 4090/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9939 - accuracy: 0.6484\n",
            "Epoch 04090: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0254 - accuracy: 0.6592 - val_loss: 3.1080 - val_accuracy: 0.4452\n",
            "Epoch 4091/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0158 - accuracy: 0.6641\n",
            "Epoch 04091: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9986 - accuracy: 0.6661 - val_loss: 3.1293 - val_accuracy: 0.4452\n",
            "Epoch 4092/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0533 - accuracy: 0.6418\n",
            "Epoch 04092: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0396 - accuracy: 0.6420 - val_loss: 3.1196 - val_accuracy: 0.4110\n",
            "Epoch 4093/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.8894 - accuracy: 0.6615\n",
            "Epoch 04093: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9812 - accuracy: 0.6592 - val_loss: 3.1838 - val_accuracy: 0.4452\n",
            "Epoch 4094/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1070 - accuracy: 0.5994\n",
            "Epoch 04094: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0978 - accuracy: 0.5972 - val_loss: 3.0647 - val_accuracy: 0.4452\n",
            "Epoch 4095/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0110 - accuracy: 0.6733\n",
            "Epoch 04095: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0262 - accuracy: 0.6713 - val_loss: 3.0797 - val_accuracy: 0.4247\n",
            "Epoch 4096/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1631 - accuracy: 0.6016\n",
            "Epoch 04096: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1330 - accuracy: 0.6076 - val_loss: 3.0769 - val_accuracy: 0.3836\n",
            "Epoch 4097/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0150 - accuracy: 0.6781\n",
            "Epoch 04097: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0262 - accuracy: 0.6523 - val_loss: 3.0186 - val_accuracy: 0.4178\n",
            "Epoch 4098/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9967 - accuracy: 0.6394\n",
            "Epoch 04098: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9944 - accuracy: 0.6489 - val_loss: 3.0667 - val_accuracy: 0.4178\n",
            "Epoch 4099/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1173 - accuracy: 0.6222\n",
            "Epoch 04099: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0646 - accuracy: 0.6213 - val_loss: 3.0025 - val_accuracy: 0.4041\n",
            "Epoch 4100/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0141 - accuracy: 0.6193\n",
            "Epoch 04100: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0204 - accuracy: 0.6437 - val_loss: 2.9769 - val_accuracy: 0.4041\n",
            "Epoch 4101/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9110 - accuracy: 0.6484\n",
            "Epoch 04101: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9368 - accuracy: 0.6472 - val_loss: 3.0552 - val_accuracy: 0.3973\n",
            "Epoch 4102/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1314 - accuracy: 0.6068\n",
            "Epoch 04102: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0784 - accuracy: 0.6231 - val_loss: 3.0134 - val_accuracy: 0.4178\n",
            "Epoch 4103/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9557 - accuracy: 0.6667\n",
            "Epoch 04103: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9923 - accuracy: 0.6558 - val_loss: 3.0898 - val_accuracy: 0.4315\n",
            "Epoch 4104/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9671 - accuracy: 0.6705\n",
            "Epoch 04104: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9912 - accuracy: 0.6695 - val_loss: 3.0516 - val_accuracy: 0.4315\n",
            "Epoch 4105/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 0.9960 - accuracy: 0.6708\n",
            "Epoch 04105: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9870 - accuracy: 0.6713 - val_loss: 3.0978 - val_accuracy: 0.4247\n",
            "Epoch 4106/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0530 - accuracy: 0.6745\n",
            "Epoch 04106: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2311 - accuracy: 0.6420 - val_loss: 3.0827 - val_accuracy: 0.4110\n",
            "Epoch 4107/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0566 - accuracy: 0.6380\n",
            "Epoch 04107: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0177 - accuracy: 0.6437 - val_loss: 3.1483 - val_accuracy: 0.4315\n",
            "Epoch 4108/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1914 - accuracy: 0.6313\n",
            "Epoch 04108: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1735 - accuracy: 0.6179 - val_loss: 3.0997 - val_accuracy: 0.4452\n",
            "Epoch 4109/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1349 - accuracy: 0.6198\n",
            "Epoch 04109: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1613 - accuracy: 0.6248 - val_loss: 3.0577 - val_accuracy: 0.4521\n",
            "Epoch 4110/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0777 - accuracy: 0.6250\n",
            "Epoch 04110: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1003 - accuracy: 0.6041 - val_loss: 3.0220 - val_accuracy: 0.4315\n",
            "Epoch 4111/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0715 - accuracy: 0.6484\n",
            "Epoch 04111: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0826 - accuracy: 0.6351 - val_loss: 3.0679 - val_accuracy: 0.4247\n",
            "Epoch 4112/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.2242 - accuracy: 0.5721\n",
            "Epoch 04112: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2673 - accuracy: 0.5731 - val_loss: 3.0463 - val_accuracy: 0.4521\n",
            "Epoch 4113/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0372 - accuracy: 0.6472\n",
            "Epoch 04113: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0372 - accuracy: 0.6472 - val_loss: 3.0801 - val_accuracy: 0.4041\n",
            "Epoch 4114/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1030 - accuracy: 0.6392\n",
            "Epoch 04114: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1610 - accuracy: 0.6145 - val_loss: 3.0095 - val_accuracy: 0.4247\n",
            "Epoch 4115/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0978 - accuracy: 0.6432\n",
            "Epoch 04115: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0426 - accuracy: 0.6575 - val_loss: 2.9599 - val_accuracy: 0.4247\n",
            "Epoch 4116/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0956 - accuracy: 0.6250\n",
            "Epoch 04116: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0920 - accuracy: 0.6299 - val_loss: 3.0236 - val_accuracy: 0.4178\n",
            "Epoch 4117/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1837 - accuracy: 0.5844\n",
            "Epoch 04117: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1499 - accuracy: 0.6024 - val_loss: 3.0992 - val_accuracy: 0.4247\n",
            "Epoch 4118/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0035 - accuracy: 0.6659\n",
            "Epoch 04118: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9758 - accuracy: 0.6695 - val_loss: 3.0570 - val_accuracy: 0.4315\n",
            "Epoch 4119/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1047 - accuracy: 0.6172\n",
            "Epoch 04119: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0396 - accuracy: 0.6368 - val_loss: 3.0156 - val_accuracy: 0.4452\n",
            "Epoch 4120/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0300 - accuracy: 0.6198\n",
            "Epoch 04120: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9950 - accuracy: 0.6317 - val_loss: 2.9817 - val_accuracy: 0.4384\n",
            "Epoch 4121/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0166 - accuracy: 0.6619\n",
            "Epoch 04121: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0100 - accuracy: 0.6540 - val_loss: 3.0590 - val_accuracy: 0.4315\n",
            "Epoch 4122/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.0025 - accuracy: 0.6500\n",
            "Epoch 04122: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0075 - accuracy: 0.6351 - val_loss: 3.1707 - val_accuracy: 0.4521\n",
            "Epoch 4123/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0848 - accuracy: 0.6198\n",
            "Epoch 04123: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0726 - accuracy: 0.6282 - val_loss: 3.1519 - val_accuracy: 0.4315\n",
            "Epoch 4124/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0243 - accuracy: 0.6454\n",
            "Epoch 04124: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0243 - accuracy: 0.6454 - val_loss: 3.1050 - val_accuracy: 0.4247\n",
            "Epoch 4125/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0244 - accuracy: 0.6307\n",
            "Epoch 04125: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9692 - accuracy: 0.6437 - val_loss: 2.9979 - val_accuracy: 0.4247\n",
            "Epoch 4126/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9224 - accuracy: 0.6875\n",
            "Epoch 04126: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9583 - accuracy: 0.6678 - val_loss: 3.1645 - val_accuracy: 0.4452\n",
            "Epoch 4127/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0773 - accuracy: 0.6328\n",
            "Epoch 04127: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9917 - accuracy: 0.6558 - val_loss: 3.2401 - val_accuracy: 0.4247\n",
            "Epoch 4128/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0189 - accuracy: 0.6510\n",
            "Epoch 04128: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0699 - accuracy: 0.6403 - val_loss: 3.2060 - val_accuracy: 0.4178\n",
            "Epoch 4129/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0466 - accuracy: 0.6538\n",
            "Epoch 04129: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0617 - accuracy: 0.6454 - val_loss: 3.2881 - val_accuracy: 0.4247\n",
            "Epoch 4130/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0140 - accuracy: 0.6380\n",
            "Epoch 04130: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0020 - accuracy: 0.6523 - val_loss: 3.1580 - val_accuracy: 0.4247\n",
            "Epoch 4131/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1016 - accuracy: 0.6354\n",
            "Epoch 04131: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1293 - accuracy: 0.6231 - val_loss: 3.1879 - val_accuracy: 0.4452\n",
            "Epoch 4132/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1171 - accuracy: 0.6322\n",
            "Epoch 04132: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0549 - accuracy: 0.6386 - val_loss: 3.2086 - val_accuracy: 0.4384\n",
            "Epoch 4133/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9933 - accuracy: 0.6562\n",
            "Epoch 04133: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0072 - accuracy: 0.6558 - val_loss: 3.0743 - val_accuracy: 0.4384\n",
            "Epoch 4134/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0265 - accuracy: 0.6506\n",
            "Epoch 04134: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0423 - accuracy: 0.6454 - val_loss: 3.1419 - val_accuracy: 0.4589\n",
            "Epoch 4135/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1549 - accuracy: 0.6418\n",
            "Epoch 04135: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1818 - accuracy: 0.6299 - val_loss: 3.0927 - val_accuracy: 0.4452\n",
            "Epoch 4136/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.8967 - accuracy: 0.6901\n",
            "Epoch 04136: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9223 - accuracy: 0.6919 - val_loss: 2.9574 - val_accuracy: 0.4178\n",
            "Epoch 4137/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1504 - accuracy: 0.6386\n",
            "Epoch 04137: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1504 - accuracy: 0.6386 - val_loss: 3.0263 - val_accuracy: 0.4521\n",
            "Epoch 4138/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0717 - accuracy: 0.6403\n",
            "Epoch 04138: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0717 - accuracy: 0.6403 - val_loss: 2.9476 - val_accuracy: 0.4452\n",
            "Epoch 4139/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0528 - accuracy: 0.6076\n",
            "Epoch 04139: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0772 - accuracy: 0.6334 - val_loss: 3.0437 - val_accuracy: 0.4315\n",
            "Epoch 4140/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0689 - accuracy: 0.6392\n",
            "Epoch 04140: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0990 - accuracy: 0.6265 - val_loss: 2.9636 - val_accuracy: 0.4384\n",
            "Epoch 4141/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9853 - accuracy: 0.6449\n",
            "Epoch 04141: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9655 - accuracy: 0.6695 - val_loss: 2.9169 - val_accuracy: 0.4384\n",
            "Epoch 4142/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0598 - accuracy: 0.6534\n",
            "Epoch 04142: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0018 - accuracy: 0.6592 - val_loss: 2.9206 - val_accuracy: 0.4315\n",
            "Epoch 4143/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0072 - accuracy: 0.6380\n",
            "Epoch 04143: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0494 - accuracy: 0.6282 - val_loss: 2.8806 - val_accuracy: 0.4315\n",
            "Epoch 4144/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0317 - accuracy: 0.6161\n",
            "Epoch 04144: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0294 - accuracy: 0.6265 - val_loss: 2.8858 - val_accuracy: 0.4521\n",
            "Epoch 4145/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1504 - accuracy: 0.6346\n",
            "Epoch 04145: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1132 - accuracy: 0.6317 - val_loss: 2.8655 - val_accuracy: 0.4452\n",
            "Epoch 4146/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0141 - accuracy: 0.6458\n",
            "Epoch 04146: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9943 - accuracy: 0.6489 - val_loss: 2.8902 - val_accuracy: 0.4315\n",
            "Epoch 4147/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.1508 - accuracy: 0.6083\n",
            "Epoch 04147: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1849 - accuracy: 0.5938 - val_loss: 2.9513 - val_accuracy: 0.4247\n",
            "Epoch 4148/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.0880 - accuracy: 0.6417\n",
            "Epoch 04148: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0736 - accuracy: 0.6437 - val_loss: 2.9829 - val_accuracy: 0.4247\n",
            "Epoch 4149/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0624 - accuracy: 0.6418\n",
            "Epoch 04149: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1188 - accuracy: 0.6127 - val_loss: 3.0579 - val_accuracy: 0.4452\n",
            "Epoch 4150/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0642 - accuracy: 0.6062\n",
            "Epoch 04150: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0634 - accuracy: 0.6196 - val_loss: 2.9809 - val_accuracy: 0.4178\n",
            "Epoch 4151/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.1944 - accuracy: 0.6024\n",
            "Epoch 04151: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1982 - accuracy: 0.6007 - val_loss: 2.8527 - val_accuracy: 0.4658\n",
            "Epoch 4152/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0474 - accuracy: 0.6193\n",
            "Epoch 04152: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0755 - accuracy: 0.6231 - val_loss: 2.8873 - val_accuracy: 0.4384\n",
            "Epoch 4153/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0239 - accuracy: 0.6380\n",
            "Epoch 04153: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1216 - accuracy: 0.6041 - val_loss: 2.8377 - val_accuracy: 0.4521\n",
            "Epoch 4154/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9895 - accuracy: 0.6615\n",
            "Epoch 04154: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9971 - accuracy: 0.6523 - val_loss: 2.8662 - val_accuracy: 0.4247\n",
            "Epoch 4155/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9978 - accuracy: 0.6322\n",
            "Epoch 04155: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9560 - accuracy: 0.6506 - val_loss: 3.1127 - val_accuracy: 0.4247\n",
            "Epoch 4156/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0220 - accuracy: 0.6172\n",
            "Epoch 04156: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9897 - accuracy: 0.6489 - val_loss: 3.0738 - val_accuracy: 0.4247\n",
            "Epoch 4157/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.0043 - accuracy: 0.6562\n",
            "Epoch 04157: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9805 - accuracy: 0.6609 - val_loss: 3.0128 - val_accuracy: 0.4589\n",
            "Epoch 4158/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0634 - accuracy: 0.6406\n",
            "Epoch 04158: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0597 - accuracy: 0.6420 - val_loss: 2.8193 - val_accuracy: 0.4384\n",
            "Epoch 4159/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.1204 - accuracy: 0.5868\n",
            "Epoch 04159: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0542 - accuracy: 0.6110 - val_loss: 3.0337 - val_accuracy: 0.4384\n",
            "Epoch 4160/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0543 - accuracy: 0.6248\n",
            "Epoch 04160: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0543 - accuracy: 0.6248 - val_loss: 3.0383 - val_accuracy: 0.4315\n",
            "Epoch 4161/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0464 - accuracy: 0.6298\n",
            "Epoch 04161: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0645 - accuracy: 0.6351 - val_loss: 2.9887 - val_accuracy: 0.4384\n",
            "Epoch 4162/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9008 - accuracy: 0.6875\n",
            "Epoch 04162: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9322 - accuracy: 0.6747 - val_loss: 3.0024 - val_accuracy: 0.4384\n",
            "Epoch 4163/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0043 - accuracy: 0.6477\n",
            "Epoch 04163: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9552 - accuracy: 0.6575 - val_loss: 3.0822 - val_accuracy: 0.4521\n",
            "Epoch 4164/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0717 - accuracy: 0.6375\n",
            "Epoch 04164: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0320 - accuracy: 0.6609 - val_loss: 2.9761 - val_accuracy: 0.4521\n",
            "Epoch 4165/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 0.9450 - accuracy: 0.6597\n",
            "Epoch 04165: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0716 - accuracy: 0.6368 - val_loss: 3.0368 - val_accuracy: 0.4452\n",
            "Epoch 4166/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1345 - accuracy: 0.6472\n",
            "Epoch 04166: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1345 - accuracy: 0.6472 - val_loss: 3.0282 - val_accuracy: 0.4247\n",
            "Epoch 4167/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0496 - accuracy: 0.6477\n",
            "Epoch 04167: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0806 - accuracy: 0.6299 - val_loss: 3.1707 - val_accuracy: 0.4315\n",
            "Epoch 4168/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0314 - accuracy: 0.6438\n",
            "Epoch 04168: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0229 - accuracy: 0.6627 - val_loss: 3.0588 - val_accuracy: 0.4384\n",
            "Epoch 4169/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0171 - accuracy: 0.6094\n",
            "Epoch 04169: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0319 - accuracy: 0.6093 - val_loss: 2.9419 - val_accuracy: 0.4521\n",
            "Epoch 4170/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0025 - accuracy: 0.6514\n",
            "Epoch 04170: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0121 - accuracy: 0.6472 - val_loss: 3.0636 - val_accuracy: 0.4521\n",
            "Epoch 4171/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9232 - accuracy: 0.6615\n",
            "Epoch 04171: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9192 - accuracy: 0.6540 - val_loss: 3.0449 - val_accuracy: 0.4452\n",
            "Epoch 4172/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9183 - accuracy: 0.6932\n",
            "Epoch 04172: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9511 - accuracy: 0.6661 - val_loss: 3.0169 - val_accuracy: 0.4726\n",
            "Epoch 4173/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9485 - accuracy: 0.6432\n",
            "Epoch 04173: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9429 - accuracy: 0.6506 - val_loss: 2.9887 - val_accuracy: 0.4452\n",
            "Epoch 4174/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9417 - accuracy: 0.6923\n",
            "Epoch 04174: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9959 - accuracy: 0.6661 - val_loss: 3.0961 - val_accuracy: 0.4452\n",
            "Epoch 4175/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9602 - accuracy: 0.6615\n",
            "Epoch 04175: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9596 - accuracy: 0.6695 - val_loss: 3.0068 - val_accuracy: 0.4658\n",
            "Epoch 4176/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9524 - accuracy: 0.6676\n",
            "Epoch 04176: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0485 - accuracy: 0.6523 - val_loss: 2.9235 - val_accuracy: 0.4384\n",
            "Epoch 4177/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9065 - accuracy: 0.6745\n",
            "Epoch 04177: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9800 - accuracy: 0.6489 - val_loss: 3.0845 - val_accuracy: 0.4521\n",
            "Epoch 4178/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0411 - accuracy: 0.6701\n",
            "Epoch 04178: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0410 - accuracy: 0.6713 - val_loss: 3.0042 - val_accuracy: 0.4589\n",
            "Epoch 4179/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9497 - accuracy: 0.6562\n",
            "Epoch 04179: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0053 - accuracy: 0.6368 - val_loss: 2.9785 - val_accuracy: 0.4315\n",
            "Epoch 4180/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9873 - accuracy: 0.6469\n",
            "Epoch 04180: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0041 - accuracy: 0.6351 - val_loss: 2.9140 - val_accuracy: 0.4589\n",
            "Epoch 4181/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 0.9795 - accuracy: 0.6458\n",
            "Epoch 04181: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0311 - accuracy: 0.6403 - val_loss: 3.0321 - val_accuracy: 0.4315\n",
            "Epoch 4182/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 0.9766 - accuracy: 0.6853\n",
            "Epoch 04182: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.0231 - accuracy: 0.6661 - val_loss: 3.0064 - val_accuracy: 0.4315\n",
            "Epoch 4183/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0022 - accuracy: 0.6635\n",
            "Epoch 04183: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0143 - accuracy: 0.6558 - val_loss: 3.0094 - val_accuracy: 0.4384\n",
            "Epoch 4184/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0162 - accuracy: 0.6458\n",
            "Epoch 04184: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0464 - accuracy: 0.6299 - val_loss: 3.0036 - val_accuracy: 0.4658\n",
            "Epoch 4185/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0502 - accuracy: 0.6392\n",
            "Epoch 04185: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0013 - accuracy: 0.6523 - val_loss: 3.0180 - val_accuracy: 0.4658\n",
            "Epoch 4186/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9635 - accuracy: 0.6812\n",
            "Epoch 04186: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0128 - accuracy: 0.6799 - val_loss: 3.1088 - val_accuracy: 0.4726\n",
            "Epoch 4187/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0819 - accuracy: 0.6250\n",
            "Epoch 04187: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0446 - accuracy: 0.6386 - val_loss: 3.0485 - val_accuracy: 0.4521\n",
            "Epoch 4188/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 0.9680 - accuracy: 0.6406\n",
            "Epoch 04188: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9917 - accuracy: 0.6403 - val_loss: 3.0754 - val_accuracy: 0.4384\n",
            "Epoch 4189/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9803 - accuracy: 0.6562\n",
            "Epoch 04189: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9614 - accuracy: 0.6609 - val_loss: 3.1814 - val_accuracy: 0.4521\n",
            "Epoch 4190/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9791 - accuracy: 0.6611\n",
            "Epoch 04190: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0063 - accuracy: 0.6592 - val_loss: 2.8685 - val_accuracy: 0.4795\n",
            "Epoch 4191/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0312 - accuracy: 0.6449\n",
            "Epoch 04191: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.0596 - accuracy: 0.6317 - val_loss: 2.9890 - val_accuracy: 0.4658\n",
            "Epoch 4192/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0041 - accuracy: 0.6731\n",
            "Epoch 04192: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0163 - accuracy: 0.6489 - val_loss: 3.0976 - val_accuracy: 0.4726\n",
            "Epoch 4193/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0675 - accuracy: 0.6763\n",
            "Epoch 04193: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0349 - accuracy: 0.6644 - val_loss: 3.0801 - val_accuracy: 0.4726\n",
            "Epoch 4194/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 0.9069 - accuracy: 0.6674\n",
            "Epoch 04194: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.8841 - accuracy: 0.6644 - val_loss: 3.1032 - val_accuracy: 0.4658\n",
            "Epoch 4195/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9778 - accuracy: 0.6591\n",
            "Epoch 04195: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0106 - accuracy: 0.6489 - val_loss: 3.0594 - val_accuracy: 0.4726\n",
            "Epoch 4196/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9546 - accuracy: 0.6818\n",
            "Epoch 04196: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9843 - accuracy: 0.6592 - val_loss: 3.0488 - val_accuracy: 0.4589\n",
            "Epoch 4197/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9317 - accuracy: 0.6648\n",
            "Epoch 04197: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0034 - accuracy: 0.6454 - val_loss: 3.1993 - val_accuracy: 0.4726\n",
            "Epoch 4198/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0172 - accuracy: 0.6276\n",
            "Epoch 04198: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0168 - accuracy: 0.6299 - val_loss: 3.1919 - val_accuracy: 0.4726\n",
            "Epoch 4199/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1690 - accuracy: 0.6023\n",
            "Epoch 04199: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0895 - accuracy: 0.6265 - val_loss: 3.2942 - val_accuracy: 0.4589\n",
            "Epoch 4200/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0201 - accuracy: 0.6250\n",
            "Epoch 04200: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0109 - accuracy: 0.6386 - val_loss: 3.1230 - val_accuracy: 0.4384\n",
            "Epoch 4201/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0517 - accuracy: 0.6562\n",
            "Epoch 04201: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0324 - accuracy: 0.6592 - val_loss: 3.0612 - val_accuracy: 0.4452\n",
            "Epoch 4202/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9554 - accuracy: 0.6641\n",
            "Epoch 04202: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9429 - accuracy: 0.6764 - val_loss: 3.1293 - val_accuracy: 0.4521\n",
            "Epoch 4203/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0438 - accuracy: 0.6322\n",
            "Epoch 04203: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0323 - accuracy: 0.6437 - val_loss: 3.1157 - val_accuracy: 0.4452\n",
            "Epoch 4204/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0141 - accuracy: 0.6108\n",
            "Epoch 04204: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0104 - accuracy: 0.6145 - val_loss: 3.0980 - val_accuracy: 0.4795\n",
            "Epoch 4205/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0825 - accuracy: 0.6531\n",
            "Epoch 04205: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0787 - accuracy: 0.6368 - val_loss: 3.0408 - val_accuracy: 0.4521\n",
            "Epoch 4206/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1340 - accuracy: 0.6302\n",
            "Epoch 04206: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1854 - accuracy: 0.6076 - val_loss: 3.0331 - val_accuracy: 0.4384\n",
            "Epoch 4207/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0525 - accuracy: 0.6165\n",
            "Epoch 04207: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0478 - accuracy: 0.6368 - val_loss: 3.0620 - val_accuracy: 0.4452\n",
            "Epoch 4208/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1688 - accuracy: 0.6136\n",
            "Epoch 04208: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1474 - accuracy: 0.6093 - val_loss: 2.8918 - val_accuracy: 0.4247\n",
            "Epoch 4209/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1009 - accuracy: 0.5911\n",
            "Epoch 04209: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0412 - accuracy: 0.6162 - val_loss: 2.9947 - val_accuracy: 0.4384\n",
            "Epoch 4210/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.0487 - accuracy: 0.6500\n",
            "Epoch 04210: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0597 - accuracy: 0.6420 - val_loss: 3.0245 - val_accuracy: 0.3973\n",
            "Epoch 4211/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0102 - accuracy: 0.6146\n",
            "Epoch 04211: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0526 - accuracy: 0.6231 - val_loss: 3.1712 - val_accuracy: 0.4384\n",
            "Epoch 4212/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9583 - accuracy: 0.6745\n",
            "Epoch 04212: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9726 - accuracy: 0.6695 - val_loss: 2.9625 - val_accuracy: 0.4452\n",
            "Epoch 4213/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0196 - accuracy: 0.6449\n",
            "Epoch 04213: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9947 - accuracy: 0.6437 - val_loss: 2.8409 - val_accuracy: 0.4110\n",
            "Epoch 4214/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1070 - accuracy: 0.6068\n",
            "Epoch 04214: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0811 - accuracy: 0.6196 - val_loss: 2.8159 - val_accuracy: 0.4247\n",
            "Epoch 4215/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0222 - accuracy: 0.6432\n",
            "Epoch 04215: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0259 - accuracy: 0.6386 - val_loss: 2.8409 - val_accuracy: 0.4384\n",
            "Epoch 4216/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9865 - accuracy: 0.6432\n",
            "Epoch 04216: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0295 - accuracy: 0.6420 - val_loss: 2.8921 - val_accuracy: 0.4521\n",
            "Epoch 4217/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9501 - accuracy: 0.6745\n",
            "Epoch 04217: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9935 - accuracy: 0.6523 - val_loss: 2.9797 - val_accuracy: 0.4315\n",
            "Epoch 4218/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9355 - accuracy: 0.6641\n",
            "Epoch 04218: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9656 - accuracy: 0.6661 - val_loss: 3.0601 - val_accuracy: 0.4384\n",
            "Epoch 4219/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9532 - accuracy: 0.6667\n",
            "Epoch 04219: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9987 - accuracy: 0.6368 - val_loss: 3.0910 - val_accuracy: 0.4452\n",
            "Epoch 4220/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0870 - accuracy: 0.6518\n",
            "Epoch 04220: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0525 - accuracy: 0.6627 - val_loss: 2.9597 - val_accuracy: 0.4521\n",
            "Epoch 4221/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9758 - accuracy: 0.6851\n",
            "Epoch 04221: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0169 - accuracy: 0.6627 - val_loss: 2.9060 - val_accuracy: 0.4315\n",
            "Epoch 4222/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0218 - accuracy: 0.6354\n",
            "Epoch 04222: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9984 - accuracy: 0.6489 - val_loss: 3.0708 - val_accuracy: 0.4384\n",
            "Epoch 4223/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0517 - accuracy: 0.6276\n",
            "Epoch 04223: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0693 - accuracy: 0.6454 - val_loss: 3.1214 - val_accuracy: 0.4384\n",
            "Epoch 4224/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.8810 - accuracy: 0.6761\n",
            "Epoch 04224: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9675 - accuracy: 0.6609 - val_loss: 3.1145 - val_accuracy: 0.4521\n",
            "Epoch 4225/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0661 - accuracy: 0.6042\n",
            "Epoch 04225: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 1.0428 - accuracy: 0.6248 - val_loss: 3.1687 - val_accuracy: 0.4247\n",
            "Epoch 4226/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1116 - accuracy: 0.6317\n",
            "Epoch 04226: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1116 - accuracy: 0.6317 - val_loss: 3.2402 - val_accuracy: 0.3973\n",
            "Epoch 4227/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1919 - accuracy: 0.6154\n",
            "Epoch 04227: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1279 - accuracy: 0.6265 - val_loss: 3.1849 - val_accuracy: 0.4178\n",
            "Epoch 4228/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1550 - accuracy: 0.6010\n",
            "Epoch 04228: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1252 - accuracy: 0.6213 - val_loss: 2.9977 - val_accuracy: 0.4452\n",
            "Epoch 4229/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0081 - accuracy: 0.6575\n",
            "Epoch 04229: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0081 - accuracy: 0.6575 - val_loss: 3.1015 - val_accuracy: 0.4384\n",
            "Epoch 4230/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0149 - accuracy: 0.6510\n",
            "Epoch 04230: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0270 - accuracy: 0.6540 - val_loss: 3.0729 - val_accuracy: 0.4521\n",
            "Epoch 4231/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0442 - accuracy: 0.6432\n",
            "Epoch 04231: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0929 - accuracy: 0.6351 - val_loss: 3.0945 - val_accuracy: 0.4315\n",
            "Epoch 4232/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0479 - accuracy: 0.6484\n",
            "Epoch 04232: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0585 - accuracy: 0.6489 - val_loss: 3.0909 - val_accuracy: 0.4589\n",
            "Epoch 4233/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0062 - accuracy: 0.6719\n",
            "Epoch 04233: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0220 - accuracy: 0.6575 - val_loss: 3.1264 - val_accuracy: 0.4589\n",
            "Epoch 4234/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9934 - accuracy: 0.6656\n",
            "Epoch 04234: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9953 - accuracy: 0.6678 - val_loss: 3.1546 - val_accuracy: 0.4247\n",
            "Epoch 4235/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9781 - accuracy: 0.6354\n",
            "Epoch 04235: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0323 - accuracy: 0.6231 - val_loss: 3.1074 - val_accuracy: 0.3904\n",
            "Epoch 4236/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9554 - accuracy: 0.6745\n",
            "Epoch 04236: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9968 - accuracy: 0.6609 - val_loss: 3.1158 - val_accuracy: 0.3973\n",
            "Epoch 4237/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9816 - accuracy: 0.6418\n",
            "Epoch 04237: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0333 - accuracy: 0.6368 - val_loss: 3.0092 - val_accuracy: 0.4589\n",
            "Epoch 4238/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9709 - accuracy: 0.7045\n",
            "Epoch 04238: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9886 - accuracy: 0.6833 - val_loss: 3.0418 - val_accuracy: 0.4384\n",
            "Epoch 4239/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.9738 - accuracy: 0.6649\n",
            "Epoch 04239: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9707 - accuracy: 0.6644 - val_loss: 3.1458 - val_accuracy: 0.4521\n",
            "Epoch 4240/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0429 - accuracy: 0.6540\n",
            "Epoch 04240: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0429 - accuracy: 0.6540 - val_loss: 3.1196 - val_accuracy: 0.4658\n",
            "Epoch 4241/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0535 - accuracy: 0.6375\n",
            "Epoch 04241: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0681 - accuracy: 0.6437 - val_loss: 3.1363 - val_accuracy: 0.4247\n",
            "Epoch 4242/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0147 - accuracy: 0.6562\n",
            "Epoch 04242: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0807 - accuracy: 0.6351 - val_loss: 3.0537 - val_accuracy: 0.4521\n",
            "Epoch 4243/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0784 - accuracy: 0.6432\n",
            "Epoch 04243: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0241 - accuracy: 0.6540 - val_loss: 3.0536 - val_accuracy: 0.4315\n",
            "Epoch 4244/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9707 - accuracy: 0.6641\n",
            "Epoch 04244: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.9550 - accuracy: 0.6678 - val_loss: 3.1003 - val_accuracy: 0.4589\n",
            "Epoch 4245/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0782 - accuracy: 0.6250\n",
            "Epoch 04245: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0674 - accuracy: 0.6127 - val_loss: 2.9668 - val_accuracy: 0.4658\n",
            "Epoch 4246/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1247 - accuracy: 0.6335\n",
            "Epoch 04246: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1120 - accuracy: 0.6334 - val_loss: 3.0397 - val_accuracy: 0.4658\n",
            "Epoch 4247/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0061 - accuracy: 0.6458\n",
            "Epoch 04247: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9897 - accuracy: 0.6420 - val_loss: 3.1115 - val_accuracy: 0.4589\n",
            "Epoch 4248/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1100 - accuracy: 0.6094\n",
            "Epoch 04248: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0080 - accuracy: 0.6592 - val_loss: 3.0881 - val_accuracy: 0.4589\n",
            "Epoch 4249/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0937 - accuracy: 0.6222\n",
            "Epoch 04249: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0892 - accuracy: 0.6179 - val_loss: 3.0676 - val_accuracy: 0.4315\n",
            "Epoch 4250/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9448 - accuracy: 0.6847\n",
            "Epoch 04250: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9694 - accuracy: 0.6764 - val_loss: 3.1353 - val_accuracy: 0.4589\n",
            "Epoch 4251/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 1.1028 - accuracy: 0.6103\n",
            "Epoch 04251: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0859 - accuracy: 0.6127 - val_loss: 3.3940 - val_accuracy: 0.4589\n",
            "Epoch 4252/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0856 - accuracy: 0.6442\n",
            "Epoch 04252: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0516 - accuracy: 0.6506 - val_loss: 3.3177 - val_accuracy: 0.4795\n",
            "Epoch 4253/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.8877 - accuracy: 0.6797\n",
            "Epoch 04253: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.8977 - accuracy: 0.6764 - val_loss: 3.1737 - val_accuracy: 0.4658\n",
            "Epoch 4254/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9538 - accuracy: 0.6641\n",
            "Epoch 04254: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9795 - accuracy: 0.6575 - val_loss: 3.1579 - val_accuracy: 0.4521\n",
            "Epoch 4255/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9250 - accuracy: 0.6611\n",
            "Epoch 04255: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9274 - accuracy: 0.6661 - val_loss: 3.2801 - val_accuracy: 0.4589\n",
            "Epoch 4256/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9560 - accuracy: 0.6386\n",
            "Epoch 04256: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9560 - accuracy: 0.6386 - val_loss: 3.2884 - val_accuracy: 0.4726\n",
            "Epoch 4257/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9702 - accuracy: 0.6615\n",
            "Epoch 04257: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9599 - accuracy: 0.6678 - val_loss: 3.2069 - val_accuracy: 0.4726\n",
            "Epoch 4258/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9623 - accuracy: 0.6683\n",
            "Epoch 04258: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9991 - accuracy: 0.6627 - val_loss: 3.2606 - val_accuracy: 0.4521\n",
            "Epoch 4259/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0156 - accuracy: 0.6406\n",
            "Epoch 04259: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9947 - accuracy: 0.6540 - val_loss: 3.2473 - val_accuracy: 0.4589\n",
            "Epoch 4260/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0701 - accuracy: 0.6536\n",
            "Epoch 04260: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0889 - accuracy: 0.6386 - val_loss: 3.3012 - val_accuracy: 0.4521\n",
            "Epoch 4261/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0499 - accuracy: 0.6224\n",
            "Epoch 04261: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0312 - accuracy: 0.6334 - val_loss: 3.2461 - val_accuracy: 0.4726\n",
            "Epoch 4262/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9324 - accuracy: 0.6847\n",
            "Epoch 04262: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9357 - accuracy: 0.6919 - val_loss: 3.2590 - val_accuracy: 0.4726\n",
            "Epoch 4263/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 0.9792 - accuracy: 0.6750\n",
            "Epoch 04263: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9679 - accuracy: 0.6816 - val_loss: 3.3848 - val_accuracy: 0.4795\n",
            "Epoch 4264/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9597 - accuracy: 0.6587\n",
            "Epoch 04264: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9830 - accuracy: 0.6420 - val_loss: 3.4913 - val_accuracy: 0.4521\n",
            "Epoch 4265/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0683 - accuracy: 0.6420\n",
            "Epoch 04265: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0193 - accuracy: 0.6558 - val_loss: 3.5906 - val_accuracy: 0.4178\n",
            "Epoch 4266/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0536 - accuracy: 0.6458\n",
            "Epoch 04266: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0511 - accuracy: 0.6454 - val_loss: 3.5039 - val_accuracy: 0.4384\n",
            "Epoch 4267/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0059 - accuracy: 0.6278\n",
            "Epoch 04267: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0488 - accuracy: 0.6213 - val_loss: 3.3768 - val_accuracy: 0.4384\n",
            "Epoch 4268/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0274 - accuracy: 0.6458\n",
            "Epoch 04268: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0044 - accuracy: 0.6540 - val_loss: 3.2835 - val_accuracy: 0.4521\n",
            "Epoch 4269/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9383 - accuracy: 0.6619\n",
            "Epoch 04269: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9494 - accuracy: 0.6678 - val_loss: 3.3801 - val_accuracy: 0.4521\n",
            "Epoch 4270/5000\n",
            "16/19 [========================>.....] - ETA: 0s - loss: 1.1518 - accuracy: 0.6250\n",
            "Epoch 04270: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.1499 - accuracy: 0.6213 - val_loss: 3.3890 - val_accuracy: 0.4452\n",
            "Epoch 4271/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0219 - accuracy: 0.6458\n",
            "Epoch 04271: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0918 - accuracy: 0.6317 - val_loss: 3.2828 - val_accuracy: 0.4315\n",
            "Epoch 4272/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0475 - accuracy: 0.6506\n",
            "Epoch 04272: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0685 - accuracy: 0.6540 - val_loss: 3.1869 - val_accuracy: 0.4247\n",
            "Epoch 4273/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0248 - accuracy: 0.6023\n",
            "Epoch 04273: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0345 - accuracy: 0.6196 - val_loss: 3.0992 - val_accuracy: 0.4110\n",
            "Epoch 4274/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0549 - accuracy: 0.6302\n",
            "Epoch 04274: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0683 - accuracy: 0.6351 - val_loss: 3.0893 - val_accuracy: 0.4247\n",
            "Epoch 4275/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0030 - accuracy: 0.6392\n",
            "Epoch 04275: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9944 - accuracy: 0.6403 - val_loss: 3.0422 - val_accuracy: 0.4110\n",
            "Epoch 4276/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0220 - accuracy: 0.6302\n",
            "Epoch 04276: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0959 - accuracy: 0.6127 - val_loss: 3.1377 - val_accuracy: 0.4041\n",
            "Epoch 4277/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9807 - accuracy: 0.6392\n",
            "Epoch 04277: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0105 - accuracy: 0.6317 - val_loss: 3.1976 - val_accuracy: 0.4041\n",
            "Epoch 4278/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0821 - accuracy: 0.6231\n",
            "Epoch 04278: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0821 - accuracy: 0.6231 - val_loss: 3.3151 - val_accuracy: 0.3973\n",
            "Epoch 4279/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.0565 - accuracy: 0.6271\n",
            "Epoch 04279: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.0736 - accuracy: 0.6231 - val_loss: 3.1651 - val_accuracy: 0.4110\n",
            "Epoch 4280/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9694 - accuracy: 0.6536\n",
            "Epoch 04280: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0073 - accuracy: 0.6368 - val_loss: 3.0131 - val_accuracy: 0.4178\n",
            "Epoch 4281/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9603 - accuracy: 0.6510\n",
            "Epoch 04281: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9897 - accuracy: 0.6403 - val_loss: 2.9200 - val_accuracy: 0.4315\n",
            "Epoch 4282/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0893 - accuracy: 0.6224\n",
            "Epoch 04282: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1001 - accuracy: 0.6265 - val_loss: 2.9569 - val_accuracy: 0.4658\n",
            "Epoch 4283/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9909 - accuracy: 0.6562\n",
            "Epoch 04283: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9541 - accuracy: 0.6678 - val_loss: 2.9235 - val_accuracy: 0.4384\n",
            "Epoch 4284/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0393 - accuracy: 0.6493\n",
            "Epoch 04284: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9997 - accuracy: 0.6678 - val_loss: 3.0157 - val_accuracy: 0.4315\n",
            "Epoch 4285/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9392 - accuracy: 0.6641\n",
            "Epoch 04285: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0449 - accuracy: 0.6386 - val_loss: 3.0701 - val_accuracy: 0.4247\n",
            "Epoch 4286/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 0.9846 - accuracy: 0.6875\n",
            "Epoch 04286: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0975 - accuracy: 0.6351 - val_loss: 3.1187 - val_accuracy: 0.4384\n",
            "Epoch 4287/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1120 - accuracy: 0.5994\n",
            "Epoch 04287: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0742 - accuracy: 0.6076 - val_loss: 3.1817 - val_accuracy: 0.4521\n",
            "Epoch 4288/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0000 - accuracy: 0.6589\n",
            "Epoch 04288: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9626 - accuracy: 0.6661 - val_loss: 3.2111 - val_accuracy: 0.4521\n",
            "Epoch 4289/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.8856 - accuracy: 0.6667\n",
            "Epoch 04289: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9039 - accuracy: 0.6609 - val_loss: 3.2026 - val_accuracy: 0.4452\n",
            "Epoch 4290/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0047 - accuracy: 0.6506\n",
            "Epoch 04290: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0823 - accuracy: 0.6351 - val_loss: 3.0837 - val_accuracy: 0.4521\n",
            "Epoch 4291/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0771 - accuracy: 0.6319\n",
            "Epoch 04291: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0796 - accuracy: 0.6299 - val_loss: 3.1858 - val_accuracy: 0.4384\n",
            "Epoch 4292/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0940 - accuracy: 0.5986\n",
            "Epoch 04292: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0511 - accuracy: 0.6179 - val_loss: 3.2393 - val_accuracy: 0.4658\n",
            "Epoch 4293/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 0.9762 - accuracy: 0.6806\n",
            "Epoch 04293: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0616 - accuracy: 0.6695 - val_loss: 3.2672 - val_accuracy: 0.4658\n",
            "Epoch 4294/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0679 - accuracy: 0.6490\n",
            "Epoch 04294: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0541 - accuracy: 0.6489 - val_loss: 3.1887 - val_accuracy: 0.4521\n",
            "Epoch 4295/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9942 - accuracy: 0.6380\n",
            "Epoch 04295: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0372 - accuracy: 0.6248 - val_loss: 3.1886 - val_accuracy: 0.4795\n",
            "Epoch 4296/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0884 - accuracy: 0.6108\n",
            "Epoch 04296: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0497 - accuracy: 0.6213 - val_loss: 2.9165 - val_accuracy: 0.4452\n",
            "Epoch 4297/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9786 - accuracy: 0.6198\n",
            "Epoch 04297: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9902 - accuracy: 0.6317 - val_loss: 3.0268 - val_accuracy: 0.4452\n",
            "Epoch 4298/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9670 - accuracy: 0.6799\n",
            "Epoch 04298: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9670 - accuracy: 0.6799 - val_loss: 3.0374 - val_accuracy: 0.4452\n",
            "Epoch 4299/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0375 - accuracy: 0.6506\n",
            "Epoch 04299: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0421 - accuracy: 0.6454 - val_loss: 3.1471 - val_accuracy: 0.4521\n",
            "Epoch 4300/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9629 - accuracy: 0.6648\n",
            "Epoch 04300: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0246 - accuracy: 0.6523 - val_loss: 3.1784 - val_accuracy: 0.4384\n",
            "Epoch 4301/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9617 - accuracy: 0.6591\n",
            "Epoch 04301: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9811 - accuracy: 0.6575 - val_loss: 3.2480 - val_accuracy: 0.4521\n",
            "Epoch 4302/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.8787 - accuracy: 0.6849\n",
            "Epoch 04302: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9654 - accuracy: 0.6764 - val_loss: 3.2407 - val_accuracy: 0.4726\n",
            "Epoch 4303/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0707 - accuracy: 0.6354\n",
            "Epoch 04303: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0818 - accuracy: 0.6265 - val_loss: 3.2618 - val_accuracy: 0.4726\n",
            "Epoch 4304/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9751 - accuracy: 0.6687\n",
            "Epoch 04304: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9783 - accuracy: 0.6506 - val_loss: 3.2506 - val_accuracy: 0.4658\n",
            "Epoch 4305/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9488 - accuracy: 0.6506\n",
            "Epoch 04305: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9880 - accuracy: 0.6437 - val_loss: 3.2401 - val_accuracy: 0.4452\n",
            "Epoch 4306/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9730 - accuracy: 0.6589\n",
            "Epoch 04306: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0326 - accuracy: 0.6523 - val_loss: 3.1033 - val_accuracy: 0.4521\n",
            "Epoch 4307/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0183 - accuracy: 0.6562\n",
            "Epoch 04307: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0200 - accuracy: 0.6592 - val_loss: 3.1195 - val_accuracy: 0.4041\n",
            "Epoch 4308/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0552 - accuracy: 0.6337\n",
            "Epoch 04308: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0515 - accuracy: 0.6351 - val_loss: 3.0977 - val_accuracy: 0.4384\n",
            "Epoch 4309/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0308 - accuracy: 0.6536\n",
            "Epoch 04309: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0814 - accuracy: 0.6299 - val_loss: 3.2458 - val_accuracy: 0.4315\n",
            "Epoch 4310/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0302 - accuracy: 0.6250\n",
            "Epoch 04310: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9980 - accuracy: 0.6437 - val_loss: 3.3025 - val_accuracy: 0.4384\n",
            "Epoch 4311/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.1903 - accuracy: 0.6458\n",
            "Epoch 04311: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1123 - accuracy: 0.6489 - val_loss: 3.3472 - val_accuracy: 0.4110\n",
            "Epoch 4312/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1241 - accuracy: 0.6477\n",
            "Epoch 04312: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1226 - accuracy: 0.6368 - val_loss: 3.1837 - val_accuracy: 0.4178\n",
            "Epoch 4313/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9729 - accuracy: 0.6344\n",
            "Epoch 04313: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9788 - accuracy: 0.6609 - val_loss: 3.2601 - val_accuracy: 0.4384\n",
            "Epoch 4314/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0227 - accuracy: 0.6250\n",
            "Epoch 04314: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9800 - accuracy: 0.6575 - val_loss: 3.3567 - val_accuracy: 0.4452\n",
            "Epoch 4315/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.8980 - accuracy: 0.6589\n",
            "Epoch 04315: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9494 - accuracy: 0.6368 - val_loss: 3.3902 - val_accuracy: 0.4452\n",
            "Epoch 4316/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0034 - accuracy: 0.6562\n",
            "Epoch 04316: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9898 - accuracy: 0.6540 - val_loss: 3.4146 - val_accuracy: 0.4452\n",
            "Epoch 4317/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0002 - accuracy: 0.6490\n",
            "Epoch 04317: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9967 - accuracy: 0.6454 - val_loss: 3.3629 - val_accuracy: 0.4452\n",
            "Epoch 4318/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0360 - accuracy: 0.6146\n",
            "Epoch 04318: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0755 - accuracy: 0.6145 - val_loss: 3.4271 - val_accuracy: 0.4589\n",
            "Epoch 4319/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0013 - accuracy: 0.6615\n",
            "Epoch 04319: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0316 - accuracy: 0.6506 - val_loss: 3.4057 - val_accuracy: 0.4452\n",
            "Epoch 4320/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.2063 - accuracy: 0.6589\n",
            "Epoch 04320: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1166 - accuracy: 0.6609 - val_loss: 3.3551 - val_accuracy: 0.4521\n",
            "Epoch 4321/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0009 - accuracy: 0.6536\n",
            "Epoch 04321: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0485 - accuracy: 0.6368 - val_loss: 3.1531 - val_accuracy: 0.4384\n",
            "Epoch 4322/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9297 - accuracy: 0.6733\n",
            "Epoch 04322: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9268 - accuracy: 0.6867 - val_loss: 3.4068 - val_accuracy: 0.4795\n",
            "Epoch 4323/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1490 - accuracy: 0.6335\n",
            "Epoch 04323: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0675 - accuracy: 0.6437 - val_loss: 3.3510 - val_accuracy: 0.4658\n",
            "Epoch 4324/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0441 - accuracy: 0.6354\n",
            "Epoch 04324: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.0412 - accuracy: 0.6351 - val_loss: 3.1912 - val_accuracy: 0.4384\n",
            "Epoch 4325/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0118 - accuracy: 0.6394\n",
            "Epoch 04325: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.9625 - accuracy: 0.6558 - val_loss: 3.1433 - val_accuracy: 0.4315\n",
            "Epoch 4326/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0642 - accuracy: 0.6562\n",
            "Epoch 04326: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0324 - accuracy: 0.6454 - val_loss: 3.1395 - val_accuracy: 0.4247\n",
            "Epoch 4327/5000\n",
            "16/19 [========================>.....] - ETA: 0s - loss: 1.0094 - accuracy: 0.6387\n",
            "Epoch 04327: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.0060 - accuracy: 0.6368 - val_loss: 3.2901 - val_accuracy: 0.4247\n",
            "Epoch 4328/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0158 - accuracy: 0.6490\n",
            "Epoch 04328: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0170 - accuracy: 0.6403 - val_loss: 3.2308 - val_accuracy: 0.4247\n",
            "Epoch 4329/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9697 - accuracy: 0.6562\n",
            "Epoch 04329: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9692 - accuracy: 0.6627 - val_loss: 3.3320 - val_accuracy: 0.4384\n",
            "Epoch 4330/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0932 - accuracy: 0.6432\n",
            "Epoch 04330: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0495 - accuracy: 0.6386 - val_loss: 3.3743 - val_accuracy: 0.4452\n",
            "Epoch 4331/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0303 - accuracy: 0.6562\n",
            "Epoch 04331: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9776 - accuracy: 0.6713 - val_loss: 3.2689 - val_accuracy: 0.4315\n",
            "Epoch 4332/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0624 - accuracy: 0.6146\n",
            "Epoch 04332: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9811 - accuracy: 0.6437 - val_loss: 3.2094 - val_accuracy: 0.4452\n",
            "Epoch 4333/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0956 - accuracy: 0.6222\n",
            "Epoch 04333: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0324 - accuracy: 0.6299 - val_loss: 3.2258 - val_accuracy: 0.4521\n",
            "Epoch 4334/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9824 - accuracy: 0.6695\n",
            "Epoch 04334: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9824 - accuracy: 0.6695 - val_loss: 3.1349 - val_accuracy: 0.4658\n",
            "Epoch 4335/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1662 - accuracy: 0.6406\n",
            "Epoch 04335: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0774 - accuracy: 0.6609 - val_loss: 3.0587 - val_accuracy: 0.4658\n",
            "Epoch 4336/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0813 - accuracy: 0.6380\n",
            "Epoch 04336: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0651 - accuracy: 0.6317 - val_loss: 3.2476 - val_accuracy: 0.4521\n",
            "Epoch 4337/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0841 - accuracy: 0.5795\n",
            "Epoch 04337: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0759 - accuracy: 0.6059 - val_loss: 3.3570 - val_accuracy: 0.4041\n",
            "Epoch 4338/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1398 - accuracy: 0.6282\n",
            "Epoch 04338: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1398 - accuracy: 0.6282 - val_loss: 2.8786 - val_accuracy: 0.4452\n",
            "Epoch 4339/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1412 - accuracy: 0.6136\n",
            "Epoch 04339: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0856 - accuracy: 0.6386 - val_loss: 2.9628 - val_accuracy: 0.4041\n",
            "Epoch 4340/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0386 - accuracy: 0.5966\n",
            "Epoch 04340: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0483 - accuracy: 0.6024 - val_loss: 3.0799 - val_accuracy: 0.3904\n",
            "Epoch 4341/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0800 - accuracy: 0.6136\n",
            "Epoch 04341: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0341 - accuracy: 0.6386 - val_loss: 3.1119 - val_accuracy: 0.4178\n",
            "Epoch 4342/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0476 - accuracy: 0.6145\n",
            "Epoch 04342: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0476 - accuracy: 0.6145 - val_loss: 3.1922 - val_accuracy: 0.4178\n",
            "Epoch 4343/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9385 - accuracy: 0.6683\n",
            "Epoch 04343: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9349 - accuracy: 0.6627 - val_loss: 3.3202 - val_accuracy: 0.4247\n",
            "Epoch 4344/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1159 - accuracy: 0.6034\n",
            "Epoch 04344: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1572 - accuracy: 0.6093 - val_loss: 3.2986 - val_accuracy: 0.4110\n",
            "Epoch 4345/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9205 - accuracy: 0.6790\n",
            "Epoch 04345: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9995 - accuracy: 0.6506 - val_loss: 3.1704 - val_accuracy: 0.4315\n",
            "Epoch 4346/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0499 - accuracy: 0.6594\n",
            "Epoch 04346: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0644 - accuracy: 0.6386 - val_loss: 3.2488 - val_accuracy: 0.4110\n",
            "Epoch 4347/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1071 - accuracy: 0.5964\n",
            "Epoch 04347: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0686 - accuracy: 0.6162 - val_loss: 3.4706 - val_accuracy: 0.4178\n",
            "Epoch 4348/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0289 - accuracy: 0.6328\n",
            "Epoch 04348: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0387 - accuracy: 0.6454 - val_loss: 3.2806 - val_accuracy: 0.4452\n",
            "Epoch 4349/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9808 - accuracy: 0.6514\n",
            "Epoch 04349: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9882 - accuracy: 0.6506 - val_loss: 3.2691 - val_accuracy: 0.4521\n",
            "Epoch 4350/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0407 - accuracy: 0.6346\n",
            "Epoch 04350: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1347 - accuracy: 0.6420 - val_loss: 3.3008 - val_accuracy: 0.4452\n",
            "Epoch 4351/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1408 - accuracy: 0.6298\n",
            "Epoch 04351: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0600 - accuracy: 0.6540 - val_loss: 3.1767 - val_accuracy: 0.4178\n",
            "Epoch 4352/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1049 - accuracy: 0.6179\n",
            "Epoch 04352: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1049 - accuracy: 0.6179 - val_loss: 3.2280 - val_accuracy: 0.4315\n",
            "Epoch 4353/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0487 - accuracy: 0.6120\n",
            "Epoch 04353: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0925 - accuracy: 0.6059 - val_loss: 3.1750 - val_accuracy: 0.4247\n",
            "Epoch 4354/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9345 - accuracy: 0.6667\n",
            "Epoch 04354: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9754 - accuracy: 0.6523 - val_loss: 3.0742 - val_accuracy: 0.4315\n",
            "Epoch 4355/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.2551 - accuracy: 0.5807\n",
            "Epoch 04355: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.2445 - accuracy: 0.5800 - val_loss: 3.1026 - val_accuracy: 0.4041\n",
            "Epoch 4356/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0105 - accuracy: 0.6490\n",
            "Epoch 04356: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0795 - accuracy: 0.6265 - val_loss: 3.1765 - val_accuracy: 0.4178\n",
            "Epoch 4357/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0254 - accuracy: 0.6328\n",
            "Epoch 04357: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0391 - accuracy: 0.6248 - val_loss: 3.1287 - val_accuracy: 0.4247\n",
            "Epoch 4358/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0558 - accuracy: 0.6198\n",
            "Epoch 04358: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0509 - accuracy: 0.6213 - val_loss: 3.2219 - val_accuracy: 0.4178\n",
            "Epoch 4359/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1058 - accuracy: 0.6394\n",
            "Epoch 04359: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1159 - accuracy: 0.6420 - val_loss: 3.3272 - val_accuracy: 0.4110\n",
            "Epoch 4360/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.1722 - accuracy: 0.5938\n",
            "Epoch 04360: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1406 - accuracy: 0.6076 - val_loss: 3.2366 - val_accuracy: 0.4178\n",
            "Epoch 4361/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9540 - accuracy: 0.6803\n",
            "Epoch 04361: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9646 - accuracy: 0.6661 - val_loss: 3.1450 - val_accuracy: 0.4452\n",
            "Epoch 4362/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0860 - accuracy: 0.6375\n",
            "Epoch 04362: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0598 - accuracy: 0.6334 - val_loss: 3.1504 - val_accuracy: 0.4384\n",
            "Epoch 4363/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9689 - accuracy: 0.6406\n",
            "Epoch 04363: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9941 - accuracy: 0.6403 - val_loss: 3.1951 - val_accuracy: 0.4452\n",
            "Epoch 4364/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0320 - accuracy: 0.6536\n",
            "Epoch 04364: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0425 - accuracy: 0.6386 - val_loss: 3.2554 - val_accuracy: 0.4384\n",
            "Epoch 4365/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1076 - accuracy: 0.6224\n",
            "Epoch 04365: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0425 - accuracy: 0.6454 - val_loss: 3.1022 - val_accuracy: 0.4384\n",
            "Epoch 4366/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1210 - accuracy: 0.6224\n",
            "Epoch 04366: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0999 - accuracy: 0.6162 - val_loss: 3.2081 - val_accuracy: 0.4384\n",
            "Epoch 4367/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1491 - accuracy: 0.6125\n",
            "Epoch 04367: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0510 - accuracy: 0.6368 - val_loss: 3.2525 - val_accuracy: 0.4247\n",
            "Epoch 4368/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0115 - accuracy: 0.6619\n",
            "Epoch 04368: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9329 - accuracy: 0.6816 - val_loss: 3.2970 - val_accuracy: 0.4521\n",
            "Epoch 4369/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9938 - accuracy: 0.6500\n",
            "Epoch 04369: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9422 - accuracy: 0.6730 - val_loss: 3.4398 - val_accuracy: 0.4452\n",
            "Epoch 4370/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0483 - accuracy: 0.6493\n",
            "Epoch 04370: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9998 - accuracy: 0.6730 - val_loss: 3.5032 - val_accuracy: 0.4589\n",
            "Epoch 4371/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9638 - accuracy: 0.6589\n",
            "Epoch 04371: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9781 - accuracy: 0.6472 - val_loss: 3.2993 - val_accuracy: 0.4521\n",
            "Epoch 4372/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9105 - accuracy: 0.6506\n",
            "Epoch 04372: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0101 - accuracy: 0.6179 - val_loss: 3.2536 - val_accuracy: 0.4384\n",
            "Epoch 4373/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9649 - accuracy: 0.6534\n",
            "Epoch 04373: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9448 - accuracy: 0.6627 - val_loss: 3.2564 - val_accuracy: 0.4726\n",
            "Epoch 4374/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0375 - accuracy: 0.6250\n",
            "Epoch 04374: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0356 - accuracy: 0.6231 - val_loss: 3.2610 - val_accuracy: 0.4315\n",
            "Epoch 4375/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0628 - accuracy: 0.6432\n",
            "Epoch 04375: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0594 - accuracy: 0.6299 - val_loss: 3.7283 - val_accuracy: 0.4247\n",
            "Epoch 4376/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1671 - accuracy: 0.6172\n",
            "Epoch 04376: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0896 - accuracy: 0.6472 - val_loss: 3.3462 - val_accuracy: 0.4452\n",
            "Epoch 4377/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0688 - accuracy: 0.6432\n",
            "Epoch 04377: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0451 - accuracy: 0.6558 - val_loss: 3.1550 - val_accuracy: 0.4452\n",
            "Epoch 4378/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0393 - accuracy: 0.6335\n",
            "Epoch 04378: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0197 - accuracy: 0.6454 - val_loss: 3.2322 - val_accuracy: 0.4178\n",
            "Epoch 4379/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9621 - accuracy: 0.6625\n",
            "Epoch 04379: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0101 - accuracy: 0.6420 - val_loss: 3.1326 - val_accuracy: 0.4315\n",
            "Epoch 4380/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0126 - accuracy: 0.6094\n",
            "Epoch 04380: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0134 - accuracy: 0.6299 - val_loss: 3.1860 - val_accuracy: 0.4315\n",
            "Epoch 4381/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0376 - accuracy: 0.6265\n",
            "Epoch 04381: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0376 - accuracy: 0.6265 - val_loss: 3.1790 - val_accuracy: 0.4452\n",
            "Epoch 4382/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.8925 - accuracy: 0.6953\n",
            "Epoch 04382: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9231 - accuracy: 0.6747 - val_loss: 3.1860 - val_accuracy: 0.4384\n",
            "Epoch 4383/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1080 - accuracy: 0.6198\n",
            "Epoch 04383: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1098 - accuracy: 0.6041 - val_loss: 3.2001 - val_accuracy: 0.4521\n",
            "Epoch 4384/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9533 - accuracy: 0.6771\n",
            "Epoch 04384: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9636 - accuracy: 0.6540 - val_loss: 3.3304 - val_accuracy: 0.4315\n",
            "Epoch 4385/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0023 - accuracy: 0.6406\n",
            "Epoch 04385: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0080 - accuracy: 0.6386 - val_loss: 3.3527 - val_accuracy: 0.4452\n",
            "Epoch 4386/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.9205 - accuracy: 0.6632\n",
            "Epoch 04386: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9211 - accuracy: 0.6627 - val_loss: 3.3404 - val_accuracy: 0.4384\n",
            "Epoch 4387/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.1599 - accuracy: 0.6183\n",
            "Epoch 04387: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1793 - accuracy: 0.6110 - val_loss: 3.2394 - val_accuracy: 0.4452\n",
            "Epoch 4388/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1165 - accuracy: 0.6051\n",
            "Epoch 04388: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0882 - accuracy: 0.6231 - val_loss: 3.1101 - val_accuracy: 0.4315\n",
            "Epoch 4389/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0369 - accuracy: 0.6364\n",
            "Epoch 04389: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0309 - accuracy: 0.6386 - val_loss: 3.0560 - val_accuracy: 0.4110\n",
            "Epoch 4390/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0557 - accuracy: 0.6224\n",
            "Epoch 04390: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0129 - accuracy: 0.6317 - val_loss: 3.1513 - val_accuracy: 0.4384\n",
            "Epoch 4391/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0058 - accuracy: 0.6354\n",
            "Epoch 04391: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9975 - accuracy: 0.6420 - val_loss: 3.0507 - val_accuracy: 0.4384\n",
            "Epoch 4392/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0204 - accuracy: 0.6420\n",
            "Epoch 04392: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0159 - accuracy: 0.6523 - val_loss: 3.1784 - val_accuracy: 0.4110\n",
            "Epoch 4393/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9936 - accuracy: 0.6719\n",
            "Epoch 04393: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9579 - accuracy: 0.6799 - val_loss: 3.2395 - val_accuracy: 0.4384\n",
            "Epoch 4394/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0575 - accuracy: 0.6187\n",
            "Epoch 04394: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9840 - accuracy: 0.6644 - val_loss: 3.3771 - val_accuracy: 0.4315\n",
            "Epoch 4395/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9677 - accuracy: 0.6745\n",
            "Epoch 04395: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9844 - accuracy: 0.6609 - val_loss: 3.3424 - val_accuracy: 0.4384\n",
            "Epoch 4396/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1283 - accuracy: 0.6368\n",
            "Epoch 04396: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1283 - accuracy: 0.6368 - val_loss: 3.3605 - val_accuracy: 0.4521\n",
            "Epoch 4397/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0461 - accuracy: 0.6432\n",
            "Epoch 04397: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0639 - accuracy: 0.6334 - val_loss: 3.1753 - val_accuracy: 0.4521\n",
            "Epoch 4398/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0711 - accuracy: 0.6594\n",
            "Epoch 04398: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1274 - accuracy: 0.6506 - val_loss: 3.0083 - val_accuracy: 0.4384\n",
            "Epoch 4399/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1210 - accuracy: 0.6380\n",
            "Epoch 04399: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1202 - accuracy: 0.6265 - val_loss: 3.0692 - val_accuracy: 0.4384\n",
            "Epoch 4400/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9996 - accuracy: 0.6693\n",
            "Epoch 04400: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0664 - accuracy: 0.6420 - val_loss: 3.1359 - val_accuracy: 0.4315\n",
            "Epoch 4401/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0786 - accuracy: 0.5764\n",
            "Epoch 04401: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0724 - accuracy: 0.6110 - val_loss: 3.1034 - val_accuracy: 0.4315\n",
            "Epoch 4402/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0880 - accuracy: 0.6589\n",
            "Epoch 04402: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0962 - accuracy: 0.6540 - val_loss: 3.2812 - val_accuracy: 0.4452\n",
            "Epoch 4403/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9245 - accuracy: 0.6589\n",
            "Epoch 04403: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9455 - accuracy: 0.6695 - val_loss: 3.1807 - val_accuracy: 0.4384\n",
            "Epoch 4404/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0581 - accuracy: 0.6589\n",
            "Epoch 04404: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0475 - accuracy: 0.6506 - val_loss: 3.2079 - val_accuracy: 0.4452\n",
            "Epoch 4405/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0244 - accuracy: 0.6771\n",
            "Epoch 04405: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9635 - accuracy: 0.6850 - val_loss: 3.1190 - val_accuracy: 0.4589\n",
            "Epoch 4406/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1218 - accuracy: 0.6510\n",
            "Epoch 04406: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0808 - accuracy: 0.6420 - val_loss: 3.1331 - val_accuracy: 0.4521\n",
            "Epoch 4407/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0330 - accuracy: 0.6719\n",
            "Epoch 04407: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0459 - accuracy: 0.6644 - val_loss: 3.1278 - val_accuracy: 0.4521\n",
            "Epoch 4408/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9909 - accuracy: 0.6432\n",
            "Epoch 04408: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.9743 - accuracy: 0.6523 - val_loss: 3.1560 - val_accuracy: 0.4658\n",
            "Epoch 4409/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0483 - accuracy: 0.6250\n",
            "Epoch 04409: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0098 - accuracy: 0.6489 - val_loss: 3.1853 - val_accuracy: 0.4658\n",
            "Epoch 4410/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0291 - accuracy: 0.6676\n",
            "Epoch 04410: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0038 - accuracy: 0.6661 - val_loss: 3.2062 - val_accuracy: 0.4589\n",
            "Epoch 4411/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0922 - accuracy: 0.6120\n",
            "Epoch 04411: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0810 - accuracy: 0.6162 - val_loss: 3.1864 - val_accuracy: 0.4521\n",
            "Epoch 4412/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1636 - accuracy: 0.5969\n",
            "Epoch 04412: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1451 - accuracy: 0.6162 - val_loss: 3.2240 - val_accuracy: 0.4315\n",
            "Epoch 4413/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9695 - accuracy: 0.6420\n",
            "Epoch 04413: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9672 - accuracy: 0.6695 - val_loss: 3.2142 - val_accuracy: 0.4384\n",
            "Epoch 4414/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0365 - accuracy: 0.6534\n",
            "Epoch 04414: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9931 - accuracy: 0.6695 - val_loss: 3.2210 - val_accuracy: 0.4247\n",
            "Epoch 4415/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0026 - accuracy: 0.6477\n",
            "Epoch 04415: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0062 - accuracy: 0.6506 - val_loss: 3.2744 - val_accuracy: 0.4247\n",
            "Epoch 4416/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0651 - accuracy: 0.6619\n",
            "Epoch 04416: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1274 - accuracy: 0.6540 - val_loss: 3.2460 - val_accuracy: 0.4247\n",
            "Epoch 4417/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0941 - accuracy: 0.6162\n",
            "Epoch 04417: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0941 - accuracy: 0.6162 - val_loss: 3.2887 - val_accuracy: 0.4315\n",
            "Epoch 4418/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1201 - accuracy: 0.6334\n",
            "Epoch 04418: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1201 - accuracy: 0.6334 - val_loss: 3.1781 - val_accuracy: 0.4247\n",
            "Epoch 4419/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1344 - accuracy: 0.6068\n",
            "Epoch 04419: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1581 - accuracy: 0.6076 - val_loss: 3.1177 - val_accuracy: 0.4110\n",
            "Epoch 4420/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0884 - accuracy: 0.6538\n",
            "Epoch 04420: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1004 - accuracy: 0.6472 - val_loss: 3.2370 - val_accuracy: 0.4178\n",
            "Epoch 4421/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1006 - accuracy: 0.6615\n",
            "Epoch 04421: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0933 - accuracy: 0.6558 - val_loss: 3.1029 - val_accuracy: 0.4110\n",
            "Epoch 4422/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0200 - accuracy: 0.6496\n",
            "Epoch 04422: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0352 - accuracy: 0.6351 - val_loss: 3.0584 - val_accuracy: 0.4178\n",
            "Epoch 4423/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 0.9169 - accuracy: 0.6912\n",
            "Epoch 04423: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.9249 - accuracy: 0.6850 - val_loss: 3.1101 - val_accuracy: 0.4178\n",
            "Epoch 4424/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0068 - accuracy: 0.6351\n",
            "Epoch 04424: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0068 - accuracy: 0.6351 - val_loss: 3.1449 - val_accuracy: 0.4384\n",
            "Epoch 4425/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0167 - accuracy: 0.6386\n",
            "Epoch 04425: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0167 - accuracy: 0.6386 - val_loss: 3.0829 - val_accuracy: 0.3836\n",
            "Epoch 4426/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9801 - accuracy: 0.6506\n",
            "Epoch 04426: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9651 - accuracy: 0.6506 - val_loss: 3.2058 - val_accuracy: 0.4452\n",
            "Epoch 4427/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0334 - accuracy: 0.6231\n",
            "Epoch 04427: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0334 - accuracy: 0.6231 - val_loss: 3.3438 - val_accuracy: 0.4110\n",
            "Epoch 4428/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.0038 - accuracy: 0.6500\n",
            "Epoch 04428: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0271 - accuracy: 0.6489 - val_loss: 3.3308 - val_accuracy: 0.4247\n",
            "Epoch 4429/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0426 - accuracy: 0.6490\n",
            "Epoch 04429: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0276 - accuracy: 0.6506 - val_loss: 3.1946 - val_accuracy: 0.3973\n",
            "Epoch 4430/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0649 - accuracy: 0.6354\n",
            "Epoch 04430: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0541 - accuracy: 0.6437 - val_loss: 3.1156 - val_accuracy: 0.3973\n",
            "Epoch 4431/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0580 - accuracy: 0.6370\n",
            "Epoch 04431: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0535 - accuracy: 0.6386 - val_loss: 3.2093 - val_accuracy: 0.4041\n",
            "Epoch 4432/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9298 - accuracy: 0.6797\n",
            "Epoch 04432: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9417 - accuracy: 0.6816 - val_loss: 3.2783 - val_accuracy: 0.4110\n",
            "Epoch 4433/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0433 - accuracy: 0.6901\n",
            "Epoch 04433: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0485 - accuracy: 0.6609 - val_loss: 3.1816 - val_accuracy: 0.4247\n",
            "Epoch 4434/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0649 - accuracy: 0.6298\n",
            "Epoch 04434: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0356 - accuracy: 0.6351 - val_loss: 3.2320 - val_accuracy: 0.4041\n",
            "Epoch 4435/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0728 - accuracy: 0.6418\n",
            "Epoch 04435: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0710 - accuracy: 0.6334 - val_loss: 3.3143 - val_accuracy: 0.4315\n",
            "Epoch 4436/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1576 - accuracy: 0.6274\n",
            "Epoch 04436: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1392 - accuracy: 0.6334 - val_loss: 3.1758 - val_accuracy: 0.4384\n",
            "Epoch 4437/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0186 - accuracy: 0.6528\n",
            "Epoch 04437: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0099 - accuracy: 0.6472 - val_loss: 3.0430 - val_accuracy: 0.4315\n",
            "Epoch 4438/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0998 - accuracy: 0.6178\n",
            "Epoch 04438: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0556 - accuracy: 0.6317 - val_loss: 3.0762 - val_accuracy: 0.4247\n",
            "Epoch 4439/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1125 - accuracy: 0.6250\n",
            "Epoch 04439: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0787 - accuracy: 0.6334 - val_loss: 3.1704 - val_accuracy: 0.4315\n",
            "Epoch 4440/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1457 - accuracy: 0.6094\n",
            "Epoch 04440: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0780 - accuracy: 0.6368 - val_loss: 3.1901 - val_accuracy: 0.4247\n",
            "Epoch 4441/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0234 - accuracy: 0.6354\n",
            "Epoch 04441: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0669 - accuracy: 0.6179 - val_loss: 3.1804 - val_accuracy: 0.4247\n",
            "Epoch 4442/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9882 - accuracy: 0.6394\n",
            "Epoch 04442: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9500 - accuracy: 0.6592 - val_loss: 3.2962 - val_accuracy: 0.4452\n",
            "Epoch 4443/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0112 - accuracy: 0.6432\n",
            "Epoch 04443: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9916 - accuracy: 0.6575 - val_loss: 3.3661 - val_accuracy: 0.4178\n",
            "Epoch 4444/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1337 - accuracy: 0.6274\n",
            "Epoch 04444: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0939 - accuracy: 0.6437 - val_loss: 3.4213 - val_accuracy: 0.4315\n",
            "Epoch 4445/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0040 - accuracy: 0.6540\n",
            "Epoch 04445: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0040 - accuracy: 0.6540 - val_loss: 3.4792 - val_accuracy: 0.4384\n",
            "Epoch 4446/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0547 - accuracy: 0.6344\n",
            "Epoch 04446: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0302 - accuracy: 0.6540 - val_loss: 3.2872 - val_accuracy: 0.4452\n",
            "Epoch 4447/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0004 - accuracy: 0.6514\n",
            "Epoch 04447: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9730 - accuracy: 0.6678 - val_loss: 3.3156 - val_accuracy: 0.4452\n",
            "Epoch 4448/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0503 - accuracy: 0.6432\n",
            "Epoch 04448: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9912 - accuracy: 0.6627 - val_loss: 3.4325 - val_accuracy: 0.4315\n",
            "Epoch 4449/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9541 - accuracy: 0.6659\n",
            "Epoch 04449: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0020 - accuracy: 0.6627 - val_loss: 3.3417 - val_accuracy: 0.4452\n",
            "Epoch 4450/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9445 - accuracy: 0.6484\n",
            "Epoch 04450: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9391 - accuracy: 0.6661 - val_loss: 3.2676 - val_accuracy: 0.4521\n",
            "Epoch 4451/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0818 - accuracy: 0.6278\n",
            "Epoch 04451: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0353 - accuracy: 0.6403 - val_loss: 3.4235 - val_accuracy: 0.4521\n",
            "Epoch 4452/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0506 - accuracy: 0.6322\n",
            "Epoch 04452: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0507 - accuracy: 0.6472 - val_loss: 3.2596 - val_accuracy: 0.4452\n",
            "Epoch 4453/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0827 - accuracy: 0.6068\n",
            "Epoch 04453: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0478 - accuracy: 0.6231 - val_loss: 3.2508 - val_accuracy: 0.4315\n",
            "Epoch 4454/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0973 - accuracy: 0.6562\n",
            "Epoch 04454: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1462 - accuracy: 0.6248 - val_loss: 3.2421 - val_accuracy: 0.4589\n",
            "Epoch 4455/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0583 - accuracy: 0.6394\n",
            "Epoch 04455: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1365 - accuracy: 0.6299 - val_loss: 3.1772 - val_accuracy: 0.4384\n",
            "Epoch 4456/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.1018 - accuracy: 0.6183\n",
            "Epoch 04456: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0889 - accuracy: 0.6196 - val_loss: 3.1538 - val_accuracy: 0.4384\n",
            "Epoch 4457/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1133 - accuracy: 0.6274\n",
            "Epoch 04457: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0748 - accuracy: 0.6351 - val_loss: 3.0878 - val_accuracy: 0.4315\n",
            "Epoch 4458/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9192 - accuracy: 0.6635\n",
            "Epoch 04458: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9245 - accuracy: 0.6575 - val_loss: 3.1135 - val_accuracy: 0.4110\n",
            "Epoch 4459/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0073 - accuracy: 0.6719\n",
            "Epoch 04459: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0275 - accuracy: 0.6540 - val_loss: 3.2605 - val_accuracy: 0.4384\n",
            "Epoch 4460/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0137 - accuracy: 0.6418\n",
            "Epoch 04460: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0257 - accuracy: 0.6403 - val_loss: 3.2312 - val_accuracy: 0.4315\n",
            "Epoch 4461/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0874 - accuracy: 0.6051\n",
            "Epoch 04461: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0251 - accuracy: 0.6248 - val_loss: 3.2291 - val_accuracy: 0.4521\n",
            "Epoch 4462/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0559 - accuracy: 0.6250\n",
            "Epoch 04462: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0150 - accuracy: 0.6265 - val_loss: 3.2334 - val_accuracy: 0.4521\n",
            "Epoch 4463/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0613 - accuracy: 0.6178\n",
            "Epoch 04463: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0410 - accuracy: 0.6299 - val_loss: 3.1898 - val_accuracy: 0.4521\n",
            "Epoch 4464/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0719 - accuracy: 0.6274\n",
            "Epoch 04464: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0204 - accuracy: 0.6386 - val_loss: 3.2295 - val_accuracy: 0.4452\n",
            "Epoch 4465/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0651 - accuracy: 0.6449\n",
            "Epoch 04465: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0484 - accuracy: 0.6523 - val_loss: 3.2031 - val_accuracy: 0.4315\n",
            "Epoch 4466/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1291 - accuracy: 0.6438\n",
            "Epoch 04466: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0938 - accuracy: 0.6299 - val_loss: 3.1402 - val_accuracy: 0.4589\n",
            "Epoch 4467/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0139 - accuracy: 0.6589\n",
            "Epoch 04467: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1063 - accuracy: 0.6145 - val_loss: 3.2107 - val_accuracy: 0.4384\n",
            "Epoch 4468/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9221 - accuracy: 0.6705\n",
            "Epoch 04468: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9727 - accuracy: 0.6420 - val_loss: 3.1597 - val_accuracy: 0.4452\n",
            "Epoch 4469/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.9794 - accuracy: 0.6441\n",
            "Epoch 04469: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9813 - accuracy: 0.6437 - val_loss: 3.1110 - val_accuracy: 0.4452\n",
            "Epoch 4470/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9366 - accuracy: 0.6589\n",
            "Epoch 04470: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9612 - accuracy: 0.6575 - val_loss: 3.0787 - val_accuracy: 0.4521\n",
            "Epoch 4471/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9327 - accuracy: 0.6771\n",
            "Epoch 04471: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9112 - accuracy: 0.6833 - val_loss: 3.1717 - val_accuracy: 0.4658\n",
            "Epoch 4472/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9682 - accuracy: 0.6781\n",
            "Epoch 04472: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9682 - accuracy: 0.6781 - val_loss: 3.4229 - val_accuracy: 0.4315\n",
            "Epoch 4473/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0053 - accuracy: 0.6687\n",
            "Epoch 04473: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9716 - accuracy: 0.6730 - val_loss: 3.2776 - val_accuracy: 0.4247\n",
            "Epoch 4474/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0564 - accuracy: 0.6510\n",
            "Epoch 04474: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0327 - accuracy: 0.6489 - val_loss: 3.1948 - val_accuracy: 0.4384\n",
            "Epoch 4475/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9489 - accuracy: 0.6667\n",
            "Epoch 04475: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9457 - accuracy: 0.6678 - val_loss: 3.2265 - val_accuracy: 0.4247\n",
            "Epoch 4476/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9557 - accuracy: 0.6506\n",
            "Epoch 04476: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9796 - accuracy: 0.6403 - val_loss: 3.2429 - val_accuracy: 0.4452\n",
            "Epoch 4477/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9912 - accuracy: 0.6562\n",
            "Epoch 04477: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9848 - accuracy: 0.6627 - val_loss: 3.2859 - val_accuracy: 0.4315\n",
            "Epoch 4478/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1141 - accuracy: 0.6172\n",
            "Epoch 04478: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0493 - accuracy: 0.6454 - val_loss: 3.2148 - val_accuracy: 0.4315\n",
            "Epoch 4479/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0796 - accuracy: 0.6224\n",
            "Epoch 04479: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0817 - accuracy: 0.6213 - val_loss: 3.3412 - val_accuracy: 0.4247\n",
            "Epoch 4480/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 0.9641 - accuracy: 0.6830\n",
            "Epoch 04480: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9584 - accuracy: 0.6764 - val_loss: 3.1025 - val_accuracy: 0.4452\n",
            "Epoch 4481/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0556 - accuracy: 0.6580\n",
            "Epoch 04481: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0508 - accuracy: 0.6592 - val_loss: 3.0350 - val_accuracy: 0.4658\n",
            "Epoch 4482/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0193 - accuracy: 0.6510\n",
            "Epoch 04482: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0171 - accuracy: 0.6386 - val_loss: 3.0399 - val_accuracy: 0.4521\n",
            "Epoch 4483/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0531 - accuracy: 0.6458\n",
            "Epoch 04483: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0404 - accuracy: 0.6437 - val_loss: 3.0937 - val_accuracy: 0.4384\n",
            "Epoch 4484/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0892 - accuracy: 0.6510\n",
            "Epoch 04484: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0295 - accuracy: 0.6644 - val_loss: 3.1267 - val_accuracy: 0.4315\n",
            "Epoch 4485/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1060 - accuracy: 0.6510\n",
            "Epoch 04485: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1353 - accuracy: 0.6540 - val_loss: 3.0816 - val_accuracy: 0.4521\n",
            "Epoch 4486/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0604 - accuracy: 0.6335\n",
            "Epoch 04486: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0660 - accuracy: 0.6317 - val_loss: 3.2570 - val_accuracy: 0.4247\n",
            "Epoch 4487/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0169 - accuracy: 0.6587\n",
            "Epoch 04487: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9913 - accuracy: 0.6558 - val_loss: 3.1532 - val_accuracy: 0.4041\n",
            "Epoch 4488/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9961 - accuracy: 0.6771\n",
            "Epoch 04488: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0293 - accuracy: 0.6575 - val_loss: 3.1705 - val_accuracy: 0.4384\n",
            "Epoch 4489/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1038 - accuracy: 0.6771\n",
            "Epoch 04489: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0808 - accuracy: 0.6661 - val_loss: 3.1716 - val_accuracy: 0.4384\n",
            "Epoch 4490/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0554 - accuracy: 0.6538\n",
            "Epoch 04490: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0381 - accuracy: 0.6540 - val_loss: 3.1055 - val_accuracy: 0.4452\n",
            "Epoch 4491/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0334 - accuracy: 0.6489\n",
            "Epoch 04491: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0334 - accuracy: 0.6489 - val_loss: 3.1503 - val_accuracy: 0.4178\n",
            "Epoch 4492/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.8784 - accuracy: 0.6906\n",
            "Epoch 04492: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.8964 - accuracy: 0.6781 - val_loss: 3.1446 - val_accuracy: 0.4589\n",
            "Epoch 4493/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0786 - accuracy: 0.6562\n",
            "Epoch 04493: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0766 - accuracy: 0.6558 - val_loss: 3.2578 - val_accuracy: 0.4178\n",
            "Epoch 4494/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.2220 - accuracy: 0.6276\n",
            "Epoch 04494: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1640 - accuracy: 0.6179 - val_loss: 3.1317 - val_accuracy: 0.4521\n",
            "Epoch 4495/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0367 - accuracy: 0.6250\n",
            "Epoch 04495: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0601 - accuracy: 0.6196 - val_loss: 3.2216 - val_accuracy: 0.4384\n",
            "Epoch 4496/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0322 - accuracy: 0.6364\n",
            "Epoch 04496: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0206 - accuracy: 0.6506 - val_loss: 3.2277 - val_accuracy: 0.4452\n",
            "Epoch 4497/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0421 - accuracy: 0.6364\n",
            "Epoch 04497: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0347 - accuracy: 0.6454 - val_loss: 3.1754 - val_accuracy: 0.4384\n",
            "Epoch 4498/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0565 - accuracy: 0.6250\n",
            "Epoch 04498: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0655 - accuracy: 0.6317 - val_loss: 3.0599 - val_accuracy: 0.4589\n",
            "Epoch 4499/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9896 - accuracy: 0.6575\n",
            "Epoch 04499: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9896 - accuracy: 0.6575 - val_loss: 3.1603 - val_accuracy: 0.4452\n",
            "Epoch 4500/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1521 - accuracy: 0.6172\n",
            "Epoch 04500: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1069 - accuracy: 0.6162 - val_loss: 3.1694 - val_accuracy: 0.4452\n",
            "Epoch 4501/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9664 - accuracy: 0.6676\n",
            "Epoch 04501: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9928 - accuracy: 0.6506 - val_loss: 3.2144 - val_accuracy: 0.4178\n",
            "Epoch 4502/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.8827 - accuracy: 0.6797\n",
            "Epoch 04502: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9512 - accuracy: 0.6627 - val_loss: 3.1975 - val_accuracy: 0.4384\n",
            "Epoch 4503/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.8470 - accuracy: 0.7005\n",
            "Epoch 04503: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.8522 - accuracy: 0.7040 - val_loss: 3.3352 - val_accuracy: 0.4452\n",
            "Epoch 4504/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9456 - accuracy: 0.6745\n",
            "Epoch 04504: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9424 - accuracy: 0.6764 - val_loss: 3.3645 - val_accuracy: 0.4589\n",
            "Epoch 4505/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9440 - accuracy: 0.6562\n",
            "Epoch 04505: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9692 - accuracy: 0.6523 - val_loss: 3.4312 - val_accuracy: 0.4315\n",
            "Epoch 4506/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0101 - accuracy: 0.6597\n",
            "Epoch 04506: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9375 - accuracy: 0.6781 - val_loss: 3.3590 - val_accuracy: 0.4315\n",
            "Epoch 4507/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9347 - accuracy: 0.6707\n",
            "Epoch 04507: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9856 - accuracy: 0.6747 - val_loss: 3.4004 - val_accuracy: 0.4521\n",
            "Epoch 4508/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 1.0479 - accuracy: 0.6342\n",
            "Epoch 04508: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0464 - accuracy: 0.6386 - val_loss: 3.3268 - val_accuracy: 0.4178\n",
            "Epoch 4509/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1144 - accuracy: 0.6484\n",
            "Epoch 04509: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1223 - accuracy: 0.6575 - val_loss: 3.1073 - val_accuracy: 0.4110\n",
            "Epoch 4510/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.0196 - accuracy: 0.6333\n",
            "Epoch 04510: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0407 - accuracy: 0.6299 - val_loss: 3.0555 - val_accuracy: 0.4315\n",
            "Epoch 4511/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9519 - accuracy: 0.6771\n",
            "Epoch 04511: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0062 - accuracy: 0.6558 - val_loss: 3.0662 - val_accuracy: 0.4384\n",
            "Epoch 4512/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9519 - accuracy: 0.6562\n",
            "Epoch 04512: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9466 - accuracy: 0.6627 - val_loss: 3.1233 - val_accuracy: 0.4452\n",
            "Epoch 4513/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9687 - accuracy: 0.6506\n",
            "Epoch 04513: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9947 - accuracy: 0.6506 - val_loss: 3.2030 - val_accuracy: 0.4315\n",
            "Epoch 4514/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0365 - accuracy: 0.6317\n",
            "Epoch 04514: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0365 - accuracy: 0.6317 - val_loss: 3.2024 - val_accuracy: 0.4178\n",
            "Epoch 4515/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 0.9932 - accuracy: 0.6701\n",
            "Epoch 04515: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9733 - accuracy: 0.6730 - val_loss: 3.2469 - val_accuracy: 0.4384\n",
            "Epoch 4516/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9307 - accuracy: 0.6687\n",
            "Epoch 04516: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9736 - accuracy: 0.6523 - val_loss: 3.2289 - val_accuracy: 0.4384\n",
            "Epoch 4517/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9624 - accuracy: 0.6641\n",
            "Epoch 04517: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9745 - accuracy: 0.6661 - val_loss: 3.1596 - val_accuracy: 0.4247\n",
            "Epoch 4518/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0578 - accuracy: 0.6514\n",
            "Epoch 04518: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0567 - accuracy: 0.6523 - val_loss: 3.2639 - val_accuracy: 0.4452\n",
            "Epoch 4519/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0156 - accuracy: 0.6506\n",
            "Epoch 04519: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0334 - accuracy: 0.6489 - val_loss: 3.3299 - val_accuracy: 0.4452\n",
            "Epoch 4520/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9737 - accuracy: 0.6676\n",
            "Epoch 04520: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9755 - accuracy: 0.6609 - val_loss: 3.6227 - val_accuracy: 0.4315\n",
            "Epoch 4521/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0513 - accuracy: 0.6592\n",
            "Epoch 04521: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0513 - accuracy: 0.6592 - val_loss: 3.2659 - val_accuracy: 0.3973\n",
            "Epoch 4522/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1525 - accuracy: 0.6198\n",
            "Epoch 04522: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1403 - accuracy: 0.6179 - val_loss: 3.1770 - val_accuracy: 0.4178\n",
            "Epoch 4523/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1424 - accuracy: 0.5881\n",
            "Epoch 04523: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0522 - accuracy: 0.6127 - val_loss: 3.2131 - val_accuracy: 0.4452\n",
            "Epoch 4524/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9760 - accuracy: 0.6761\n",
            "Epoch 04524: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0014 - accuracy: 0.6644 - val_loss: 3.2900 - val_accuracy: 0.4315\n",
            "Epoch 4525/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0625 - accuracy: 0.6615\n",
            "Epoch 04525: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0752 - accuracy: 0.6523 - val_loss: 3.1784 - val_accuracy: 0.4452\n",
            "Epoch 4526/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0458 - accuracy: 0.6432\n",
            "Epoch 04526: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.0314 - accuracy: 0.6403 - val_loss: 3.3586 - val_accuracy: 0.4247\n",
            "Epoch 4527/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.2760 - accuracy: 0.5511\n",
            "Epoch 04527: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1844 - accuracy: 0.5869 - val_loss: 3.2401 - val_accuracy: 0.4110\n",
            "Epoch 4528/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0997 - accuracy: 0.6051\n",
            "Epoch 04528: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1258 - accuracy: 0.5921 - val_loss: 3.2700 - val_accuracy: 0.4384\n",
            "Epoch 4529/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.1456 - accuracy: 0.6161\n",
            "Epoch 04529: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1483 - accuracy: 0.6127 - val_loss: 3.1585 - val_accuracy: 0.4726\n",
            "Epoch 4530/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0691 - accuracy: 0.6302\n",
            "Epoch 04530: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0654 - accuracy: 0.6265 - val_loss: 3.1374 - val_accuracy: 0.4521\n",
            "Epoch 4531/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0622 - accuracy: 0.6224\n",
            "Epoch 04531: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0811 - accuracy: 0.6162 - val_loss: 3.1525 - val_accuracy: 0.4178\n",
            "Epoch 4532/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0727 - accuracy: 0.6510\n",
            "Epoch 04532: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0512 - accuracy: 0.6575 - val_loss: 3.4599 - val_accuracy: 0.4041\n",
            "Epoch 4533/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1502 - accuracy: 0.6172\n",
            "Epoch 04533: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1214 - accuracy: 0.6454 - val_loss: 3.2528 - val_accuracy: 0.4384\n",
            "Epoch 4534/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0423 - accuracy: 0.6531\n",
            "Epoch 04534: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0378 - accuracy: 0.6334 - val_loss: 3.1118 - val_accuracy: 0.4041\n",
            "Epoch 4535/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.1365 - accuracy: 0.6319\n",
            "Epoch 04535: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1301 - accuracy: 0.6334 - val_loss: 3.1445 - val_accuracy: 0.3973\n",
            "Epoch 4536/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1517 - accuracy: 0.6178\n",
            "Epoch 04536: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1137 - accuracy: 0.6282 - val_loss: 3.0961 - val_accuracy: 0.4384\n",
            "Epoch 4537/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0169 - accuracy: 0.6667\n",
            "Epoch 04537: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0356 - accuracy: 0.6558 - val_loss: 3.0510 - val_accuracy: 0.4041\n",
            "Epoch 4538/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1468 - accuracy: 0.6213\n",
            "Epoch 04538: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1468 - accuracy: 0.6213 - val_loss: 3.0826 - val_accuracy: 0.4178\n",
            "Epoch 4539/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1566 - accuracy: 0.6328\n",
            "Epoch 04539: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1517 - accuracy: 0.6196 - val_loss: 3.0235 - val_accuracy: 0.4452\n",
            "Epoch 4540/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.2727 - accuracy: 0.6058\n",
            "Epoch 04540: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.2094 - accuracy: 0.6127 - val_loss: 3.0920 - val_accuracy: 0.4315\n",
            "Epoch 4541/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0609 - accuracy: 0.6302\n",
            "Epoch 04541: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0667 - accuracy: 0.6368 - val_loss: 3.1045 - val_accuracy: 0.4247\n",
            "Epoch 4542/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0907 - accuracy: 0.6354\n",
            "Epoch 04542: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0340 - accuracy: 0.6454 - val_loss: 3.2195 - val_accuracy: 0.3973\n",
            "Epoch 4543/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1243 - accuracy: 0.6016\n",
            "Epoch 04543: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0781 - accuracy: 0.6248 - val_loss: 3.2179 - val_accuracy: 0.3836\n",
            "Epoch 4544/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0579 - accuracy: 0.6265\n",
            "Epoch 04544: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0579 - accuracy: 0.6265 - val_loss: 3.3519 - val_accuracy: 0.4110\n",
            "Epoch 4545/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1056 - accuracy: 0.6276\n",
            "Epoch 04545: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0799 - accuracy: 0.6351 - val_loss: 3.3429 - val_accuracy: 0.4315\n",
            "Epoch 4546/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0492 - accuracy: 0.6380\n",
            "Epoch 04546: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0340 - accuracy: 0.6299 - val_loss: 3.2707 - val_accuracy: 0.4384\n",
            "Epoch 4547/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0616 - accuracy: 0.6198\n",
            "Epoch 04547: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0555 - accuracy: 0.6231 - val_loss: 3.2306 - val_accuracy: 0.4521\n",
            "Epoch 4548/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1877 - accuracy: 0.6224\n",
            "Epoch 04548: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1392 - accuracy: 0.6351 - val_loss: 3.2410 - val_accuracy: 0.4452\n",
            "Epoch 4549/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0778 - accuracy: 0.6328\n",
            "Epoch 04549: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1009 - accuracy: 0.6299 - val_loss: 3.3022 - val_accuracy: 0.4315\n",
            "Epoch 4550/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9592 - accuracy: 0.6693\n",
            "Epoch 04550: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9856 - accuracy: 0.6558 - val_loss: 3.2956 - val_accuracy: 0.4315\n",
            "Epoch 4551/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1647 - accuracy: 0.6172\n",
            "Epoch 04551: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1439 - accuracy: 0.6093 - val_loss: 3.3182 - val_accuracy: 0.4178\n",
            "Epoch 4552/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9819 - accuracy: 0.6466\n",
            "Epoch 04552: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9943 - accuracy: 0.6489 - val_loss: 3.2449 - val_accuracy: 0.4521\n",
            "Epoch 4553/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0056 - accuracy: 0.6562\n",
            "Epoch 04553: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0015 - accuracy: 0.6575 - val_loss: 3.3122 - val_accuracy: 0.4589\n",
            "Epoch 4554/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0782 - accuracy: 0.6226\n",
            "Epoch 04554: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0627 - accuracy: 0.6145 - val_loss: 3.2906 - val_accuracy: 0.4589\n",
            "Epoch 4555/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0655 - accuracy: 0.6418\n",
            "Epoch 04555: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0760 - accuracy: 0.6317 - val_loss: 3.3693 - val_accuracy: 0.4589\n",
            "Epoch 4556/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0614 - accuracy: 0.6477\n",
            "Epoch 04556: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0805 - accuracy: 0.6403 - val_loss: 3.4649 - val_accuracy: 0.4589\n",
            "Epoch 4557/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0616 - accuracy: 0.6386\n",
            "Epoch 04557: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0616 - accuracy: 0.6386 - val_loss: 3.3086 - val_accuracy: 0.4658\n",
            "Epoch 4558/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0394 - accuracy: 0.6265\n",
            "Epoch 04558: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0394 - accuracy: 0.6265 - val_loss: 3.3042 - val_accuracy: 0.4521\n",
            "Epoch 4559/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0682 - accuracy: 0.6335\n",
            "Epoch 04559: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0482 - accuracy: 0.6248 - val_loss: 3.1961 - val_accuracy: 0.4384\n",
            "Epoch 4560/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1272 - accuracy: 0.5885\n",
            "Epoch 04560: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1806 - accuracy: 0.5869 - val_loss: 3.2553 - val_accuracy: 0.4452\n",
            "Epoch 4561/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0782 - accuracy: 0.6534\n",
            "Epoch 04561: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1224 - accuracy: 0.6248 - val_loss: 3.2416 - val_accuracy: 0.4452\n",
            "Epoch 4562/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0809 - accuracy: 0.6531\n",
            "Epoch 04562: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0590 - accuracy: 0.6506 - val_loss: 3.1811 - val_accuracy: 0.4521\n",
            "Epoch 4563/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0621 - accuracy: 0.6198\n",
            "Epoch 04563: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0120 - accuracy: 0.6351 - val_loss: 3.2291 - val_accuracy: 0.4247\n",
            "Epoch 4564/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0195 - accuracy: 0.6302\n",
            "Epoch 04564: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0885 - accuracy: 0.6299 - val_loss: 3.2728 - val_accuracy: 0.4041\n",
            "Epoch 4565/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1498 - accuracy: 0.6187\n",
            "Epoch 04565: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1678 - accuracy: 0.6162 - val_loss: 3.1883 - val_accuracy: 0.4247\n",
            "Epoch 4566/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0298 - accuracy: 0.6250\n",
            "Epoch 04566: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0272 - accuracy: 0.6265 - val_loss: 3.0838 - val_accuracy: 0.4315\n",
            "Epoch 4567/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9654 - accuracy: 0.6344\n",
            "Epoch 04567: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0320 - accuracy: 0.6196 - val_loss: 3.1248 - val_accuracy: 0.4384\n",
            "Epoch 4568/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.1954 - accuracy: 0.6181\n",
            "Epoch 04568: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1898 - accuracy: 0.6179 - val_loss: 3.2258 - val_accuracy: 0.4110\n",
            "Epoch 4569/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1640 - accuracy: 0.6250\n",
            "Epoch 04569: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1332 - accuracy: 0.6179 - val_loss: 3.1615 - val_accuracy: 0.4384\n",
            "Epoch 4570/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0314 - accuracy: 0.6648\n",
            "Epoch 04570: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0732 - accuracy: 0.6437 - val_loss: 3.2374 - val_accuracy: 0.4110\n",
            "Epoch 4571/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.0909 - accuracy: 0.6229\n",
            "Epoch 04571: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.0954 - accuracy: 0.6145 - val_loss: 3.2038 - val_accuracy: 0.4452\n",
            "Epoch 4572/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1028 - accuracy: 0.6193\n",
            "Epoch 04572: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0704 - accuracy: 0.6248 - val_loss: 3.2614 - val_accuracy: 0.4452\n",
            "Epoch 4573/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1172 - accuracy: 0.6282\n",
            "Epoch 04573: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1172 - accuracy: 0.6282 - val_loss: 3.2071 - val_accuracy: 0.4384\n",
            "Epoch 4574/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0820 - accuracy: 0.5964\n",
            "Epoch 04574: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1032 - accuracy: 0.6024 - val_loss: 3.1586 - val_accuracy: 0.4589\n",
            "Epoch 4575/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0463 - accuracy: 0.6536\n",
            "Epoch 04575: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0264 - accuracy: 0.6592 - val_loss: 3.0590 - val_accuracy: 0.4521\n",
            "Epoch 4576/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0560 - accuracy: 0.6224\n",
            "Epoch 04576: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0212 - accuracy: 0.6420 - val_loss: 3.2164 - val_accuracy: 0.4315\n",
            "Epoch 4577/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9584 - accuracy: 0.6438\n",
            "Epoch 04577: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0270 - accuracy: 0.6299 - val_loss: 3.0888 - val_accuracy: 0.4384\n",
            "Epoch 4578/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0359 - accuracy: 0.6302\n",
            "Epoch 04578: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0361 - accuracy: 0.6145 - val_loss: 3.0278 - val_accuracy: 0.4452\n",
            "Epoch 4579/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1421 - accuracy: 0.6222\n",
            "Epoch 04579: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1394 - accuracy: 0.6265 - val_loss: 3.0336 - val_accuracy: 0.4315\n",
            "Epoch 4580/5000\n",
            "16/19 [========================>.....] - ETA: 0s - loss: 1.1760 - accuracy: 0.6035\n",
            "Epoch 04580: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.1571 - accuracy: 0.6179 - val_loss: 2.9125 - val_accuracy: 0.4384\n",
            "Epoch 4581/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.2036 - accuracy: 0.6051\n",
            "Epoch 04581: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1706 - accuracy: 0.6127 - val_loss: 2.9103 - val_accuracy: 0.4315\n",
            "Epoch 4582/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0046 - accuracy: 0.6693\n",
            "Epoch 04582: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0603 - accuracy: 0.6506 - val_loss: 2.9383 - val_accuracy: 0.4452\n",
            "Epoch 4583/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0156 - accuracy: 0.6406\n",
            "Epoch 04583: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9930 - accuracy: 0.6437 - val_loss: 2.9176 - val_accuracy: 0.4247\n",
            "Epoch 4584/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1336 - accuracy: 0.6156\n",
            "Epoch 04584: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0965 - accuracy: 0.6179 - val_loss: 2.9499 - val_accuracy: 0.4247\n",
            "Epoch 4585/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0287 - accuracy: 0.6418\n",
            "Epoch 04585: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0109 - accuracy: 0.6506 - val_loss: 3.1228 - val_accuracy: 0.4247\n",
            "Epoch 4586/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9732 - accuracy: 0.6587\n",
            "Epoch 04586: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9690 - accuracy: 0.6558 - val_loss: 3.1705 - val_accuracy: 0.4178\n",
            "Epoch 4587/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0029 - accuracy: 0.6536\n",
            "Epoch 04587: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9919 - accuracy: 0.6506 - val_loss: 3.1248 - val_accuracy: 0.4247\n",
            "Epoch 4588/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9143 - accuracy: 0.6797\n",
            "Epoch 04588: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9683 - accuracy: 0.6695 - val_loss: 3.1653 - val_accuracy: 0.4384\n",
            "Epoch 4589/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0185 - accuracy: 0.6536\n",
            "Epoch 04589: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0456 - accuracy: 0.6386 - val_loss: 3.1390 - val_accuracy: 0.4452\n",
            "Epoch 4590/5000\n",
            "16/19 [========================>.....] - ETA: 0s - loss: 1.0808 - accuracy: 0.6152\n",
            "Epoch 04590: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.0647 - accuracy: 0.6196 - val_loss: 3.1181 - val_accuracy: 0.4452\n",
            "Epoch 4591/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0179 - accuracy: 0.6609\n",
            "Epoch 04591: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0179 - accuracy: 0.6609 - val_loss: 3.1572 - val_accuracy: 0.4521\n",
            "Epoch 4592/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9612 - accuracy: 0.6695\n",
            "Epoch 04592: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9612 - accuracy: 0.6695 - val_loss: 3.1686 - val_accuracy: 0.4247\n",
            "Epoch 4593/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0835 - accuracy: 0.6172\n",
            "Epoch 04593: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0877 - accuracy: 0.6231 - val_loss: 3.0614 - val_accuracy: 0.4247\n",
            "Epoch 4594/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0928 - accuracy: 0.6335\n",
            "Epoch 04594: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1533 - accuracy: 0.6024 - val_loss: 2.9957 - val_accuracy: 0.4726\n",
            "Epoch 4595/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0091 - accuracy: 0.6392\n",
            "Epoch 04595: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0179 - accuracy: 0.6403 - val_loss: 3.1037 - val_accuracy: 0.4589\n",
            "Epoch 4596/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1043 - accuracy: 0.6484\n",
            "Epoch 04596: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1395 - accuracy: 0.6231 - val_loss: 3.1478 - val_accuracy: 0.4315\n",
            "Epoch 4597/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0203 - accuracy: 0.6562\n",
            "Epoch 04597: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0959 - accuracy: 0.6196 - val_loss: 3.0821 - val_accuracy: 0.4384\n",
            "Epoch 4598/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1391 - accuracy: 0.5911\n",
            "Epoch 04598: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0764 - accuracy: 0.6196 - val_loss: 3.0726 - val_accuracy: 0.4384\n",
            "Epoch 4599/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0504 - accuracy: 0.6307\n",
            "Epoch 04599: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1656 - accuracy: 0.6196 - val_loss: 2.9618 - val_accuracy: 0.4384\n",
            "Epoch 4600/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0376 - accuracy: 0.6432\n",
            "Epoch 04600: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0880 - accuracy: 0.6248 - val_loss: 2.9030 - val_accuracy: 0.4178\n",
            "Epoch 4601/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 1.1271 - accuracy: 0.6176\n",
            "Epoch 04601: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1122 - accuracy: 0.6231 - val_loss: 2.9644 - val_accuracy: 0.4384\n",
            "Epoch 4602/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0174 - accuracy: 0.6276\n",
            "Epoch 04602: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0428 - accuracy: 0.6179 - val_loss: 2.9537 - val_accuracy: 0.4384\n",
            "Epoch 4603/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1264 - accuracy: 0.6370\n",
            "Epoch 04603: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1245 - accuracy: 0.6351 - val_loss: 3.1393 - val_accuracy: 0.4315\n",
            "Epoch 4604/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1543 - accuracy: 0.6178\n",
            "Epoch 04604: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1384 - accuracy: 0.6248 - val_loss: 3.1149 - val_accuracy: 0.3904\n",
            "Epoch 4605/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1768 - accuracy: 0.6108\n",
            "Epoch 04605: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1169 - accuracy: 0.6179 - val_loss: 3.0968 - val_accuracy: 0.3904\n",
            "Epoch 4606/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1099 - accuracy: 0.6193\n",
            "Epoch 04606: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0754 - accuracy: 0.6351 - val_loss: 3.2105 - val_accuracy: 0.4041\n",
            "Epoch 4607/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1532 - accuracy: 0.6051\n",
            "Epoch 04607: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0905 - accuracy: 0.6127 - val_loss: 3.2129 - val_accuracy: 0.4315\n",
            "Epoch 4608/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 1.1375 - accuracy: 0.6250\n",
            "Epoch 04608: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.1216 - accuracy: 0.6265 - val_loss: 3.2026 - val_accuracy: 0.4384\n",
            "Epoch 4609/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0362 - accuracy: 0.6449\n",
            "Epoch 04609: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0210 - accuracy: 0.6489 - val_loss: 3.0448 - val_accuracy: 0.4384\n",
            "Epoch 4610/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1023 - accuracy: 0.6406\n",
            "Epoch 04610: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1427 - accuracy: 0.6317 - val_loss: 3.0065 - val_accuracy: 0.4178\n",
            "Epoch 4611/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0501 - accuracy: 0.6328\n",
            "Epoch 04611: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1043 - accuracy: 0.6024 - val_loss: 3.0618 - val_accuracy: 0.4452\n",
            "Epoch 4612/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0742 - accuracy: 0.5955\n",
            "Epoch 04612: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0753 - accuracy: 0.5955 - val_loss: 3.3128 - val_accuracy: 0.4589\n",
            "Epoch 4613/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1226 - accuracy: 0.6068\n",
            "Epoch 04613: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1081 - accuracy: 0.6145 - val_loss: 3.1814 - val_accuracy: 0.4521\n",
            "Epoch 4614/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1151 - accuracy: 0.6354\n",
            "Epoch 04614: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0789 - accuracy: 0.6472 - val_loss: 3.1641 - val_accuracy: 0.4384\n",
            "Epoch 4615/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1447 - accuracy: 0.6420\n",
            "Epoch 04615: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0708 - accuracy: 0.6454 - val_loss: 3.0839 - val_accuracy: 0.4247\n",
            "Epoch 4616/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1610 - accuracy: 0.6041\n",
            "Epoch 04616: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1610 - accuracy: 0.6041 - val_loss: 3.0978 - val_accuracy: 0.4315\n",
            "Epoch 4617/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.2707 - accuracy: 0.6010\n",
            "Epoch 04617: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.1609 - accuracy: 0.6282 - val_loss: 3.1577 - val_accuracy: 0.4452\n",
            "Epoch 4618/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.8859 - accuracy: 0.6693\n",
            "Epoch 04618: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9206 - accuracy: 0.6472 - val_loss: 3.1257 - val_accuracy: 0.4452\n",
            "Epoch 4619/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9805 - accuracy: 0.6250\n",
            "Epoch 04619: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9902 - accuracy: 0.6317 - val_loss: 3.1929 - val_accuracy: 0.4589\n",
            "Epoch 4620/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0822 - accuracy: 0.6094\n",
            "Epoch 04620: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0669 - accuracy: 0.6351 - val_loss: 3.1112 - val_accuracy: 0.4452\n",
            "Epoch 4621/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0465 - accuracy: 0.6406\n",
            "Epoch 04621: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0508 - accuracy: 0.6368 - val_loss: 3.1518 - val_accuracy: 0.4521\n",
            "Epoch 4622/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9832 - accuracy: 0.6514\n",
            "Epoch 04622: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9970 - accuracy: 0.6420 - val_loss: 3.1897 - val_accuracy: 0.4384\n",
            "Epoch 4623/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9837 - accuracy: 0.6538\n",
            "Epoch 04623: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0026 - accuracy: 0.6437 - val_loss: 3.1358 - val_accuracy: 0.4658\n",
            "Epoch 4624/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0340 - accuracy: 0.6484\n",
            "Epoch 04624: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0749 - accuracy: 0.6437 - val_loss: 3.1886 - val_accuracy: 0.4589\n",
            "Epoch 4625/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0037 - accuracy: 0.6562\n",
            "Epoch 04625: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0397 - accuracy: 0.6368 - val_loss: 3.0541 - val_accuracy: 0.4521\n",
            "Epoch 4626/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9647 - accuracy: 0.6562\n",
            "Epoch 04626: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9698 - accuracy: 0.6627 - val_loss: 3.1246 - val_accuracy: 0.4384\n",
            "Epoch 4627/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0439 - accuracy: 0.6322\n",
            "Epoch 04627: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0162 - accuracy: 0.6437 - val_loss: 3.0885 - val_accuracy: 0.4658\n",
            "Epoch 4628/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9815 - accuracy: 0.6849\n",
            "Epoch 04628: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9836 - accuracy: 0.6764 - val_loss: 3.2382 - val_accuracy: 0.4589\n",
            "Epoch 4629/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0141 - accuracy: 0.6302\n",
            "Epoch 04629: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0155 - accuracy: 0.6317 - val_loss: 3.1917 - val_accuracy: 0.4589\n",
            "Epoch 4630/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0111 - accuracy: 0.6641\n",
            "Epoch 04630: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0025 - accuracy: 0.6609 - val_loss: 3.1040 - val_accuracy: 0.4521\n",
            "Epoch 4631/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9305 - accuracy: 0.6733\n",
            "Epoch 04631: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9451 - accuracy: 0.6644 - val_loss: 3.2225 - val_accuracy: 0.4521\n",
            "Epoch 4632/5000\n",
            "16/19 [========================>.....] - ETA: 0s - loss: 1.0635 - accuracy: 0.6270\n",
            "Epoch 04632: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.0647 - accuracy: 0.6299 - val_loss: 3.2591 - val_accuracy: 0.4658\n",
            "Epoch 4633/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9929 - accuracy: 0.6469\n",
            "Epoch 04633: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0174 - accuracy: 0.6506 - val_loss: 3.2003 - val_accuracy: 0.4658\n",
            "Epoch 4634/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0661 - accuracy: 0.6298\n",
            "Epoch 04634: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0088 - accuracy: 0.6420 - val_loss: 3.2009 - val_accuracy: 0.4658\n",
            "Epoch 4635/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9799 - accuracy: 0.6510\n",
            "Epoch 04635: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9745 - accuracy: 0.6592 - val_loss: 3.1171 - val_accuracy: 0.4452\n",
            "Epoch 4636/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0912 - accuracy: 0.6406\n",
            "Epoch 04636: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0498 - accuracy: 0.6575 - val_loss: 3.1621 - val_accuracy: 0.4452\n",
            "Epoch 4637/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9418 - accuracy: 0.6589\n",
            "Epoch 04637: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9293 - accuracy: 0.6730 - val_loss: 3.3289 - val_accuracy: 0.4315\n",
            "Epoch 4638/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9296 - accuracy: 0.6591\n",
            "Epoch 04638: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9193 - accuracy: 0.6747 - val_loss: 3.4084 - val_accuracy: 0.3973\n",
            "Epoch 4639/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9343 - accuracy: 0.6867\n",
            "Epoch 04639: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9343 - accuracy: 0.6867 - val_loss: 3.3649 - val_accuracy: 0.4384\n",
            "Epoch 4640/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9398 - accuracy: 0.6823\n",
            "Epoch 04640: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9109 - accuracy: 0.6816 - val_loss: 3.2925 - val_accuracy: 0.4452\n",
            "Epoch 4641/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9180 - accuracy: 0.6989\n",
            "Epoch 04641: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.8658 - accuracy: 0.7005 - val_loss: 3.3270 - val_accuracy: 0.4452\n",
            "Epoch 4642/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9360 - accuracy: 0.6641\n",
            "Epoch 04642: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9547 - accuracy: 0.6609 - val_loss: 3.2540 - val_accuracy: 0.4452\n",
            "Epoch 4643/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0014 - accuracy: 0.6351\n",
            "Epoch 04643: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0014 - accuracy: 0.6351 - val_loss: 3.4917 - val_accuracy: 0.4247\n",
            "Epoch 4644/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9746 - accuracy: 0.6705\n",
            "Epoch 04644: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1194 - accuracy: 0.6386 - val_loss: 3.2521 - val_accuracy: 0.4521\n",
            "Epoch 4645/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1054 - accuracy: 0.6094\n",
            "Epoch 04645: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0139 - accuracy: 0.6351 - val_loss: 3.3509 - val_accuracy: 0.4384\n",
            "Epoch 4646/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0646 - accuracy: 0.6510\n",
            "Epoch 04646: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0206 - accuracy: 0.6558 - val_loss: 3.2946 - val_accuracy: 0.4452\n",
            "Epoch 4647/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9958 - accuracy: 0.6661\n",
            "Epoch 04647: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9958 - accuracy: 0.6661 - val_loss: 3.1800 - val_accuracy: 0.4315\n",
            "Epoch 4648/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1104 - accuracy: 0.6432\n",
            "Epoch 04648: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1131 - accuracy: 0.6386 - val_loss: 3.3992 - val_accuracy: 0.4589\n",
            "Epoch 4649/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1270 - accuracy: 0.6477\n",
            "Epoch 04649: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0962 - accuracy: 0.6437 - val_loss: 3.4493 - val_accuracy: 0.4589\n",
            "Epoch 4650/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0989 - accuracy: 0.6432\n",
            "Epoch 04650: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0695 - accuracy: 0.6437 - val_loss: 3.3714 - val_accuracy: 0.4452\n",
            "Epoch 4651/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1123 - accuracy: 0.6179\n",
            "Epoch 04651: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.1123 - accuracy: 0.6179 - val_loss: 3.2090 - val_accuracy: 0.4589\n",
            "Epoch 4652/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1395 - accuracy: 0.6165\n",
            "Epoch 04652: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0579 - accuracy: 0.6454 - val_loss: 3.2715 - val_accuracy: 0.4452\n",
            "Epoch 4653/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9835 - accuracy: 0.6589\n",
            "Epoch 04653: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0413 - accuracy: 0.6351 - val_loss: 3.3650 - val_accuracy: 0.4521\n",
            "Epoch 4654/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0758 - accuracy: 0.6364\n",
            "Epoch 04654: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0170 - accuracy: 0.6558 - val_loss: 3.2649 - val_accuracy: 0.4178\n",
            "Epoch 4655/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0660 - accuracy: 0.6458\n",
            "Epoch 04655: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0551 - accuracy: 0.6472 - val_loss: 3.2088 - val_accuracy: 0.4247\n",
            "Epoch 4656/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0456 - accuracy: 0.6392\n",
            "Epoch 04656: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0671 - accuracy: 0.6437 - val_loss: 3.1529 - val_accuracy: 0.4315\n",
            "Epoch 4657/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9807 - accuracy: 0.6562\n",
            "Epoch 04657: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0205 - accuracy: 0.6437 - val_loss: 3.1844 - val_accuracy: 0.4452\n",
            "Epoch 4658/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0279 - accuracy: 0.6477\n",
            "Epoch 04658: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0125 - accuracy: 0.6575 - val_loss: 3.1407 - val_accuracy: 0.4315\n",
            "Epoch 4659/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0359 - accuracy: 0.6278\n",
            "Epoch 04659: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9909 - accuracy: 0.6489 - val_loss: 3.2148 - val_accuracy: 0.4315\n",
            "Epoch 4660/5000\n",
            " 8/19 [===========>..................] - ETA: 0s - loss: 1.0026 - accuracy: 0.6484\n",
            "Epoch 04660: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0025 - accuracy: 0.6627 - val_loss: 3.1783 - val_accuracy: 0.4384\n",
            "Epoch 4661/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9467 - accuracy: 0.6849\n",
            "Epoch 04661: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9365 - accuracy: 0.6816 - val_loss: 3.2023 - val_accuracy: 0.4247\n",
            "Epoch 4662/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9652 - accuracy: 0.6667\n",
            "Epoch 04662: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9521 - accuracy: 0.6695 - val_loss: 3.1428 - val_accuracy: 0.4178\n",
            "Epoch 4663/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0395 - accuracy: 0.6676\n",
            "Epoch 04663: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0349 - accuracy: 0.6575 - val_loss: 3.5030 - val_accuracy: 0.4178\n",
            "Epoch 4664/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0323 - accuracy: 0.6656\n",
            "Epoch 04664: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0028 - accuracy: 0.6644 - val_loss: 3.1759 - val_accuracy: 0.4110\n",
            "Epoch 4665/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.9365 - accuracy: 0.6493\n",
            "Epoch 04665: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9381 - accuracy: 0.6489 - val_loss: 3.1879 - val_accuracy: 0.4658\n",
            "Epoch 4666/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0506 - accuracy: 0.6761\n",
            "Epoch 04666: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0792 - accuracy: 0.6437 - val_loss: 3.1017 - val_accuracy: 0.4452\n",
            "Epoch 4667/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0245 - accuracy: 0.6354\n",
            "Epoch 04667: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0722 - accuracy: 0.6317 - val_loss: 3.2016 - val_accuracy: 0.4384\n",
            "Epoch 4668/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.1491 - accuracy: 0.5938\n",
            "Epoch 04668: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1165 - accuracy: 0.6024 - val_loss: 3.0584 - val_accuracy: 0.4452\n",
            "Epoch 4669/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1655 - accuracy: 0.6120\n",
            "Epoch 04669: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1208 - accuracy: 0.6179 - val_loss: 3.0743 - val_accuracy: 0.4589\n",
            "Epoch 4670/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0753 - accuracy: 0.6536\n",
            "Epoch 04670: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0917 - accuracy: 0.6540 - val_loss: 3.1236 - val_accuracy: 0.3836\n",
            "Epoch 4671/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0066 - accuracy: 0.6250\n",
            "Epoch 04671: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0245 - accuracy: 0.6127 - val_loss: 2.9457 - val_accuracy: 0.4247\n",
            "Epoch 4672/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0206 - accuracy: 0.6500\n",
            "Epoch 04672: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0525 - accuracy: 0.6403 - val_loss: 3.0828 - val_accuracy: 0.4384\n",
            "Epoch 4673/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0808 - accuracy: 0.6224\n",
            "Epoch 04673: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0755 - accuracy: 0.6076 - val_loss: 3.1536 - val_accuracy: 0.4178\n",
            "Epoch 4674/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0796 - accuracy: 0.6172\n",
            "Epoch 04674: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0981 - accuracy: 0.6213 - val_loss: 3.0834 - val_accuracy: 0.4452\n",
            "Epoch 4675/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0651 - accuracy: 0.6432\n",
            "Epoch 04675: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0980 - accuracy: 0.6351 - val_loss: 2.9529 - val_accuracy: 0.4452\n",
            "Epoch 4676/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1121 - accuracy: 0.5986\n",
            "Epoch 04676: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1141 - accuracy: 0.6059 - val_loss: 3.0445 - val_accuracy: 0.4247\n",
            "Epoch 4677/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0486 - accuracy: 0.6317\n",
            "Epoch 04677: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0486 - accuracy: 0.6317 - val_loss: 3.0898 - val_accuracy: 0.4452\n",
            "Epoch 4678/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0820 - accuracy: 0.6198\n",
            "Epoch 04678: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0735 - accuracy: 0.6196 - val_loss: 3.1349 - val_accuracy: 0.4521\n",
            "Epoch 4679/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0424 - accuracy: 0.6224\n",
            "Epoch 04679: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0230 - accuracy: 0.6334 - val_loss: 3.1984 - val_accuracy: 0.4247\n",
            "Epoch 4680/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0084 - accuracy: 0.6432\n",
            "Epoch 04680: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0323 - accuracy: 0.6317 - val_loss: 3.2538 - val_accuracy: 0.4178\n",
            "Epoch 4681/5000\n",
            "16/19 [========================>.....] - ETA: 0s - loss: 1.0372 - accuracy: 0.6309\n",
            "Epoch 04681: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0557 - accuracy: 0.6231 - val_loss: 3.1412 - val_accuracy: 0.4178\n",
            "Epoch 4682/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1266 - accuracy: 0.6346\n",
            "Epoch 04682: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0805 - accuracy: 0.6437 - val_loss: 3.2489 - val_accuracy: 0.4247\n",
            "Epoch 4683/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0533 - accuracy: 0.6181\n",
            "Epoch 04683: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0284 - accuracy: 0.6334 - val_loss: 3.3332 - val_accuracy: 0.4178\n",
            "Epoch 4684/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9049 - accuracy: 0.7043\n",
            "Epoch 04684: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.8977 - accuracy: 0.7040 - val_loss: 3.2641 - val_accuracy: 0.4247\n",
            "Epoch 4685/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0073 - accuracy: 0.6364\n",
            "Epoch 04685: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0383 - accuracy: 0.6282 - val_loss: 3.1972 - val_accuracy: 0.4384\n",
            "Epoch 4686/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9791 - accuracy: 0.6587\n",
            "Epoch 04686: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9799 - accuracy: 0.6609 - val_loss: 3.1736 - val_accuracy: 0.4521\n",
            "Epoch 4687/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9250 - accuracy: 0.6587\n",
            "Epoch 04687: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9693 - accuracy: 0.6558 - val_loss: 3.2590 - val_accuracy: 0.4452\n",
            "Epoch 4688/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0830 - accuracy: 0.6278\n",
            "Epoch 04688: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1179 - accuracy: 0.6179 - val_loss: 3.1234 - val_accuracy: 0.4384\n",
            "Epoch 4689/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0456 - accuracy: 0.6250\n",
            "Epoch 04689: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0265 - accuracy: 0.6231 - val_loss: 3.1406 - val_accuracy: 0.4178\n",
            "Epoch 4690/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0792 - accuracy: 0.6198\n",
            "Epoch 04690: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1180 - accuracy: 0.6145 - val_loss: 3.0824 - val_accuracy: 0.4110\n",
            "Epoch 4691/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1358 - accuracy: 0.6224\n",
            "Epoch 04691: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1246 - accuracy: 0.6162 - val_loss: 3.1574 - val_accuracy: 0.3973\n",
            "Epoch 4692/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9981 - accuracy: 0.6484\n",
            "Epoch 04692: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0053 - accuracy: 0.6506 - val_loss: 3.1040 - val_accuracy: 0.4178\n",
            "Epoch 4693/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0807 - accuracy: 0.5990\n",
            "Epoch 04693: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0534 - accuracy: 0.6282 - val_loss: 3.3751 - val_accuracy: 0.4521\n",
            "Epoch 4694/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0005 - accuracy: 0.6797\n",
            "Epoch 04694: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0020 - accuracy: 0.6764 - val_loss: 3.2980 - val_accuracy: 0.4658\n",
            "Epoch 4695/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9928 - accuracy: 0.6534\n",
            "Epoch 04695: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0181 - accuracy: 0.6506 - val_loss: 3.1761 - val_accuracy: 0.4452\n",
            "Epoch 4696/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9068 - accuracy: 0.6818\n",
            "Epoch 04696: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9064 - accuracy: 0.6764 - val_loss: 3.1990 - val_accuracy: 0.4384\n",
            "Epoch 4697/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0146 - accuracy: 0.6313\n",
            "Epoch 04697: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9788 - accuracy: 0.6420 - val_loss: 3.1846 - val_accuracy: 0.4452\n",
            "Epoch 4698/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.9471 - accuracy: 0.6736\n",
            "Epoch 04698: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9578 - accuracy: 0.6713 - val_loss: 3.1764 - val_accuracy: 0.4589\n",
            "Epoch 4699/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9906 - accuracy: 0.6627\n",
            "Epoch 04699: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9906 - accuracy: 0.6627 - val_loss: 3.3416 - val_accuracy: 0.4589\n",
            "Epoch 4700/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0960 - accuracy: 0.5938\n",
            "Epoch 04700: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0270 - accuracy: 0.6299 - val_loss: 3.3309 - val_accuracy: 0.4110\n",
            "Epoch 4701/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0164 - accuracy: 0.6274\n",
            "Epoch 04701: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0225 - accuracy: 0.6265 - val_loss: 3.2973 - val_accuracy: 0.4247\n",
            "Epoch 4702/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.9953 - accuracy: 0.6489\n",
            "Epoch 04702: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9953 - accuracy: 0.6489 - val_loss: 3.3257 - val_accuracy: 0.4315\n",
            "Epoch 4703/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0801 - accuracy: 0.6510\n",
            "Epoch 04703: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0366 - accuracy: 0.6472 - val_loss: 3.4464 - val_accuracy: 0.4315\n",
            "Epoch 4704/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0067 - accuracy: 0.6538\n",
            "Epoch 04704: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0059 - accuracy: 0.6437 - val_loss: 3.3627 - val_accuracy: 0.4178\n",
            "Epoch 4705/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9412 - accuracy: 0.6827\n",
            "Epoch 04705: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9763 - accuracy: 0.6661 - val_loss: 3.3752 - val_accuracy: 0.4315\n",
            "Epoch 4706/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0240 - accuracy: 0.6458\n",
            "Epoch 04706: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0855 - accuracy: 0.6334 - val_loss: 3.3092 - val_accuracy: 0.4247\n",
            "Epoch 4707/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0083 - accuracy: 0.6328\n",
            "Epoch 04707: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0199 - accuracy: 0.6299 - val_loss: 3.1710 - val_accuracy: 0.4178\n",
            "Epoch 4708/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 0.9766 - accuracy: 0.6451\n",
            "Epoch 04708: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9909 - accuracy: 0.6540 - val_loss: 3.1068 - val_accuracy: 0.4178\n",
            "Epoch 4709/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0042 - accuracy: 0.6346\n",
            "Epoch 04709: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0077 - accuracy: 0.6334 - val_loss: 3.1026 - val_accuracy: 0.4178\n",
            "Epoch 4710/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9942 - accuracy: 0.6354\n",
            "Epoch 04710: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9726 - accuracy: 0.6420 - val_loss: 3.2055 - val_accuracy: 0.4315\n",
            "Epoch 4711/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0232 - accuracy: 0.6370\n",
            "Epoch 04711: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0480 - accuracy: 0.6317 - val_loss: 3.2792 - val_accuracy: 0.4315\n",
            "Epoch 4712/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0160 - accuracy: 0.6386\n",
            "Epoch 04712: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0160 - accuracy: 0.6386 - val_loss: 3.2158 - val_accuracy: 0.4178\n",
            "Epoch 4713/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9934 - accuracy: 0.6062\n",
            "Epoch 04713: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0122 - accuracy: 0.6127 - val_loss: 3.1626 - val_accuracy: 0.4452\n",
            "Epoch 4714/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1136 - accuracy: 0.6418\n",
            "Epoch 04714: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1206 - accuracy: 0.6368 - val_loss: 3.1483 - val_accuracy: 0.4521\n",
            "Epoch 4715/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0018 - accuracy: 0.6562\n",
            "Epoch 04715: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0294 - accuracy: 0.6472 - val_loss: 3.0239 - val_accuracy: 0.4589\n",
            "Epoch 4716/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 0.9566 - accuracy: 0.6597\n",
            "Epoch 04716: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0263 - accuracy: 0.6368 - val_loss: 3.0894 - val_accuracy: 0.4589\n",
            "Epoch 4717/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9672 - accuracy: 0.6827\n",
            "Epoch 04717: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0218 - accuracy: 0.6609 - val_loss: 3.2467 - val_accuracy: 0.4452\n",
            "Epoch 4718/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0787 - accuracy: 0.6589\n",
            "Epoch 04718: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0809 - accuracy: 0.6454 - val_loss: 3.2161 - val_accuracy: 0.4521\n",
            "Epoch 4719/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9745 - accuracy: 0.6484\n",
            "Epoch 04719: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0031 - accuracy: 0.6403 - val_loss: 3.2096 - val_accuracy: 0.4521\n",
            "Epoch 4720/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9684 - accuracy: 0.6901\n",
            "Epoch 04720: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0136 - accuracy: 0.6713 - val_loss: 3.3290 - val_accuracy: 0.4247\n",
            "Epoch 4721/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0832 - accuracy: 0.6806\n",
            "Epoch 04721: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0824 - accuracy: 0.6472 - val_loss: 3.2238 - val_accuracy: 0.4178\n",
            "Epoch 4722/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0878 - accuracy: 0.6322\n",
            "Epoch 04722: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0936 - accuracy: 0.6196 - val_loss: 3.1778 - val_accuracy: 0.4178\n",
            "Epoch 4723/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1794 - accuracy: 0.5833\n",
            "Epoch 04723: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1133 - accuracy: 0.6041 - val_loss: 3.1873 - val_accuracy: 0.4247\n",
            "Epoch 4724/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0638 - accuracy: 0.6120\n",
            "Epoch 04724: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1741 - accuracy: 0.5904 - val_loss: 3.3946 - val_accuracy: 0.4315\n",
            "Epoch 4725/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.2426 - accuracy: 0.6068\n",
            "Epoch 04725: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1875 - accuracy: 0.6093 - val_loss: 3.1382 - val_accuracy: 0.4452\n",
            "Epoch 4726/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0956 - accuracy: 0.6163\n",
            "Epoch 04726: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0977 - accuracy: 0.6162 - val_loss: 3.2248 - val_accuracy: 0.4247\n",
            "Epoch 4727/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1119 - accuracy: 0.6068\n",
            "Epoch 04727: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1050 - accuracy: 0.5990 - val_loss: 3.2421 - val_accuracy: 0.4247\n",
            "Epoch 4728/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0189 - accuracy: 0.6575\n",
            "Epoch 04728: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0189 - accuracy: 0.6575 - val_loss: 3.3907 - val_accuracy: 0.4315\n",
            "Epoch 4729/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0434 - accuracy: 0.6364\n",
            "Epoch 04729: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0772 - accuracy: 0.6265 - val_loss: 3.2097 - val_accuracy: 0.4452\n",
            "Epoch 4730/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0533 - accuracy: 0.6000\n",
            "Epoch 04730: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1168 - accuracy: 0.5852 - val_loss: 3.1004 - val_accuracy: 0.4247\n",
            "Epoch 4731/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.9592 - accuracy: 0.6406\n",
            "Epoch 04731: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9606 - accuracy: 0.6403 - val_loss: 3.1103 - val_accuracy: 0.4384\n",
            "Epoch 4732/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0057 - accuracy: 0.6454\n",
            "Epoch 04732: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0057 - accuracy: 0.6454 - val_loss: 3.1625 - val_accuracy: 0.4247\n",
            "Epoch 4733/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1028 - accuracy: 0.6370\n",
            "Epoch 04733: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0992 - accuracy: 0.6299 - val_loss: 3.3199 - val_accuracy: 0.4315\n",
            "Epoch 4734/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0378 - accuracy: 0.6328\n",
            "Epoch 04734: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0424 - accuracy: 0.6317 - val_loss: 3.1905 - val_accuracy: 0.4521\n",
            "Epoch 4735/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9828 - accuracy: 0.6619\n",
            "Epoch 04735: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9576 - accuracy: 0.6695 - val_loss: 3.1120 - val_accuracy: 0.4315\n",
            "Epoch 4736/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0932 - accuracy: 0.6080\n",
            "Epoch 04736: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0265 - accuracy: 0.6351 - val_loss: 3.2513 - val_accuracy: 0.4110\n",
            "Epoch 4737/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9929 - accuracy: 0.6648\n",
            "Epoch 04737: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0171 - accuracy: 0.6506 - val_loss: 3.3310 - val_accuracy: 0.4178\n",
            "Epoch 4738/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.1048 - accuracy: 0.5972\n",
            "Epoch 04738: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0411 - accuracy: 0.6386 - val_loss: 3.2539 - val_accuracy: 0.4110\n",
            "Epoch 4739/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0495 - accuracy: 0.6384\n",
            "Epoch 04739: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0177 - accuracy: 0.6454 - val_loss: 3.3168 - val_accuracy: 0.4110\n",
            "Epoch 4740/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9870 - accuracy: 0.6562\n",
            "Epoch 04740: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0229 - accuracy: 0.6523 - val_loss: 3.3900 - val_accuracy: 0.4178\n",
            "Epoch 4741/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0155 - accuracy: 0.6676\n",
            "Epoch 04741: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0170 - accuracy: 0.6454 - val_loss: 3.3939 - val_accuracy: 0.4178\n",
            "Epoch 4742/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1897 - accuracy: 0.6042\n",
            "Epoch 04742: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1601 - accuracy: 0.6179 - val_loss: 3.3636 - val_accuracy: 0.3836\n",
            "Epoch 4743/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.1172 - accuracy: 0.6083\n",
            "Epoch 04743: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1007 - accuracy: 0.6162 - val_loss: 3.2706 - val_accuracy: 0.4178\n",
            "Epoch 4744/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0667 - accuracy: 0.6202\n",
            "Epoch 04744: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0535 - accuracy: 0.6127 - val_loss: 3.1677 - val_accuracy: 0.4178\n",
            "Epoch 4745/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0574 - accuracy: 0.6265\n",
            "Epoch 04745: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0574 - accuracy: 0.6265 - val_loss: 3.1387 - val_accuracy: 0.4110\n",
            "Epoch 4746/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9875 - accuracy: 0.6761\n",
            "Epoch 04746: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0940 - accuracy: 0.6437 - val_loss: 3.2261 - val_accuracy: 0.4247\n",
            "Epoch 4747/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0972 - accuracy: 0.6302\n",
            "Epoch 04747: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0814 - accuracy: 0.6196 - val_loss: 3.3751 - val_accuracy: 0.3973\n",
            "Epoch 4748/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0150 - accuracy: 0.6755\n",
            "Epoch 04748: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0450 - accuracy: 0.6454 - val_loss: 3.3880 - val_accuracy: 0.4178\n",
            "Epoch 4749/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9163 - accuracy: 0.6619\n",
            "Epoch 04749: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.9399 - accuracy: 0.6695 - val_loss: 3.3883 - val_accuracy: 0.4110\n",
            "Epoch 4750/5000\n",
            "16/19 [========================>.....] - ETA: 0s - loss: 0.9745 - accuracy: 0.6602\n",
            "Epoch 04750: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0008 - accuracy: 0.6575 - val_loss: 3.3744 - val_accuracy: 0.4110\n",
            "Epoch 4751/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0612 - accuracy: 0.6406\n",
            "Epoch 04751: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0309 - accuracy: 0.6437 - val_loss: 3.3009 - val_accuracy: 0.4110\n",
            "Epoch 4752/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0719 - accuracy: 0.6250\n",
            "Epoch 04752: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1044 - accuracy: 0.6196 - val_loss: 3.3363 - val_accuracy: 0.4178\n",
            "Epoch 4753/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0323 - accuracy: 0.6187\n",
            "Epoch 04753: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0395 - accuracy: 0.6317 - val_loss: 3.3778 - val_accuracy: 0.4178\n",
            "Epoch 4754/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0322 - accuracy: 0.6562\n",
            "Epoch 04754: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0288 - accuracy: 0.6558 - val_loss: 3.2744 - val_accuracy: 0.4452\n",
            "Epoch 4755/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9277 - accuracy: 0.6591\n",
            "Epoch 04755: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9529 - accuracy: 0.6437 - val_loss: 3.3011 - val_accuracy: 0.4384\n",
            "Epoch 4756/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0304 - accuracy: 0.6222\n",
            "Epoch 04756: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0989 - accuracy: 0.6213 - val_loss: 3.3324 - val_accuracy: 0.4178\n",
            "Epoch 4757/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9058 - accuracy: 0.7102\n",
            "Epoch 04757: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9223 - accuracy: 0.6919 - val_loss: 3.4085 - val_accuracy: 0.4110\n",
            "Epoch 4758/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 0.9550 - accuracy: 0.6544\n",
            "Epoch 04758: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9652 - accuracy: 0.6523 - val_loss: 3.3027 - val_accuracy: 0.4521\n",
            "Epoch 4759/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9804 - accuracy: 0.6534\n",
            "Epoch 04759: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9795 - accuracy: 0.6523 - val_loss: 3.3781 - val_accuracy: 0.4315\n",
            "Epoch 4760/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0721 - accuracy: 0.6609\n",
            "Epoch 04760: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0721 - accuracy: 0.6609 - val_loss: 3.2819 - val_accuracy: 0.4384\n",
            "Epoch 4761/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0389 - accuracy: 0.6531\n",
            "Epoch 04761: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0553 - accuracy: 0.6196 - val_loss: 3.1852 - val_accuracy: 0.4178\n",
            "Epoch 4762/5000\n",
            "16/19 [========================>.....] - ETA: 0s - loss: 0.9534 - accuracy: 0.6465\n",
            "Epoch 04762: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9904 - accuracy: 0.6437 - val_loss: 3.2856 - val_accuracy: 0.4247\n",
            "Epoch 4763/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9948 - accuracy: 0.6335\n",
            "Epoch 04763: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9906 - accuracy: 0.6420 - val_loss: 3.1705 - val_accuracy: 0.4247\n",
            "Epoch 4764/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9622 - accuracy: 0.6619\n",
            "Epoch 04764: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9930 - accuracy: 0.6523 - val_loss: 3.2632 - val_accuracy: 0.4315\n",
            "Epoch 4765/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0280 - accuracy: 0.6619\n",
            "Epoch 04765: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0692 - accuracy: 0.6351 - val_loss: 3.2095 - val_accuracy: 0.4452\n",
            "Epoch 4766/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9120 - accuracy: 0.6733\n",
            "Epoch 04766: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9674 - accuracy: 0.6437 - val_loss: 3.1811 - val_accuracy: 0.4315\n",
            "Epoch 4767/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9385 - accuracy: 0.6615\n",
            "Epoch 04767: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9788 - accuracy: 0.6403 - val_loss: 3.0748 - val_accuracy: 0.4315\n",
            "Epoch 4768/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0311 - accuracy: 0.6473\n",
            "Epoch 04768: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0407 - accuracy: 0.6334 - val_loss: 3.1269 - val_accuracy: 0.4384\n",
            "Epoch 4769/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9804 - accuracy: 0.6335\n",
            "Epoch 04769: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0141 - accuracy: 0.6317 - val_loss: 3.1930 - val_accuracy: 0.4521\n",
            "Epoch 4770/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 0.9945 - accuracy: 0.6285\n",
            "Epoch 04770: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9781 - accuracy: 0.6420 - val_loss: 3.3230 - val_accuracy: 0.4452\n",
            "Epoch 4771/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.9790 - accuracy: 0.6406\n",
            "Epoch 04771: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.9769 - accuracy: 0.6420 - val_loss: 3.4120 - val_accuracy: 0.3836\n",
            "Epoch 4772/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0193 - accuracy: 0.6575\n",
            "Epoch 04772: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0193 - accuracy: 0.6575 - val_loss: 3.2804 - val_accuracy: 0.4247\n",
            "Epoch 4773/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0334 - accuracy: 0.6420\n",
            "Epoch 04773: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0607 - accuracy: 0.6386 - val_loss: 3.3194 - val_accuracy: 0.4247\n",
            "Epoch 4774/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1913 - accuracy: 0.5852\n",
            "Epoch 04774: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1378 - accuracy: 0.5955 - val_loss: 3.3376 - val_accuracy: 0.4315\n",
            "Epoch 4775/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1180 - accuracy: 0.6108\n",
            "Epoch 04775: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0747 - accuracy: 0.6145 - val_loss: 3.3961 - val_accuracy: 0.4384\n",
            "Epoch 4776/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1333 - accuracy: 0.6080\n",
            "Epoch 04776: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1338 - accuracy: 0.6041 - val_loss: 3.3058 - val_accuracy: 0.4178\n",
            "Epoch 4777/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0243 - accuracy: 0.6406\n",
            "Epoch 04777: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0236 - accuracy: 0.6368 - val_loss: 3.3657 - val_accuracy: 0.4452\n",
            "Epoch 4778/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0705 - accuracy: 0.6629\n",
            "Epoch 04778: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0894 - accuracy: 0.6558 - val_loss: 3.3664 - val_accuracy: 0.4384\n",
            "Epoch 4779/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0523 - accuracy: 0.6484\n",
            "Epoch 04779: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0849 - accuracy: 0.6368 - val_loss: 3.3117 - val_accuracy: 0.4452\n",
            "Epoch 4780/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1136 - accuracy: 0.6062\n",
            "Epoch 04780: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1071 - accuracy: 0.6127 - val_loss: 3.2564 - val_accuracy: 0.4315\n",
            "Epoch 4781/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0585 - accuracy: 0.6328\n",
            "Epoch 04781: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0675 - accuracy: 0.6351 - val_loss: 3.3523 - val_accuracy: 0.4521\n",
            "Epoch 4782/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1453 - accuracy: 0.6354\n",
            "Epoch 04782: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1129 - accuracy: 0.6299 - val_loss: 3.2917 - val_accuracy: 0.4315\n",
            "Epoch 4783/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0428 - accuracy: 0.6484\n",
            "Epoch 04783: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9930 - accuracy: 0.6523 - val_loss: 3.2527 - val_accuracy: 0.4589\n",
            "Epoch 4784/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0544 - accuracy: 0.6597\n",
            "Epoch 04784: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9911 - accuracy: 0.6609 - val_loss: 3.2570 - val_accuracy: 0.4178\n",
            "Epoch 4785/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0235 - accuracy: 0.6319\n",
            "Epoch 04785: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0268 - accuracy: 0.6317 - val_loss: 3.3901 - val_accuracy: 0.4178\n",
            "Epoch 4786/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0601 - accuracy: 0.6437\n",
            "Epoch 04786: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0601 - accuracy: 0.6437 - val_loss: 3.2428 - val_accuracy: 0.4178\n",
            "Epoch 4787/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0509 - accuracy: 0.6441\n",
            "Epoch 04787: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0513 - accuracy: 0.6437 - val_loss: 3.2269 - val_accuracy: 0.3904\n",
            "Epoch 4788/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1694 - accuracy: 0.5767\n",
            "Epoch 04788: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1210 - accuracy: 0.6093 - val_loss: 3.2494 - val_accuracy: 0.3904\n",
            "Epoch 4789/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1423 - accuracy: 0.6023\n",
            "Epoch 04789: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1851 - accuracy: 0.6317 - val_loss: 3.2411 - val_accuracy: 0.4384\n",
            "Epoch 4790/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1239 - accuracy: 0.6438\n",
            "Epoch 04790: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1031 - accuracy: 0.6351 - val_loss: 2.9736 - val_accuracy: 0.4247\n",
            "Epoch 4791/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0944 - accuracy: 0.6041\n",
            "Epoch 04791: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0944 - accuracy: 0.6041 - val_loss: 2.9947 - val_accuracy: 0.4521\n",
            "Epoch 4792/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.2310 - accuracy: 0.5885\n",
            "Epoch 04792: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1414 - accuracy: 0.5904 - val_loss: 2.9698 - val_accuracy: 0.4452\n",
            "Epoch 4793/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0676 - accuracy: 0.6172\n",
            "Epoch 04793: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0497 - accuracy: 0.6231 - val_loss: 3.0016 - val_accuracy: 0.4384\n",
            "Epoch 4794/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9888 - accuracy: 0.6302\n",
            "Epoch 04794: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0025 - accuracy: 0.6248 - val_loss: 2.9989 - val_accuracy: 0.4452\n",
            "Epoch 4795/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1413 - accuracy: 0.6276\n",
            "Epoch 04795: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1404 - accuracy: 0.6420 - val_loss: 3.0457 - val_accuracy: 0.4384\n",
            "Epoch 4796/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1170 - accuracy: 0.5972\n",
            "Epoch 04796: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1170 - accuracy: 0.5972 - val_loss: 2.9761 - val_accuracy: 0.4247\n",
            "Epoch 4797/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0877 - accuracy: 0.6302\n",
            "Epoch 04797: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0922 - accuracy: 0.6213 - val_loss: 3.3095 - val_accuracy: 0.3973\n",
            "Epoch 4798/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.3191 - accuracy: 0.5869\n",
            "Epoch 04798: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.3191 - accuracy: 0.5869 - val_loss: 3.0237 - val_accuracy: 0.4041\n",
            "Epoch 4799/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.1625 - accuracy: 0.5972\n",
            "Epoch 04799: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1679 - accuracy: 0.5972 - val_loss: 2.8753 - val_accuracy: 0.4315\n",
            "Epoch 4800/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 1.1617 - accuracy: 0.6048\n",
            "Epoch 04800: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.1437 - accuracy: 0.6076 - val_loss: 2.8399 - val_accuracy: 0.4384\n",
            "Epoch 4801/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.2099 - accuracy: 0.5767\n",
            "Epoch 04801: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2290 - accuracy: 0.5835 - val_loss: 2.7623 - val_accuracy: 0.4178\n",
            "Epoch 4802/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0926 - accuracy: 0.6165\n",
            "Epoch 04802: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1290 - accuracy: 0.6162 - val_loss: 2.8268 - val_accuracy: 0.4384\n",
            "Epoch 4803/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1068 - accuracy: 0.6094\n",
            "Epoch 04803: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1072 - accuracy: 0.6041 - val_loss: 2.8095 - val_accuracy: 0.4315\n",
            "Epoch 4804/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0196 - accuracy: 0.6130\n",
            "Epoch 04804: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0384 - accuracy: 0.6196 - val_loss: 2.8350 - val_accuracy: 0.4247\n",
            "Epoch 4805/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1785 - accuracy: 0.5398\n",
            "Epoch 04805: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0755 - accuracy: 0.5835 - val_loss: 2.8847 - val_accuracy: 0.4384\n",
            "Epoch 4806/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0379 - accuracy: 0.6562\n",
            "Epoch 04806: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0310 - accuracy: 0.6420 - val_loss: 2.8292 - val_accuracy: 0.4521\n",
            "Epoch 4807/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0350 - accuracy: 0.6562\n",
            "Epoch 04807: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0261 - accuracy: 0.6609 - val_loss: 2.9182 - val_accuracy: 0.4726\n",
            "Epoch 4808/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0323 - accuracy: 0.6484\n",
            "Epoch 04808: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0643 - accuracy: 0.6299 - val_loss: 3.0304 - val_accuracy: 0.4589\n",
            "Epoch 4809/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0674 - accuracy: 0.6024\n",
            "Epoch 04809: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0682 - accuracy: 0.6024 - val_loss: 3.1250 - val_accuracy: 0.4521\n",
            "Epoch 4810/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1243 - accuracy: 0.6082\n",
            "Epoch 04810: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1371 - accuracy: 0.6076 - val_loss: 3.1453 - val_accuracy: 0.4384\n",
            "Epoch 4811/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0528 - accuracy: 0.6130\n",
            "Epoch 04811: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0798 - accuracy: 0.6093 - val_loss: 3.1389 - val_accuracy: 0.4178\n",
            "Epoch 4812/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1117 - accuracy: 0.5833\n",
            "Epoch 04812: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1171 - accuracy: 0.5938 - val_loss: 3.1081 - val_accuracy: 0.4315\n",
            "Epoch 4813/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1081 - accuracy: 0.5913\n",
            "Epoch 04813: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1426 - accuracy: 0.6007 - val_loss: 3.2009 - val_accuracy: 0.4315\n",
            "Epoch 4814/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0720 - accuracy: 0.6719\n",
            "Epoch 04814: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0462 - accuracy: 0.6695 - val_loss: 2.9643 - val_accuracy: 0.4110\n",
            "Epoch 4815/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0754 - accuracy: 0.6224\n",
            "Epoch 04815: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0714 - accuracy: 0.6196 - val_loss: 3.0004 - val_accuracy: 0.4384\n",
            "Epoch 4816/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1205 - accuracy: 0.6156\n",
            "Epoch 04816: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1282 - accuracy: 0.6127 - val_loss: 2.9639 - val_accuracy: 0.4247\n",
            "Epoch 4817/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.2419 - accuracy: 0.5909\n",
            "Epoch 04817: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2274 - accuracy: 0.5972 - val_loss: 3.1076 - val_accuracy: 0.4384\n",
            "Epoch 4818/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1729 - accuracy: 0.5994\n",
            "Epoch 04818: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1502 - accuracy: 0.6076 - val_loss: 2.9973 - val_accuracy: 0.4384\n",
            "Epoch 4819/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.2048 - accuracy: 0.5906\n",
            "Epoch 04819: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1755 - accuracy: 0.5886 - val_loss: 3.0928 - val_accuracy: 0.4521\n",
            "Epoch 4820/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1776 - accuracy: 0.6172\n",
            "Epoch 04820: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1950 - accuracy: 0.6162 - val_loss: 3.0312 - val_accuracy: 0.4589\n",
            "Epoch 4821/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0850 - accuracy: 0.6619\n",
            "Epoch 04821: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1463 - accuracy: 0.6299 - val_loss: 3.0270 - val_accuracy: 0.4384\n",
            "Epoch 4822/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1056 - accuracy: 0.6154\n",
            "Epoch 04822: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1284 - accuracy: 0.6059 - val_loss: 3.1196 - val_accuracy: 0.4247\n",
            "Epoch 4823/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0982 - accuracy: 0.6281\n",
            "Epoch 04823: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1364 - accuracy: 0.6231 - val_loss: 3.0701 - val_accuracy: 0.4384\n",
            "Epoch 4824/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1106 - accuracy: 0.6172\n",
            "Epoch 04824: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0807 - accuracy: 0.6351 - val_loss: 3.0295 - val_accuracy: 0.4247\n",
            "Epoch 4825/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 1.1412 - accuracy: 0.6213\n",
            "Epoch 04825: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1628 - accuracy: 0.6127 - val_loss: 3.0234 - val_accuracy: 0.4384\n",
            "Epoch 4826/5000\n",
            "17/19 [=========================>....] - ETA: 0s - loss: 1.1148 - accuracy: 0.6195\n",
            "Epoch 04826: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.1163 - accuracy: 0.6179 - val_loss: 2.9448 - val_accuracy: 0.4178\n",
            "Epoch 4827/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1157 - accuracy: 0.6165\n",
            "Epoch 04827: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0940 - accuracy: 0.6162 - val_loss: 2.9619 - val_accuracy: 0.4110\n",
            "Epoch 4828/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0903 - accuracy: 0.6108\n",
            "Epoch 04828: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1098 - accuracy: 0.6248 - val_loss: 3.0166 - val_accuracy: 0.4041\n",
            "Epoch 4829/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.2715 - accuracy: 0.5486\n",
            "Epoch 04829: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1371 - accuracy: 0.6076 - val_loss: 3.1079 - val_accuracy: 0.3836\n",
            "Epoch 4830/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0208 - accuracy: 0.6178\n",
            "Epoch 04830: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0781 - accuracy: 0.6110 - val_loss: 3.0951 - val_accuracy: 0.4041\n",
            "Epoch 4831/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1674 - accuracy: 0.6010\n",
            "Epoch 04831: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1477 - accuracy: 0.5972 - val_loss: 3.1049 - val_accuracy: 0.3904\n",
            "Epoch 4832/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0400 - accuracy: 0.6346\n",
            "Epoch 04832: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0959 - accuracy: 0.6265 - val_loss: 3.2348 - val_accuracy: 0.4041\n",
            "Epoch 4833/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1270 - accuracy: 0.6276\n",
            "Epoch 04833: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0894 - accuracy: 0.6351 - val_loss: 2.9751 - val_accuracy: 0.4178\n",
            "Epoch 4834/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.2357 - accuracy: 0.5852\n",
            "Epoch 04834: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.2217 - accuracy: 0.6024 - val_loss: 2.8633 - val_accuracy: 0.4384\n",
            "Epoch 4835/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0587 - accuracy: 0.6285\n",
            "Epoch 04835: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.0956 - accuracy: 0.6317 - val_loss: 3.0431 - val_accuracy: 0.4178\n",
            "Epoch 4836/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0783 - accuracy: 0.6094\n",
            "Epoch 04836: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0570 - accuracy: 0.6162 - val_loss: 3.0975 - val_accuracy: 0.3973\n",
            "Epoch 4837/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0208 - accuracy: 0.6490\n",
            "Epoch 04837: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1114 - accuracy: 0.6231 - val_loss: 3.1641 - val_accuracy: 0.4384\n",
            "Epoch 4838/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0287 - accuracy: 0.6562\n",
            "Epoch 04838: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0386 - accuracy: 0.6575 - val_loss: 3.0017 - val_accuracy: 0.4110\n",
            "Epoch 4839/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0355 - accuracy: 0.6202\n",
            "Epoch 04839: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0654 - accuracy: 0.6179 - val_loss: 3.0176 - val_accuracy: 0.4110\n",
            "Epoch 4840/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1003 - accuracy: 0.5938\n",
            "Epoch 04840: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0820 - accuracy: 0.6059 - val_loss: 3.1561 - val_accuracy: 0.4110\n",
            "Epoch 4841/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1200 - accuracy: 0.6154\n",
            "Epoch 04841: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1319 - accuracy: 0.6076 - val_loss: 3.2076 - val_accuracy: 0.4110\n",
            "Epoch 4842/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0251 - accuracy: 0.6265\n",
            "Epoch 04842: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0251 - accuracy: 0.6265 - val_loss: 3.2732 - val_accuracy: 0.4178\n",
            "Epoch 4843/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.1626 - accuracy: 0.6208\n",
            "Epoch 04843: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1554 - accuracy: 0.6076 - val_loss: 3.1495 - val_accuracy: 0.4247\n",
            "Epoch 4844/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1700 - accuracy: 0.5881\n",
            "Epoch 04844: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1069 - accuracy: 0.6179 - val_loss: 3.2051 - val_accuracy: 0.3973\n",
            "Epoch 4845/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0853 - accuracy: 0.6172\n",
            "Epoch 04845: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0828 - accuracy: 0.6024 - val_loss: 3.3397 - val_accuracy: 0.4247\n",
            "Epoch 4846/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0965 - accuracy: 0.6082\n",
            "Epoch 04846: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0654 - accuracy: 0.6248 - val_loss: 3.3816 - val_accuracy: 0.4110\n",
            "Epoch 4847/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1465 - accuracy: 0.5966\n",
            "Epoch 04847: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0965 - accuracy: 0.6179 - val_loss: 3.3920 - val_accuracy: 0.4041\n",
            "Epoch 4848/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0798 - accuracy: 0.5909\n",
            "Epoch 04848: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0225 - accuracy: 0.6265 - val_loss: 3.0805 - val_accuracy: 0.4384\n",
            "Epoch 4849/5000\n",
            "16/19 [========================>.....] - ETA: 0s - loss: 1.1275 - accuracy: 0.6016\n",
            "Epoch 04849: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.0875 - accuracy: 0.6162 - val_loss: 3.0618 - val_accuracy: 0.4247\n",
            "Epoch 4850/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1036 - accuracy: 0.6023\n",
            "Epoch 04850: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1836 - accuracy: 0.5835 - val_loss: 3.1762 - val_accuracy: 0.4315\n",
            "Epoch 4851/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0415 - accuracy: 0.6490\n",
            "Epoch 04851: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0382 - accuracy: 0.6403 - val_loss: 3.1701 - val_accuracy: 0.4178\n",
            "Epoch 4852/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1469 - accuracy: 0.6094\n",
            "Epoch 04852: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0969 - accuracy: 0.6248 - val_loss: 3.2731 - val_accuracy: 0.3904\n",
            "Epoch 4853/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0416 - accuracy: 0.6215\n",
            "Epoch 04853: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0341 - accuracy: 0.6334 - val_loss: 3.2782 - val_accuracy: 0.4247\n",
            "Epoch 4854/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1517 - accuracy: 0.6094\n",
            "Epoch 04854: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0840 - accuracy: 0.6386 - val_loss: 3.1689 - val_accuracy: 0.4384\n",
            "Epoch 4855/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9386 - accuracy: 0.6693\n",
            "Epoch 04855: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9372 - accuracy: 0.6747 - val_loss: 3.2729 - val_accuracy: 0.4315\n",
            "Epoch 4856/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0908 - accuracy: 0.6108\n",
            "Epoch 04856: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0537 - accuracy: 0.6317 - val_loss: 3.2860 - val_accuracy: 0.4247\n",
            "Epoch 4857/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9801 - accuracy: 0.6719\n",
            "Epoch 04857: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0006 - accuracy: 0.6472 - val_loss: 3.2733 - val_accuracy: 0.4247\n",
            "Epoch 4858/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0828 - accuracy: 0.6484\n",
            "Epoch 04858: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0480 - accuracy: 0.6523 - val_loss: 3.1931 - val_accuracy: 0.4521\n",
            "Epoch 4859/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1143 - accuracy: 0.5911\n",
            "Epoch 04859: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1206 - accuracy: 0.6024 - val_loss: 3.2375 - val_accuracy: 0.4315\n",
            "Epoch 4860/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0495 - accuracy: 0.6420\n",
            "Epoch 04860: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0584 - accuracy: 0.6299 - val_loss: 3.2325 - val_accuracy: 0.4452\n",
            "Epoch 4861/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1701 - accuracy: 0.5938\n",
            "Epoch 04861: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0961 - accuracy: 0.6317 - val_loss: 3.0726 - val_accuracy: 0.4315\n",
            "Epoch 4862/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0419 - accuracy: 0.6489\n",
            "Epoch 04862: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0419 - accuracy: 0.6489 - val_loss: 3.1975 - val_accuracy: 0.4658\n",
            "Epoch 4863/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0779 - accuracy: 0.6250\n",
            "Epoch 04863: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0414 - accuracy: 0.6368 - val_loss: 3.3732 - val_accuracy: 0.4384\n",
            "Epoch 4864/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0332 - accuracy: 0.6322\n",
            "Epoch 04864: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0413 - accuracy: 0.6162 - val_loss: 3.0965 - val_accuracy: 0.4452\n",
            "Epoch 4865/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0276 - accuracy: 0.6274\n",
            "Epoch 04865: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0337 - accuracy: 0.6282 - val_loss: 3.0595 - val_accuracy: 0.4589\n",
            "Epoch 4866/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9864 - accuracy: 0.6380\n",
            "Epoch 04866: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0204 - accuracy: 0.6282 - val_loss: 3.1069 - val_accuracy: 0.4521\n",
            "Epoch 4867/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0918 - accuracy: 0.5964\n",
            "Epoch 04867: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0496 - accuracy: 0.6213 - val_loss: 3.1266 - val_accuracy: 0.4589\n",
            "Epoch 4868/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1863 - accuracy: 0.5729\n",
            "Epoch 04868: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1955 - accuracy: 0.5818 - val_loss: 3.1863 - val_accuracy: 0.4658\n",
            "Epoch 4869/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0715 - accuracy: 0.6162\n",
            "Epoch 04869: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0715 - accuracy: 0.6162 - val_loss: 3.1896 - val_accuracy: 0.4452\n",
            "Epoch 4870/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1314 - accuracy: 0.5885\n",
            "Epoch 04870: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1454 - accuracy: 0.5835 - val_loss: 3.2148 - val_accuracy: 0.4452\n",
            "Epoch 4871/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.1349 - accuracy: 0.5990\n",
            "Epoch 04871: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1302 - accuracy: 0.6007 - val_loss: 3.2252 - val_accuracy: 0.4315\n",
            "Epoch 4872/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1278 - accuracy: 0.6059\n",
            "Epoch 04872: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1278 - accuracy: 0.6059 - val_loss: 3.1338 - val_accuracy: 0.4521\n",
            "Epoch 4873/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1755 - accuracy: 0.6042\n",
            "Epoch 04873: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1524 - accuracy: 0.6213 - val_loss: 3.2846 - val_accuracy: 0.4384\n",
            "Epoch 4874/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1443 - accuracy: 0.5885\n",
            "Epoch 04874: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2121 - accuracy: 0.5904 - val_loss: 3.1855 - val_accuracy: 0.4315\n",
            "Epoch 4875/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0235 - accuracy: 0.6302\n",
            "Epoch 04875: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1829 - accuracy: 0.5835 - val_loss: 3.1345 - val_accuracy: 0.4658\n",
            "Epoch 4876/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1716 - accuracy: 0.5994\n",
            "Epoch 04876: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1828 - accuracy: 0.6059 - val_loss: 3.1725 - val_accuracy: 0.4041\n",
            "Epoch 4877/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0375 - accuracy: 0.6472\n",
            "Epoch 04877: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0375 - accuracy: 0.6472 - val_loss: 3.2103 - val_accuracy: 0.4178\n",
            "Epoch 4878/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1599 - accuracy: 0.6162\n",
            "Epoch 04878: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1599 - accuracy: 0.6162 - val_loss: 3.3240 - val_accuracy: 0.4247\n",
            "Epoch 4879/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1242 - accuracy: 0.6302\n",
            "Epoch 04879: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0945 - accuracy: 0.6282 - val_loss: 3.4957 - val_accuracy: 0.4384\n",
            "Epoch 4880/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0578 - accuracy: 0.6466\n",
            "Epoch 04880: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0649 - accuracy: 0.6420 - val_loss: 3.4012 - val_accuracy: 0.4384\n",
            "Epoch 4881/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1420 - accuracy: 0.6162\n",
            "Epoch 04881: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.1420 - accuracy: 0.6162 - val_loss: 3.4600 - val_accuracy: 0.4110\n",
            "Epoch 4882/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0793 - accuracy: 0.6250\n",
            "Epoch 04882: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0751 - accuracy: 0.6265 - val_loss: 3.5454 - val_accuracy: 0.4110\n",
            "Epoch 4883/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0857 - accuracy: 0.6265\n",
            "Epoch 04883: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0857 - accuracy: 0.6265 - val_loss: 3.3936 - val_accuracy: 0.4247\n",
            "Epoch 4884/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.2732 - accuracy: 0.6392\n",
            "Epoch 04884: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2189 - accuracy: 0.6196 - val_loss: 3.3803 - val_accuracy: 0.4315\n",
            "Epoch 4885/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1564 - accuracy: 0.6250\n",
            "Epoch 04885: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1792 - accuracy: 0.6317 - val_loss: 3.4843 - val_accuracy: 0.4247\n",
            "Epoch 4886/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1082 - accuracy: 0.6449\n",
            "Epoch 04886: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1832 - accuracy: 0.6179 - val_loss: 2.9846 - val_accuracy: 0.4178\n",
            "Epoch 4887/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.2526 - accuracy: 0.5682\n",
            "Epoch 04887: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2137 - accuracy: 0.5921 - val_loss: 3.0749 - val_accuracy: 0.3973\n",
            "Epoch 4888/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.2116 - accuracy: 0.5972\n",
            "Epoch 04888: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.2028 - accuracy: 0.6007 - val_loss: 3.2616 - val_accuracy: 0.4178\n",
            "Epoch 4889/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1069 - accuracy: 0.6196\n",
            "Epoch 04889: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1069 - accuracy: 0.6196 - val_loss: 3.2742 - val_accuracy: 0.4178\n",
            "Epoch 4890/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.1712 - accuracy: 0.6292\n",
            "Epoch 04890: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 1.2178 - accuracy: 0.6213 - val_loss: 3.3007 - val_accuracy: 0.4315\n",
            "Epoch 4891/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.2625 - accuracy: 0.5540\n",
            "Epoch 04891: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1906 - accuracy: 0.5835 - val_loss: 3.2243 - val_accuracy: 0.4110\n",
            "Epoch 4892/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1173 - accuracy: 0.5938\n",
            "Epoch 04892: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0912 - accuracy: 0.6162 - val_loss: 3.3610 - val_accuracy: 0.4041\n",
            "Epoch 4893/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0254 - accuracy: 0.6591\n",
            "Epoch 04893: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0856 - accuracy: 0.6506 - val_loss: 3.4396 - val_accuracy: 0.4041\n",
            "Epoch 4894/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.2689 - accuracy: 0.5881\n",
            "Epoch 04894: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1582 - accuracy: 0.6231 - val_loss: 3.3742 - val_accuracy: 0.4315\n",
            "Epoch 4895/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0721 - accuracy: 0.6354\n",
            "Epoch 04895: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0653 - accuracy: 0.6420 - val_loss: 3.3222 - val_accuracy: 0.4521\n",
            "Epoch 4896/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1551 - accuracy: 0.6193\n",
            "Epoch 04896: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1345 - accuracy: 0.6145 - val_loss: 3.2506 - val_accuracy: 0.4247\n",
            "Epoch 4897/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0630 - accuracy: 0.6449\n",
            "Epoch 04897: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0979 - accuracy: 0.6403 - val_loss: 3.2018 - val_accuracy: 0.4521\n",
            "Epoch 4898/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9898 - accuracy: 0.6165\n",
            "Epoch 04898: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0188 - accuracy: 0.6110 - val_loss: 3.1846 - val_accuracy: 0.4384\n",
            "Epoch 4899/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9865 - accuracy: 0.6418\n",
            "Epoch 04899: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0387 - accuracy: 0.6282 - val_loss: 3.1952 - val_accuracy: 0.4726\n",
            "Epoch 4900/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0545 - accuracy: 0.6635\n",
            "Epoch 04900: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0789 - accuracy: 0.6540 - val_loss: 3.2352 - val_accuracy: 0.4384\n",
            "Epoch 4901/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.2277 - accuracy: 0.5781\n",
            "Epoch 04901: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1510 - accuracy: 0.6127 - val_loss: 3.2162 - val_accuracy: 0.3904\n",
            "Epoch 4902/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1017 - accuracy: 0.6615\n",
            "Epoch 04902: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1230 - accuracy: 0.6489 - val_loss: 3.2823 - val_accuracy: 0.4315\n",
            "Epoch 4903/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.3467 - accuracy: 0.5966\n",
            "Epoch 04903: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2401 - accuracy: 0.6162 - val_loss: 3.1172 - val_accuracy: 0.4110\n",
            "Epoch 4904/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1509 - accuracy: 0.5969\n",
            "Epoch 04904: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2068 - accuracy: 0.5835 - val_loss: 3.0392 - val_accuracy: 0.4315\n",
            "Epoch 4905/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1258 - accuracy: 0.6068\n",
            "Epoch 04905: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1511 - accuracy: 0.5990 - val_loss: 3.0074 - val_accuracy: 0.4521\n",
            "Epoch 4906/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1586 - accuracy: 0.6106\n",
            "Epoch 04906: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1317 - accuracy: 0.6110 - val_loss: 3.0615 - val_accuracy: 0.4452\n",
            "Epoch 4907/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.1024 - accuracy: 0.6302\n",
            "Epoch 04907: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0932 - accuracy: 0.6334 - val_loss: 2.9755 - val_accuracy: 0.4589\n",
            "Epoch 4908/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1136 - accuracy: 0.6299\n",
            "Epoch 04908: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1136 - accuracy: 0.6299 - val_loss: 2.8932 - val_accuracy: 0.4521\n",
            "Epoch 4909/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.2237 - accuracy: 0.5994\n",
            "Epoch 04909: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1995 - accuracy: 0.6041 - val_loss: 3.0085 - val_accuracy: 0.4521\n",
            "Epoch 4910/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1117 - accuracy: 0.6108\n",
            "Epoch 04910: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2091 - accuracy: 0.5904 - val_loss: 3.1696 - val_accuracy: 0.4315\n",
            "Epoch 4911/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1413 - accuracy: 0.5990\n",
            "Epoch 04911: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1838 - accuracy: 0.5972 - val_loss: 3.0962 - val_accuracy: 0.4178\n",
            "Epoch 4912/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1398 - accuracy: 0.6162\n",
            "Epoch 04912: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1398 - accuracy: 0.6162 - val_loss: 3.0462 - val_accuracy: 0.4247\n",
            "Epoch 4913/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 0.9812 - accuracy: 0.6632\n",
            "Epoch 04913: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0697 - accuracy: 0.6351 - val_loss: 3.1052 - val_accuracy: 0.4110\n",
            "Epoch 4914/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0726 - accuracy: 0.6278\n",
            "Epoch 04914: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0473 - accuracy: 0.6437 - val_loss: 3.0922 - val_accuracy: 0.4110\n",
            "Epoch 4915/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0950 - accuracy: 0.6307\n",
            "Epoch 04915: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1521 - accuracy: 0.6265 - val_loss: 3.1000 - val_accuracy: 0.4315\n",
            "Epoch 4916/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0743 - accuracy: 0.6392\n",
            "Epoch 04916: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0316 - accuracy: 0.6506 - val_loss: 3.2012 - val_accuracy: 0.4110\n",
            "Epoch 4917/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0545 - accuracy: 0.6597\n",
            "Epoch 04917: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0541 - accuracy: 0.6592 - val_loss: 3.2301 - val_accuracy: 0.4315\n",
            "Epoch 4918/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1658 - accuracy: 0.5938\n",
            "Epoch 04918: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0903 - accuracy: 0.6213 - val_loss: 3.1396 - val_accuracy: 0.4247\n",
            "Epoch 4919/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0545 - accuracy: 0.6406\n",
            "Epoch 04919: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0716 - accuracy: 0.6403 - val_loss: 3.2813 - val_accuracy: 0.4178\n",
            "Epoch 4920/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0480 - accuracy: 0.6380\n",
            "Epoch 04920: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0523 - accuracy: 0.6386 - val_loss: 3.3471 - val_accuracy: 0.4247\n",
            "Epoch 4921/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0974 - accuracy: 0.6181\n",
            "Epoch 04921: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0764 - accuracy: 0.6351 - val_loss: 3.3559 - val_accuracy: 0.4521\n",
            "Epoch 4922/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.0865 - accuracy: 0.6250\n",
            "Epoch 04922: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0841 - accuracy: 0.6265 - val_loss: 3.1979 - val_accuracy: 0.4178\n",
            "Epoch 4923/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9998 - accuracy: 0.6406\n",
            "Epoch 04923: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0242 - accuracy: 0.6299 - val_loss: 3.2837 - val_accuracy: 0.4041\n",
            "Epoch 4924/5000\n",
            "15/19 [======================>.......] - ETA: 0s - loss: 1.1560 - accuracy: 0.5854\n",
            "Epoch 04924: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1141 - accuracy: 0.6007 - val_loss: 3.1088 - val_accuracy: 0.4041\n",
            "Epoch 4925/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0499 - accuracy: 0.6406\n",
            "Epoch 04925: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0461 - accuracy: 0.6351 - val_loss: 3.0339 - val_accuracy: 0.4041\n",
            "Epoch 4926/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1379 - accuracy: 0.6187\n",
            "Epoch 04926: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0728 - accuracy: 0.6386 - val_loss: 3.0110 - val_accuracy: 0.4315\n",
            "Epoch 4927/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0502 - accuracy: 0.6165\n",
            "Epoch 04927: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0624 - accuracy: 0.6196 - val_loss: 3.0141 - val_accuracy: 0.4178\n",
            "Epoch 4928/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1186 - accuracy: 0.6193\n",
            "Epoch 04928: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0614 - accuracy: 0.6403 - val_loss: 3.0542 - val_accuracy: 0.4384\n",
            "Epoch 4929/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1256 - accuracy: 0.6042\n",
            "Epoch 04929: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0996 - accuracy: 0.6334 - val_loss: 3.0849 - val_accuracy: 0.4452\n",
            "Epoch 4930/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0546 - accuracy: 0.6198\n",
            "Epoch 04930: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0413 - accuracy: 0.6231 - val_loss: 3.0410 - val_accuracy: 0.4521\n",
            "Epoch 4931/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0238 - accuracy: 0.6302\n",
            "Epoch 04931: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0519 - accuracy: 0.6110 - val_loss: 3.0428 - val_accuracy: 0.4452\n",
            "Epoch 4932/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0315 - accuracy: 0.6625\n",
            "Epoch 04932: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0431 - accuracy: 0.6609 - val_loss: 3.1133 - val_accuracy: 0.4589\n",
            "Epoch 4933/5000\n",
            "16/19 [========================>.....] - ETA: 0s - loss: 1.0260 - accuracy: 0.6504\n",
            "Epoch 04933: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.0740 - accuracy: 0.6351 - val_loss: 3.1007 - val_accuracy: 0.4178\n",
            "Epoch 4934/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9084 - accuracy: 0.6901\n",
            "Epoch 04934: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9407 - accuracy: 0.6730 - val_loss: 3.1916 - val_accuracy: 0.4247\n",
            "Epoch 4935/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0101 - accuracy: 0.6534\n",
            "Epoch 04935: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0618 - accuracy: 0.6368 - val_loss: 3.2891 - val_accuracy: 0.3973\n",
            "Epoch 4936/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1893 - accuracy: 0.5885\n",
            "Epoch 04936: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1883 - accuracy: 0.5852 - val_loss: 3.3570 - val_accuracy: 0.4384\n",
            "Epoch 4937/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1702 - accuracy: 0.5904\n",
            "Epoch 04937: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1702 - accuracy: 0.5904 - val_loss: 3.1914 - val_accuracy: 0.4315\n",
            "Epoch 4938/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.1998 - accuracy: 0.6161\n",
            "Epoch 04938: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1461 - accuracy: 0.6162 - val_loss: 3.0980 - val_accuracy: 0.4521\n",
            "Epoch 4939/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.2047 - accuracy: 0.6156\n",
            "Epoch 04939: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1680 - accuracy: 0.6213 - val_loss: 2.9845 - val_accuracy: 0.4521\n",
            "Epoch 4940/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.1320 - accuracy: 0.6110\n",
            "Epoch 04940: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1320 - accuracy: 0.6110 - val_loss: 2.9980 - val_accuracy: 0.4452\n",
            "Epoch 4941/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1965 - accuracy: 0.6328\n",
            "Epoch 04941: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1752 - accuracy: 0.6334 - val_loss: 2.8339 - val_accuracy: 0.4384\n",
            "Epoch 4942/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1426 - accuracy: 0.6172\n",
            "Epoch 04942: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1591 - accuracy: 0.6196 - val_loss: 2.9044 - val_accuracy: 0.4315\n",
            "Epoch 4943/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9708 - accuracy: 0.6534\n",
            "Epoch 04943: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0135 - accuracy: 0.6334 - val_loss: 2.8428 - val_accuracy: 0.4521\n",
            "Epoch 4944/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1029 - accuracy: 0.6198\n",
            "Epoch 04944: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1187 - accuracy: 0.6334 - val_loss: 2.8881 - val_accuracy: 0.4315\n",
            "Epoch 4945/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0489 - accuracy: 0.6224\n",
            "Epoch 04945: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0917 - accuracy: 0.6127 - val_loss: 2.8023 - val_accuracy: 0.4521\n",
            "Epoch 4946/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.1337 - accuracy: 0.5729\n",
            "Epoch 04946: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1166 - accuracy: 0.6007 - val_loss: 2.8206 - val_accuracy: 0.4452\n",
            "Epoch 4947/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1233 - accuracy: 0.6172\n",
            "Epoch 04947: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1289 - accuracy: 0.6076 - val_loss: 2.7954 - val_accuracy: 0.4384\n",
            "Epoch 4948/5000\n",
            "14/19 [=====================>........] - ETA: 0s - loss: 1.0136 - accuracy: 0.6518\n",
            "Epoch 04948: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0253 - accuracy: 0.6454 - val_loss: 2.9256 - val_accuracy: 0.4384\n",
            "Epoch 4949/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1495 - accuracy: 0.6108\n",
            "Epoch 04949: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1064 - accuracy: 0.6179 - val_loss: 2.8954 - val_accuracy: 0.4384\n",
            "Epoch 4950/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0780 - accuracy: 0.6042\n",
            "Epoch 04950: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1032 - accuracy: 0.6007 - val_loss: 2.7914 - val_accuracy: 0.4315\n",
            "Epoch 4951/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0569 - accuracy: 0.6276\n",
            "Epoch 04951: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0743 - accuracy: 0.6231 - val_loss: 2.8008 - val_accuracy: 0.4315\n",
            "Epoch 4952/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1180 - accuracy: 0.6125\n",
            "Epoch 04952: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1438 - accuracy: 0.6024 - val_loss: 2.9941 - val_accuracy: 0.4247\n",
            "Epoch 4953/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1139 - accuracy: 0.6274\n",
            "Epoch 04953: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0860 - accuracy: 0.6282 - val_loss: 3.0502 - val_accuracy: 0.4110\n",
            "Epoch 4954/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0720 - accuracy: 0.6449\n",
            "Epoch 04954: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1075 - accuracy: 0.6196 - val_loss: 3.0627 - val_accuracy: 0.4315\n",
            "Epoch 4955/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1921 - accuracy: 0.6392\n",
            "Epoch 04955: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1557 - accuracy: 0.6196 - val_loss: 3.0416 - val_accuracy: 0.4315\n",
            "Epoch 4956/5000\n",
            "16/19 [========================>.....] - ETA: 0s - loss: 1.0685 - accuracy: 0.6270\n",
            "Epoch 04956: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0748 - accuracy: 0.6213 - val_loss: 2.9855 - val_accuracy: 0.4315\n",
            "Epoch 4957/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1365 - accuracy: 0.6120\n",
            "Epoch 04957: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1384 - accuracy: 0.5955 - val_loss: 3.0708 - val_accuracy: 0.4315\n",
            "Epoch 4958/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.0663 - accuracy: 0.6000\n",
            "Epoch 04958: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0309 - accuracy: 0.6265 - val_loss: 3.2116 - val_accuracy: 0.4315\n",
            "Epoch 4959/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0821 - accuracy: 0.6334\n",
            "Epoch 04959: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0821 - accuracy: 0.6334 - val_loss: 3.2843 - val_accuracy: 0.4041\n",
            "Epoch 4960/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0764 - accuracy: 0.6276\n",
            "Epoch 04960: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0938 - accuracy: 0.6231 - val_loss: 3.3393 - val_accuracy: 0.4178\n",
            "Epoch 4961/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.1213 - accuracy: 0.6111\n",
            "Epoch 04961: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1159 - accuracy: 0.6127 - val_loss: 3.0383 - val_accuracy: 0.4178\n",
            "Epoch 4962/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0213 - accuracy: 0.6449\n",
            "Epoch 04962: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0911 - accuracy: 0.6317 - val_loss: 3.0421 - val_accuracy: 0.4247\n",
            "Epoch 4963/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0367 - accuracy: 0.6587\n",
            "Epoch 04963: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0535 - accuracy: 0.6454 - val_loss: 3.0046 - val_accuracy: 0.4178\n",
            "Epoch 4964/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.1935 - accuracy: 0.5972\n",
            "Epoch 04964: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1941 - accuracy: 0.5972 - val_loss: 3.1066 - val_accuracy: 0.4110\n",
            "Epoch 4965/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1502 - accuracy: 0.6346\n",
            "Epoch 04965: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.1337 - accuracy: 0.6248 - val_loss: 3.0458 - val_accuracy: 0.4178\n",
            "Epoch 4966/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.0116 - accuracy: 0.6538\n",
            "Epoch 04966: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0216 - accuracy: 0.6489 - val_loss: 3.0462 - val_accuracy: 0.4315\n",
            "Epoch 4967/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 0.9947 - accuracy: 0.6420\n",
            "Epoch 04967: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0334 - accuracy: 0.6334 - val_loss: 3.0489 - val_accuracy: 0.4315\n",
            "Epoch 4968/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 0.9519 - accuracy: 0.6536\n",
            "Epoch 04968: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9725 - accuracy: 0.6368 - val_loss: 3.0600 - val_accuracy: 0.4315\n",
            "Epoch 4969/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0125 - accuracy: 0.6484\n",
            "Epoch 04969: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 1.0221 - accuracy: 0.6386 - val_loss: 3.0872 - val_accuracy: 0.4452\n",
            "Epoch 4970/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 0.9465 - accuracy: 0.6438\n",
            "Epoch 04970: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0316 - accuracy: 0.6282 - val_loss: 3.0791 - val_accuracy: 0.4178\n",
            "Epoch 4971/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.1739 - accuracy: 0.6364\n",
            "Epoch 04971: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1380 - accuracy: 0.6196 - val_loss: 3.1621 - val_accuracy: 0.4384\n",
            "Epoch 4972/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1695 - accuracy: 0.5911\n",
            "Epoch 04972: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.2042 - accuracy: 0.5904 - val_loss: 3.1719 - val_accuracy: 0.4110\n",
            "Epoch 4973/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1074 - accuracy: 0.6202\n",
            "Epoch 04973: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1453 - accuracy: 0.6145 - val_loss: 3.0757 - val_accuracy: 0.4178\n",
            "Epoch 4974/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0122 - accuracy: 0.6380\n",
            "Epoch 04974: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0085 - accuracy: 0.6317 - val_loss: 3.0578 - val_accuracy: 0.4315\n",
            "Epoch 4975/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0366 - accuracy: 0.6265\n",
            "Epoch 04975: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.0366 - accuracy: 0.6265 - val_loss: 3.0747 - val_accuracy: 0.4384\n",
            "Epoch 4976/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0379 - accuracy: 0.6127\n",
            "Epoch 04976: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0379 - accuracy: 0.6127 - val_loss: 3.1320 - val_accuracy: 0.4110\n",
            "Epoch 4977/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9874 - accuracy: 0.6202\n",
            "Epoch 04977: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0082 - accuracy: 0.6334 - val_loss: 3.0604 - val_accuracy: 0.4041\n",
            "Epoch 4978/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.0168 - accuracy: 0.6299\n",
            "Epoch 04978: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0168 - accuracy: 0.6299 - val_loss: 3.0255 - val_accuracy: 0.4315\n",
            "Epoch 4979/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 0.9768 - accuracy: 0.6442\n",
            "Epoch 04979: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 0.9749 - accuracy: 0.6523 - val_loss: 3.0010 - val_accuracy: 0.4521\n",
            "Epoch 4980/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1378 - accuracy: 0.6094\n",
            "Epoch 04980: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1079 - accuracy: 0.6213 - val_loss: 3.0621 - val_accuracy: 0.4521\n",
            "Epoch 4981/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0965 - accuracy: 0.6328\n",
            "Epoch 04981: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1200 - accuracy: 0.6213 - val_loss: 3.2064 - val_accuracy: 0.4452\n",
            "Epoch 4982/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1493 - accuracy: 0.6068\n",
            "Epoch 04982: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1279 - accuracy: 0.6213 - val_loss: 3.1020 - val_accuracy: 0.4247\n",
            "Epoch 4983/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0816 - accuracy: 0.6276\n",
            "Epoch 04983: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1286 - accuracy: 0.6282 - val_loss: 3.1512 - val_accuracy: 0.4315\n",
            "Epoch 4984/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.2014 - accuracy: 0.5817\n",
            "Epoch 04984: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1599 - accuracy: 0.5955 - val_loss: 3.2995 - val_accuracy: 0.4178\n",
            "Epoch 4985/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0635 - accuracy: 0.6172\n",
            "Epoch 04985: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0923 - accuracy: 0.6110 - val_loss: 3.2191 - val_accuracy: 0.4041\n",
            "Epoch 4986/5000\n",
            " 9/19 [=============>................] - ETA: 0s - loss: 1.0546 - accuracy: 0.6215\n",
            "Epoch 04986: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.0988 - accuracy: 0.6041 - val_loss: 3.3988 - val_accuracy: 0.3973\n",
            "Epoch 4987/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0632 - accuracy: 0.6172\n",
            "Epoch 04987: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1118 - accuracy: 0.6231 - val_loss: 3.3760 - val_accuracy: 0.4178\n",
            "Epoch 4988/5000\n",
            "11/19 [================>.............] - ETA: 0s - loss: 1.0180 - accuracy: 0.6165\n",
            "Epoch 04988: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0558 - accuracy: 0.6248 - val_loss: 3.2198 - val_accuracy: 0.4247\n",
            "Epoch 4989/5000\n",
            " 8/19 [===========>..................] - ETA: 0s - loss: 1.0433 - accuracy: 0.6523\n",
            "Epoch 04989: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.2069 - accuracy: 0.6351 - val_loss: 3.1461 - val_accuracy: 0.4452\n",
            "Epoch 4990/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1153 - accuracy: 0.5990\n",
            "Epoch 04990: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0965 - accuracy: 0.5990 - val_loss: 3.0456 - val_accuracy: 0.4384\n",
            "Epoch 4991/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.2192 - accuracy: 0.5729\n",
            "Epoch 04991: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.2417 - accuracy: 0.5594 - val_loss: 3.1150 - val_accuracy: 0.4041\n",
            "Epoch 4992/5000\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 1.2113 - accuracy: 0.5816\n",
            "Epoch 04992: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.2239 - accuracy: 0.5818 - val_loss: 3.1614 - val_accuracy: 0.3904\n",
            "Epoch 4993/5000\n",
            "19/19 [==============================] - ETA: 0s - loss: 1.2237 - accuracy: 0.5680\n",
            "Epoch 04993: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 1.2237 - accuracy: 0.5680 - val_loss: 3.1181 - val_accuracy: 0.3767\n",
            "Epoch 4994/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.2061 - accuracy: 0.5673\n",
            "Epoch 04994: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.2124 - accuracy: 0.5645 - val_loss: 3.3543 - val_accuracy: 0.3904\n",
            "Epoch 4995/5000\n",
            "10/19 [==============>...............] - ETA: 0s - loss: 1.1295 - accuracy: 0.5906\n",
            "Epoch 04995: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 8ms/step - loss: 1.1978 - accuracy: 0.5886 - val_loss: 3.3246 - val_accuracy: 0.3973\n",
            "Epoch 4996/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.2290 - accuracy: 0.5781\n",
            "Epoch 04996: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1684 - accuracy: 0.5938 - val_loss: 3.3061 - val_accuracy: 0.3973\n",
            "Epoch 4997/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.0950 - accuracy: 0.6328\n",
            "Epoch 04997: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0721 - accuracy: 0.6299 - val_loss: 3.1978 - val_accuracy: 0.3904\n",
            "Epoch 4998/5000\n",
            "13/19 [===================>..........] - ETA: 0s - loss: 1.1416 - accuracy: 0.5962\n",
            "Epoch 04998: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.1375 - accuracy: 0.6041 - val_loss: 3.1061 - val_accuracy: 0.3973\n",
            "Epoch 4999/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1217 - accuracy: 0.6224\n",
            "Epoch 04999: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1604 - accuracy: 0.6093 - val_loss: 3.1851 - val_accuracy: 0.3973\n",
            "Epoch 5000/5000\n",
            "12/19 [=================>............] - ETA: 0s - loss: 1.1410 - accuracy: 0.5964\n",
            "Epoch 05000: val_loss did not improve from 2.21545\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1480 - accuracy: 0.5938 - val_loss: 3.1294 - val_accuracy: 0.3973\n",
            "Training completed in time:  0:12:21.976939\n"
          ]
        }
      ]
    }
  ]
}